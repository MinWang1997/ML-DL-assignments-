{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“Simple_regression_solution_min.ipynb”",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBagYe8jqRfd",
        "colab_type": "text"
      },
      "source": [
        "# **Min Wang**\n",
        "\n",
        "# **Matrikelnummer   3440557**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjlEFdXpMgvw",
        "colab_type": "text"
      },
      "source": [
        "# Deep learning programming I-A: Regression\n",
        "Felix Wiewel, Institute of Signal Processing and System Theory, University of Stuttgart, 24.04.2020\n",
        "\n",
        "## Introduction\n",
        "This programming exercise is the first of a series of exercises, which are intended as a supplement to the theoretical part of the Deep Learning course offered by the ISS. The goal is to introduce you to basic tasks and applications of methods you have encountered in the lecture. After completing the exercise you should be familiar with the basic ideas and one, possibly simple, way of solving the respective task. It is worth mentioning that most of the tasks can be solved in many different, not necessarily deep learning based, ways and the solution presented here is just one of them.\n",
        "\n",
        "## Regression\n",
        "\n",
        "In this exercise we consider the problem of regression, where we are interested in modeling a functional dependence between different variables with, possibly noisy, observations of input-output pairs. Mathematically such a dependence can be formulated as\n",
        "\n",
        "$\\mathbf{y}=f(\\mathbf{x})+\\boldsymbol{\\epsilon}$,\n",
        "\n",
        "where $\\mathbf{y}\\in\\mathbb{R}^{M}$ and $\\mathbf{x}\\in\\mathbb{R}^{N}$ are the input and output observations, $f:\\mathbb{R}^{N}\\rightarrow\\mathbb{R}^{M}$ is the function mapping inputs to outputs and $\\boldsymbol{\\epsilon}\\in\\mathbb{R}^{M}$ is a random vector, which models noise in our observations. Note that this assumes additive noise that only acts on the output and not on the input variable, which might not be true in all practical applications but is a reasonable approximation. For regression we are now interested in estimating the functional relationship $f$ between the inputs and outputs. This can be done in many different ways, not just with neural networks, but for this exercise we focus on approximating this relationship with a neural network $g_{\\boldsymbol{\\theta}}:\\mathbb{R}^{N}\\rightarrow\\mathbb{R}^{M}$ with parameter vector $\\boldsymbol{\\theta}$. The task is now to choose the parameters of the neural network in a way that results in a \"good\" approximation of $f$ with $g_{\\boldsymbol{\\theta}}$.\n",
        "\n",
        "In order to quantify how \"good\" our neural network can approximate $f$, we adopt a probabilistic view. For this we make the assumption that the noise $\\boldsymbol{\\epsilon}$ is a random vector drawn from a known dustribution, which enables us to derive a suitable cost function for training our neural network and also for quantifying a \"good\" approximation.\n",
        "\n",
        "### Mathematical formulation\n",
        "If we assume that the noise $\\boldsymbol{\\epsilon}$ is drawn from a gaussian distribution, e.g. $\\boldsymbol{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\sigma^{2}\\mathbf{I})$, we can use\n",
        "\n",
        "$\\mathbf{y}=g_{\\boldsymbol{\\theta}}(\\mathbf{x})+\\boldsymbol{\\epsilon}\\Rightarrow \\mathbf{y}-g_{\\boldsymbol{\\theta}}(\\mathbf{x})=\\boldsymbol{\\epsilon}$\n",
        "\n",
        "to derive a log likelihood. Since the probability density function (pdf) of a multivariate normal distribution is given by\n",
        "\n",
        "$p(\\mathbf{x})=\\dfrac{1}{\\sqrt{(2\\pi)^{D}\\vert\\mathbf{C}\\vert}}\\mathrm{e}^{-\\dfrac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})\\mathbf{C}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})^{T}}$,\n",
        "\n",
        "we get\n",
        "\n",
        "$\\ln{p(\\boldsymbol{\\epsilon})}=\\ln{\\dfrac{1}{\\sqrt{(2\\pi)^{M}\\sigma^{2}}}\\mathrm{e}^{-\\dfrac{1}{2\\sigma^{2}}\\Vert\\boldsymbol{\\epsilon}\\Vert_{2}^{2}}}=-\\dfrac{1}{2\\sigma^{2}}\\Vert\\boldsymbol{\\epsilon}\\Vert_{2}^{2}-\\dfrac{1}{2}\\ln{(2\\pi)^{M}\\sigma^{2}}$.\n",
        "\n",
        "Replacing $\\boldsymbol{\\epsilon}$ by $\\mathbf{y}-g_{\\boldsymbol{\\theta}}(\\mathbf{x})$ yields the log likelihood for one particular input-output pair:\n",
        "\n",
        "$\\mathcal{L}(\\mathbf{x},\\mathbf{y},\\boldsymbol{\\theta})=\\ln {p(\\mathbf{y}\\vert\\mathbf{x},\\boldsymbol{\\theta})}=-\\dfrac{1}{2\\sigma^{2}}\\Vert\\boldsymbol{\\mathbf{y}-g_{\\boldsymbol{\\theta}}(\\mathbf{x})}\\Vert_{2}^{2}-\\dfrac{1}{2}\\ln{(2\\pi)^{M}\\sigma^{2}}$\n",
        "\n",
        "This log likelihood measures how likely the input-output pair is and we can use it to train our neural network. For this we maximize the expected log likelihood over all input-output pairs under the assumption that the noise is idependent and identically distributed (i.i.d.) over all input-output pairs. This corresponds to finding the parameters $\\boldsymbol{\\theta}^{\\star}$ of our neural network, which maximize the the expected probability for observing the corresponding input-output pairs. Mathematically the optimal parameters for our neural network are given by\n",
        "\n",
        "$\\boldsymbol{\\theta}^{\\star}=\\arg\\max_{\\boldsymbol{\\theta}}\\mathbb{E}\\left[\\mathcal{L}(\\mathbf{x},\\mathbf{y},\\boldsymbol{\\theta})\\right]=\\arg\\max_{\\boldsymbol{\\theta}}\\mathbb{E}\\left[-\\Vert\\boldsymbol{\\mathbf{y}-g_{\\boldsymbol{\\theta}}(\\mathbf{x})}\\Vert_{2}^{2}\\right]\\approx\\arg\\min_{\\boldsymbol{\\theta}}\\dfrac{1}{N_{D}}\\sum_{i=1}^{N_{D}}\\Vert\\boldsymbol{\\mathbf{y}_{i}-g_{\\boldsymbol{\\theta}}(\\mathbf{x}_{i})}\\Vert_{2}^{2}$,\n",
        "\n",
        "where all terms, which are independent of $\\boldsymbol{\\theta}$, are ignored and the expectation operator is approximated by the mean over all $N_{D}$ input-output pairs. In other words we are maximizing the log likelihood by minimizing the mean squared error loss over all input-output pairs in our dataset, hence this approach is called Maximum Likelihood (ML) estimation. For solving this optimization problem and obtaining the optimal network parameters, stochastic gradient descent (SGD) or one of it's many variants is typically used.\n",
        "\n",
        "It is worth noting, that choosing different distributions for the noise $\\boldsymbol{\\epsilon}$ leads to different log likelihoods and therefore different cost functions for training the neural network. Another commonly used distribution for modelling the noise in regression tasks is the laplace distribution. Deriving the log likelihood and the corresponding costfunction leads to the mean absolute error, which is given by the $l_{1}$-norm of the difference between observations predictions of the neural network. This cost function is considered more robust against outliers since these have less influence on the averall loss compared to the mean squared error.\n",
        "\n",
        "###  Implementation\n",
        "\n",
        "In the following we consider a simple regression task, implement a neural network and train it based on the mathematical fomrulation above. For this we first need to create a set of input-output pairs, which then needs to be partitioned into a training, validation and test set. We also define some constants to be used for partitioning the data and the hyperparameters for our neural network.\n",
        "\n",
        "But before we can start, we need to install tensorflow and import the necessary packages tensorflow, numpy and matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JL0d398Mgvz",
        "colab_type": "code",
        "outputId": "9ee9f778-714d-4a72-8909-4a4a0b8b4b46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        }
      },
      "source": [
        "!pip3 install tensorflow-gpu==2.0.0-beta0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0-beta0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/7e/87c4c94686cda7066f52cbca4c344248516490acdd6b258ec6b8a805d956/tensorflow_gpu-2.0.0b0-cp36-cp36m-manylinux1_x86_64.whl (348.8MB)\n",
            "\u001b[K     |████████████████████████████████| 348.9MB 76kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.16.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (3.7.1)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 (from tensorflow-gpu==2.0.0-beta0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/dd/99c47dd007dcf10d63fd895611b063732646f23059c618a373e85019eb0e/tf_estimator_nightly-1.14.0.dev2019060501-py2.py3-none-any.whl (496kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 41.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (0.2.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (0.7.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (0.1.7)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (0.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.11.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (0.33.4)\n",
            "Collecting tb-nightly<1.14.0a20190604,>=1.14.0a20190603 (from tensorflow-gpu==2.0.0-beta0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/96/571b875cd81dda9d5dfa1422a4f9d749e67c0a8d4f4f0b33a4e5f5f35e27/tb_nightly-1.14.0a20190603-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 26.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-beta0) (41.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-beta0) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta0) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta0) (0.15.4)\n",
            "Installing collected packages: tf-estimator-nightly, tb-nightly, tensorflow-gpu\n",
            "Successfully installed tb-nightly-1.14.0a20190603 tensorflow-gpu-2.0.0b0 tf-estimator-nightly-1.14.0.dev2019060501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPuVp2lyNK2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J84v9uucMgv5",
        "colab_type": "text"
      },
      "source": [
        "Next we define our constants and set the random seeds of tensorflow and numpy in order to get reproducable results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwC-1OnHMgv7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_train_samples = 600\n",
        "N_validation_samples = 100\n",
        "N_test_samples = 100\n",
        "N_samples = N_train_samples + N_validation_samples + N_test_samples\n",
        "noise_sig = 0.1\n",
        "N_epochs = 150\n",
        "batch_size = 8\n",
        "learning_rate = 0.01\n",
        "\n",
        "tf.random.set_seed(0)\n",
        "np.random.seed(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngFFSyG-MgwA",
        "colab_type": "text"
      },
      "source": [
        "We create $600$ training samples, $100$ validation samples to optimize our hyperparameters and $100$ test samples, which are used to check if our model can generalize to unseen data. Furthermore we set the level of noise added to the observations. For training the model we plan to train it for $150$ epochs with a batch size of $8$ and a learning rate of $0.81$. Next we create the actual signal and plot it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s8AtsdQMgwB",
        "colab_type": "code",
        "outputId": "966e8016-de6a-4bcd-e9aa-a301cc1a2325",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "x = np.linspace(0.0, 3.0, N_samples, dtype=np.float32)\n",
        "y = np.expand_dims(np.sin(1.0+x*x) + noise_sig*np.random.randn(N_samples).astype(np.float32), axis=-1)\n",
        "y_true = np.sin(1.0+x*x)\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.plot(x, y_true)\n",
        "plt.legend([\"Observation\", \"Ground truth\"])\n",
        "plt.show()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3wx8vJlMgwI",
        "colab_type": "text"
      },
      "source": [
        "With the input-output pairs created, your first task is now to partition the data in the training, validation and test sets. Keep in mind that we have created the data in a structured way, i.e. the input-output pairs are ordered. This means you need to shuffle the data before partitioning it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDIMUZs0MgwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shuffle_idx = np.arange(0, N_samples)\n",
        "np.random.shuffle(shuffle_idx)\n",
        "x_shuffled = x[shuffle_idx]\n",
        "y_shuffled = y[shuffle_idx]\n",
        "x_train = x_shuffled[0:N_train_samples]\n",
        "y_train = y_shuffled[0:N_train_samples]\n",
        "x_validation = x_shuffled[N_train_samples:N_train_samples+N_validation_samples]\n",
        "y_validation = y_shuffled[N_train_samples:N_train_samples+N_validation_samples]\n",
        "x_test = x_shuffled[N_train_samples+N_validation_samples:]\n",
        "y_test = y_shuffled[N_train_samples+N_validation_samples:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ucvKRhOMgwN",
        "colab_type": "text"
      },
      "source": [
        "In order to feed the data to our model, we will use the Dataset class provided by tensorflow. This class is simple to use and provides all the functionality we need for shuffling, batching and feeding the data to our model. It is also tightly integrated into the tensorflow framework, which makes it very performant. Performance is not an aspect we need to worry about in this exercise, but it is important in more demanding applications.\n",
        "\n",
        "In this exercise we instantiate a separate Dataset object for the training, validation and test data sets, where we shuffle and repeat just the training data set. Shuffling the validation and test data sets is not necessary, since we only evaluate the loss on those data sets and do not perform SGD on it. Please fill in the missing part of the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HifQ63iPMgwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(N_train_samples).batch(batch_size).repeat()\n",
        "validation_ds = tf.data.Dataset.from_tensor_slices((x_validation, y_validation))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Crr6fIkMgwT",
        "colab_type": "text"
      },
      "source": [
        "In this exercise we will create a a simple neural network with two hidden layers containing $10$ neurons. For creating a model and keeping track of its weights a class called MyModel is used. When initializing an instance of this class the necessary variables are created and stored in a list called \"trainable_variables\". This makes it easy to get all trainable variables of the model. We also override the \\__call__ method of this class in order to implement the forward pass of the neural network. This method should accept the inputs to the neural network and should return the result of the forward pass as an output. Please fill in the missing part of the code and select suitable activation functions for the different layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq8ri416MgwX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel(object):\n",
        "    def __init__(self):\n",
        "        # Create model variables\n",
        "        self.W0 = tf.Variable(tf.random.normal([1, 10]), name=\"W0\")\n",
        "        self.b0 = tf.Variable(tf.zeros(10), name=\"b0\")\n",
        "        self.W1 = tf.Variable(tf.random.normal([10, 10]), name=\"W1\")\n",
        "        self.b1 = tf.Variable(tf.zeros(10), name=\"b1\")\n",
        "        self.W2 = tf.Variable(tf.random.normal([10, 1]), name=\"W2\")\n",
        "        self.b2 = tf.Variable(tf.zeros(1), name=\"b2\")\n",
        "        self.trainable_variables = [self.W0, self.b0, self.W1, self.b1, self.W2, self.b2]\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        # Compute forward pass\n",
        "        output = tf.reshape(inputs, [-1, 1])\n",
        "        output = tf.nn.tanh(tf.add(tf.matmul(output, self.W0), self.b0))\n",
        "        output = tf.nn.tanh(tf.add(tf.matmul(output, self.W1), self.b1))\n",
        "        output = tf.add(tf.matmul(output, self.W2), self.b2)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf6m8zXoMgwb",
        "colab_type": "text"
      },
      "source": [
        "Now after the model class is defined we can instantiate a MyModel object by running"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSfI8wjLMgwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mdl = MyModel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KraeH6MsMgwh",
        "colab_type": "text"
      },
      "source": [
        "We can now use the model to make predictions by calling it. In the following we predict on the inputs an plot the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1at5RObMgwi",
        "colab_type": "code",
        "outputId": "5d0c8a28-da32-4e8a-902b-2269fad6ad7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "y_pred = mdl(x)\n",
        "plt.plot(x, y)\n",
        "plt.plot(x, y_true)\n",
        "plt.plot(x, y_pred.numpy())\n",
        "plt.legend([\"Observation\", \"Target\", \"Prediction\"])\n",
        "plt.show()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utFkv4F3NULQ",
        "colab_type": "text"
      },
      "source": [
        "Since we have initialized the variables of the neural network randomly, it's prediction is also random. In order to fit the model we need to minimize the expected mean squared error over all input-ouput pairs in our training data set. For this we need to create a function, that performs a training step when provided with the model, an optimizer and a batch of input-ouput pairs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4zFN0-kOdu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_step(model, optimizer, x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(model.trainable_variables)\n",
        "        y_pred = model(x)\n",
        "        loss_val = tf.reduce_mean(tf.square(y-y_pred))\n",
        "    grads = tape.gradient(loss_val, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    return loss_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ax-Kfd-tOm7I",
        "colab_type": "text"
      },
      "source": [
        "This function uses the GradientTape to record the operations for which gradients have to be calculated. In our case this is the forward pass through our model and the computation of the loss function. After these operations are recoded we can get its gradients and apply these through the use of an optimizer. Finally we return the loss value in order to print it.\n",
        "\n",
        "With the training step function defined we now need to choose a suitable optimizer. Tensorflow offers a wide variety of optimizers but in this exercise we will use the Adam optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PahsqMscPcKD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = tf.optimizers.RMSprop(learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_WjHrAwQB9e",
        "colab_type": "text"
      },
      "source": [
        "We now have everything we need to start training the model. For this we repeatedly sample a batch of input-output pairs from our training data set and use the train_step function to minimize the loss function over this batch. We repeat this until we have iterated over the complete training data set once. After this we compute the loss on the validation data set and print it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3LqS3d_QrX6",
        "colab_type": "code",
        "outputId": "c457d15c-f1c7-43e0-8d93-02d2693d47be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epoch = 0\n",
        "train_iters = 0\n",
        "train_loss = 0.0\n",
        "for x_t, y_t in train_ds:\n",
        "    train_loss += train_step(mdl, opt, x_t, y_t)\n",
        "    train_iters += 1\n",
        "    if (train_iters > int(N_train_samples/batch_size)):\n",
        "        for x_v, y_v in validation_ds:\n",
        "            y_pred = mdl(x_v)\n",
        "            validation_loss = tf.reduce_mean(tf.square(y_v-y_pred))\n",
        "        print(\"Epoch: {} Train loss: {:.5} Validation loss: {:.5}\".format(epoch, train_loss/train_iters, validation_loss))\n",
        "        train_iters = 0\n",
        "        train_loss = 0.0\n",
        "        epoch += 1\n",
        "    if (epoch == N_epochs):\n",
        "        break"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Train loss: 2.1867 Validation loss: 0.1047\n",
            "Epoch: 1 Train loss: 0.27741 Validation loss: 0.15366\n",
            "Epoch: 2 Train loss: 0.21959 Validation loss: 0.014643\n",
            "Epoch: 3 Train loss: 0.17835 Validation loss: 0.00085622\n",
            "Epoch: 4 Train loss: 0.1487 Validation loss: 0.00032905\n",
            "Epoch: 5 Train loss: 0.11961 Validation loss: 0.0010291\n",
            "Epoch: 6 Train loss: 0.10106 Validation loss: 0.00036267\n",
            "Epoch: 7 Train loss: 0.10434 Validation loss: 0.0024425\n",
            "Epoch: 8 Train loss: 0.091014 Validation loss: 0.0091751\n",
            "Epoch: 9 Train loss: 0.087101 Validation loss: 0.00012837\n",
            "Epoch: 10 Train loss: 0.094265 Validation loss: 0.045635\n",
            "Epoch: 11 Train loss: 0.077363 Validation loss: 0.0027354\n",
            "Epoch: 12 Train loss: 0.080432 Validation loss: 0.021195\n",
            "Epoch: 13 Train loss: 0.06635 Validation loss: 0.025338\n",
            "Epoch: 14 Train loss: 0.059181 Validation loss: 0.00041925\n",
            "Epoch: 15 Train loss: 0.062319 Validation loss: 0.009653\n",
            "Epoch: 16 Train loss: 0.050635 Validation loss: 0.041304\n",
            "Epoch: 17 Train loss: 0.056349 Validation loss: 0.018733\n",
            "Epoch: 18 Train loss: 0.049467 Validation loss: 0.0056957\n",
            "Epoch: 19 Train loss: 0.041028 Validation loss: 0.0023671\n",
            "Epoch: 20 Train loss: 0.041426 Validation loss: 0.00018133\n",
            "Epoch: 21 Train loss: 0.039305 Validation loss: 0.0024311\n",
            "Epoch: 22 Train loss: 0.037282 Validation loss: 0.014265\n",
            "Epoch: 23 Train loss: 0.039449 Validation loss: 0.010138\n",
            "Epoch: 24 Train loss: 0.038784 Validation loss: 0.003275\n",
            "Epoch: 25 Train loss: 0.038351 Validation loss: 0.0018657\n",
            "Epoch: 26 Train loss: 0.039597 Validation loss: 0.00020024\n",
            "Epoch: 27 Train loss: 0.031605 Validation loss: 0.00044394\n",
            "Epoch: 28 Train loss: 0.034578 Validation loss: 0.00031044\n",
            "Epoch: 29 Train loss: 0.035264 Validation loss: 0.00257\n",
            "Epoch: 30 Train loss: 0.032819 Validation loss: 0.0011773\n",
            "Epoch: 31 Train loss: 0.037042 Validation loss: 0.021397\n",
            "Epoch: 32 Train loss: 0.032616 Validation loss: 0.0032228\n",
            "Epoch: 33 Train loss: 0.033344 Validation loss: 0.010214\n",
            "Epoch: 34 Train loss: 0.036016 Validation loss: 9.2693e-05\n",
            "Epoch: 35 Train loss: 0.033501 Validation loss: 0.0016839\n",
            "Epoch: 36 Train loss: 0.032409 Validation loss: 0.0027025\n",
            "Epoch: 37 Train loss: 0.0319 Validation loss: 0.0004715\n",
            "Epoch: 38 Train loss: 0.03481 Validation loss: 0.0022279\n",
            "Epoch: 39 Train loss: 0.031956 Validation loss: 8.0607e-05\n",
            "Epoch: 40 Train loss: 0.036587 Validation loss: 5.1875e-06\n",
            "Epoch: 41 Train loss: 0.02977 Validation loss: 0.0059926\n",
            "Epoch: 42 Train loss: 0.035285 Validation loss: 1.9041e-07\n",
            "Epoch: 43 Train loss: 0.030824 Validation loss: 0.0014045\n",
            "Epoch: 44 Train loss: 0.034521 Validation loss: 0.00038274\n",
            "Epoch: 45 Train loss: 0.031685 Validation loss: 0.0029787\n",
            "Epoch: 46 Train loss: 0.031952 Validation loss: 0.00051039\n",
            "Epoch: 47 Train loss: 0.032527 Validation loss: 0.001216\n",
            "Epoch: 48 Train loss: 0.032857 Validation loss: 1.2036e-05\n",
            "Epoch: 49 Train loss: 0.031206 Validation loss: 0.0016934\n",
            "Epoch: 50 Train loss: 0.032326 Validation loss: 0.0012768\n",
            "Epoch: 51 Train loss: 0.030022 Validation loss: 0.0013877\n",
            "Epoch: 52 Train loss: 0.031622 Validation loss: 0.0021565\n",
            "Epoch: 53 Train loss: 0.029334 Validation loss: 0.0041609\n",
            "Epoch: 54 Train loss: 0.031901 Validation loss: 0.0012672\n",
            "Epoch: 55 Train loss: 0.028076 Validation loss: 0.00018914\n",
            "Epoch: 56 Train loss: 0.033653 Validation loss: 0.00081087\n",
            "Epoch: 57 Train loss: 0.028431 Validation loss: 0.00058392\n",
            "Epoch: 58 Train loss: 0.03084 Validation loss: 0.0003164\n",
            "Epoch: 59 Train loss: 0.029889 Validation loss: 0.004458\n",
            "Epoch: 60 Train loss: 0.02823 Validation loss: 0.00018808\n",
            "Epoch: 61 Train loss: 0.027853 Validation loss: 6.33e-05\n",
            "Epoch: 62 Train loss: 0.030204 Validation loss: 6.983e-05\n",
            "Epoch: 63 Train loss: 0.028912 Validation loss: 0.00048373\n",
            "Epoch: 64 Train loss: 0.029631 Validation loss: 0.015556\n",
            "Epoch: 65 Train loss: 0.028934 Validation loss: 0.0084924\n",
            "Epoch: 66 Train loss: 0.027994 Validation loss: 0.0010917\n",
            "Epoch: 67 Train loss: 0.028947 Validation loss: 0.0060026\n",
            "Epoch: 68 Train loss: 0.026992 Validation loss: 0.0017251\n",
            "Epoch: 69 Train loss: 0.027361 Validation loss: 0.0026048\n",
            "Epoch: 70 Train loss: 0.028156 Validation loss: 0.0029486\n",
            "Epoch: 71 Train loss: 0.027431 Validation loss: 9.0004e-05\n",
            "Epoch: 72 Train loss: 0.028104 Validation loss: 0.00016721\n",
            "Epoch: 73 Train loss: 0.026879 Validation loss: 0.0022591\n",
            "Epoch: 74 Train loss: 0.025635 Validation loss: 1.1157e-05\n",
            "Epoch: 75 Train loss: 0.028069 Validation loss: 0.0072812\n",
            "Epoch: 76 Train loss: 0.030208 Validation loss: 0.0029261\n",
            "Epoch: 77 Train loss: 0.029454 Validation loss: 0.00032755\n",
            "Epoch: 78 Train loss: 0.032872 Validation loss: 0.0001151\n",
            "Epoch: 79 Train loss: 0.024294 Validation loss: 7.6789e-05\n",
            "Epoch: 80 Train loss: 0.027382 Validation loss: 0.0015938\n",
            "Epoch: 81 Train loss: 0.027511 Validation loss: 0.00018882\n",
            "Epoch: 82 Train loss: 0.028685 Validation loss: 0.001192\n",
            "Epoch: 83 Train loss: 0.024681 Validation loss: 0.0010562\n",
            "Epoch: 84 Train loss: 0.028317 Validation loss: 7.8469e-06\n",
            "Epoch: 85 Train loss: 0.027873 Validation loss: 0.0021454\n",
            "Epoch: 86 Train loss: 0.022542 Validation loss: 0.0027106\n",
            "Epoch: 87 Train loss: 0.028622 Validation loss: 0.0047049\n",
            "Epoch: 88 Train loss: 0.027484 Validation loss: 1.1464e-06\n",
            "Epoch: 89 Train loss: 0.026737 Validation loss: 2.6947e-05\n",
            "Epoch: 90 Train loss: 0.02435 Validation loss: 0.00064155\n",
            "Epoch: 91 Train loss: 0.026571 Validation loss: 0.013834\n",
            "Epoch: 92 Train loss: 0.025084 Validation loss: 0.00075171\n",
            "Epoch: 93 Train loss: 0.028011 Validation loss: 0.00051944\n",
            "Epoch: 94 Train loss: 0.025226 Validation loss: 0.003551\n",
            "Epoch: 95 Train loss: 0.025182 Validation loss: 0.00022878\n",
            "Epoch: 96 Train loss: 0.026648 Validation loss: 0.0039931\n",
            "Epoch: 97 Train loss: 0.026663 Validation loss: 0.0015636\n",
            "Epoch: 98 Train loss: 0.026724 Validation loss: 9.5196e-05\n",
            "Epoch: 99 Train loss: 0.026899 Validation loss: 0.0048458\n",
            "Epoch: 100 Train loss: 0.021235 Validation loss: 0.0013377\n",
            "Epoch: 101 Train loss: 0.026767 Validation loss: 0.00037893\n",
            "Epoch: 102 Train loss: 0.027039 Validation loss: 0.00040934\n",
            "Epoch: 103 Train loss: 0.026639 Validation loss: 0.001254\n",
            "Epoch: 104 Train loss: 0.024625 Validation loss: 0.017957\n",
            "Epoch: 105 Train loss: 0.027695 Validation loss: 0.0025942\n",
            "Epoch: 106 Train loss: 0.020492 Validation loss: 2.1152e-07\n",
            "Epoch: 107 Train loss: 0.024952 Validation loss: 0.00079162\n",
            "Epoch: 108 Train loss: 0.02397 Validation loss: 0.0047181\n",
            "Epoch: 109 Train loss: 0.02194 Validation loss: 0.0028596\n",
            "Epoch: 110 Train loss: 0.02621 Validation loss: 0.013792\n",
            "Epoch: 111 Train loss: 0.025335 Validation loss: 0.0069118\n",
            "Epoch: 112 Train loss: 0.027655 Validation loss: 0.0056745\n",
            "Epoch: 113 Train loss: 0.020929 Validation loss: 6.4101e-06\n",
            "Epoch: 114 Train loss: 0.025901 Validation loss: 0.0018388\n",
            "Epoch: 115 Train loss: 0.024515 Validation loss: 0.0061966\n",
            "Epoch: 116 Train loss: 0.023069 Validation loss: 0.0027838\n",
            "Epoch: 117 Train loss: 0.027919 Validation loss: 0.0046941\n",
            "Epoch: 118 Train loss: 0.022689 Validation loss: 0.00087603\n",
            "Epoch: 119 Train loss: 0.02562 Validation loss: 0.0012744\n",
            "Epoch: 120 Train loss: 0.02492 Validation loss: 0.00032372\n",
            "Epoch: 121 Train loss: 0.023224 Validation loss: 0.0016016\n",
            "Epoch: 122 Train loss: 0.022866 Validation loss: 0.00042714\n",
            "Epoch: 123 Train loss: 0.025155 Validation loss: 0.0040835\n",
            "Epoch: 124 Train loss: 0.024589 Validation loss: 0.0011887\n",
            "Epoch: 125 Train loss: 0.022441 Validation loss: 0.0013819\n",
            "Epoch: 126 Train loss: 0.022082 Validation loss: 0.0091247\n",
            "Epoch: 127 Train loss: 0.022909 Validation loss: 0.0022825\n",
            "Epoch: 128 Train loss: 0.023814 Validation loss: 0.0035499\n",
            "Epoch: 129 Train loss: 0.021324 Validation loss: 0.00010082\n",
            "Epoch: 130 Train loss: 0.023763 Validation loss: 0.0019874\n",
            "Epoch: 131 Train loss: 0.023582 Validation loss: 0.0056882\n",
            "Epoch: 132 Train loss: 0.023062 Validation loss: 0.0013957\n",
            "Epoch: 133 Train loss: 0.023254 Validation loss: 0.0041753\n",
            "Epoch: 134 Train loss: 0.021385 Validation loss: 7.1897e-05\n",
            "Epoch: 135 Train loss: 0.020803 Validation loss: 0.0034334\n",
            "Epoch: 136 Train loss: 0.023211 Validation loss: 1.4263e-05\n",
            "Epoch: 137 Train loss: 0.023109 Validation loss: 0.002745\n",
            "Epoch: 138 Train loss: 0.021314 Validation loss: 0.0029578\n",
            "Epoch: 139 Train loss: 0.022546 Validation loss: 0.0023695\n",
            "Epoch: 140 Train loss: 0.021061 Validation loss: 0.00047608\n",
            "Epoch: 141 Train loss: 0.021427 Validation loss: 0.0038089\n",
            "Epoch: 142 Train loss: 0.023036 Validation loss: 5.3904e-06\n",
            "Epoch: 143 Train loss: 0.021976 Validation loss: 0.00010128\n",
            "Epoch: 144 Train loss: 0.020512 Validation loss: 0.00045043\n",
            "Epoch: 145 Train loss: 0.021424 Validation loss: 0.0020251\n",
            "Epoch: 146 Train loss: 0.020969 Validation loss: 0.0036112\n",
            "Epoch: 147 Train loss: 0.022027 Validation loss: 0.0034841\n",
            "Epoch: 148 Train loss: 0.02018 Validation loss: 0.0003494\n",
            "Epoch: 149 Train loss: 0.024043 Validation loss: 0.00038732\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DduaR_pCQ0k-",
        "colab_type": "text"
      },
      "source": [
        "After completion of the training process we use the test data set to test the models generalization to unseen data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ofClNnHRAUy",
        "colab_type": "code",
        "outputId": "d39e8581-ba80-4074-8706-eb0403ad44c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "for x_t, y_t in test_ds:\n",
        "    y_pred = mdl(x_t)\n",
        "    test_loss = tf.reduce_mean(tf.square(y_t-y_pred))\n",
        "print(\"Test loss: {:.5}\".format(test_loss))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.016434\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzJ7wOZmRbez",
        "colab_type": "text"
      },
      "source": [
        "After we have verified that our model achieves a similar loss on the test as on the validation and training data set, we can conclude that our model is not overfitting or underfitting and generalizes to unseen data. We can now predict on the inputs again and plot the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyvFN03bR8ez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = mdl(x)\n",
        "plt.plot(x, y)\n",
        "plt.plot(x, y_true)\n",
        "plt.plot(x, y_pred.numpy())\n",
        "plt.legend([\"Observation\", \"Target\", \"Prediction\"])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKmuAAmLSmiR",
        "colab_type": "text"
      },
      "source": [
        "Now our model has learned to approximate the function mapping from the input to the output. The capability of neural networks to learn from input-ouput pairs alone and approximate an arbitrary function, see universal approximation theorem, can be very useful if the mapping between the input and output is too complex to be captured with model based approaches. But learning from input-ouput pairs alone implies that the model will only be able to make accurate predictions over input ranges it has seen during training. In order to demonstrate this we will predict on an interval that exeeds the $\\left[0,3\\right]$ interval the model was trained on, i.e. we will predict on the interval $\\left[-2,5\\right]$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRF8hAmmVFcH",
        "colab_type": "code",
        "outputId": "29fd35ad-1110-4ca9-e964-30da9691a9ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "x_generalize = np.linspace(-2.0, 5.0, N_samples, dtype=np.float32)\n",
        "y_generalize = np.sin(1.0+x_generalize*x_generalize) + noise_sig*np.random.randn(N_samples).astype(np.float32)\n",
        "y_truey_generalize = np.sin(1.0+x_generalize*x_generalize)\n",
        "y_pred = mdl(x_generalize)\n",
        "plt.plot(x_generalize, y_generalize)\n",
        "plt.plot(x_generalize, y_truey_generalize)\n",
        "plt.plot(x_generalize, y_pred.numpy())\n",
        "plt.legend([\"Observation\", \"Target\", \"Prediction\"])\n",
        "plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2yr-0UCVIli",
        "colab_type": "text"
      },
      "source": [
        "As expected, the model is able to make farely accurate predictions on the interval it was trained on but makes unreliable predictions outside this interval.\n",
        "\n",
        "### Regularization\n",
        "In this section we will explore the concept of regularization. As there is no theorem that can be used to determine the required size and structure of a neural network given a certain task, one has to find a suitable neural architecture by trial and error. This can result in choosing a architecture with a capacity that is higher than required for solving the given task and hence overfitting might occur. A common way to prevent large neural networks networks from overfitting is to employ some sort of regularization, e.g. weight norm penalty, dropout, early stopping and data augmentation. In this section we will focus on weight norm penalty as a regularization and derive a probabilistic interpretation for some of those.\n",
        "\n",
        "We start by restating the conditional probability of the output $\\mathbf{y}$ given the input $\\mathbf{x}$ and the networks parameters $\\boldsymbol{\\theta}$\n",
        "\n",
        "$p(\\mathbf{y}\\vert\\mathbf{x},\\boldsymbol{\\theta})=\\dfrac{1}{\\sqrt{(2\\pi)^{M}\\sigma^{2}}}\\mathrm{e}^{-\\dfrac{1}{2\\sigma^{2}}\\Vert\\boldsymbol{\\mathbf{y}-g_{\\boldsymbol{\\theta}}(\\mathbf{x})}\\Vert_{2}^{2}}$,\n",
        "\n",
        "which we used to derive the log likelihood. If we have some prior knowledge about the parameters of the neural network, which is given by a pdf $p(\\boldsymbol{\\theta})$ over the weights, we can use Bayes theorem to derive a posterior distribution\n",
        "\n",
        "$p(\\boldsymbol{\\theta}\\vert\\mathbf{x},\\mathbf{y})=\\dfrac{p(\\boldsymbol{\\theta},\\mathbf{y}\\vert\\mathbf{x})}{p(\\mathbf{y})}=\\dfrac{p(\\mathbf{y}\\vert\\mathbf{x},\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{p(\\mathbf{y})}$\n",
        "\n",
        "where $p(\\boldsymbol{\\theta})$ is the  prior over the networks parameters. Similar to the derivation at the beginning of the exercise, we can use this posterior distribution to derive a cost function for training the neural network. In this case, however, we are not maximizing the log likelihood but the posterior distribution over the weights, hence this approach is called Maximum A Posteori (MAP) estimation of the parameters. Mathematically we can formulate this as\n",
        "\n",
        "$\\boldsymbol{\\theta}^{\\star}=\\arg\\max_{\\boldsymbol{\\theta}}\\mathbb{E}\\left[p(\\boldsymbol{\\theta}\\vert\\mathbf{x},\\mathbf{y})\\right]=\\arg\\max_{\\boldsymbol{\\theta}}\\mathbb{E}\\left[\\ln{p(\\boldsymbol{\\theta}\\vert\\mathbf{x},\\mathbf{y})}\\right]=\\arg\\max_{\\boldsymbol{\\theta}}\\ln{p(\\boldsymbol{\\theta})}+\\mathbb{E}\\left[\\ln{p(\\mathbf{y}\\vert\\mathbf{x},\\boldsymbol{\\theta})}\\right]$,\n",
        "\n",
        "where we used the fact that applying a strictly increasing function, e.g. $\\ln{}$, does not change the position of the maximum of a cost function, ignored $p(\\mathbf{y})$, since it is independent of the network parameters and dropped the expectation operator for $\\ln{p(\\boldsymbol{\\theta})}$, since it is not depending on the random variable $\\mathbf{x}$. Comparing the MAP estimate of the parameters with the ML estimate we derived above, shows that the only difference is the addition of $\\ln{p(\\boldsymbol{\\theta})}$. This term is the regularization, i.e. the weight norm penalty. Depending on the distribution over $\\boldsymbol{\\theta}$ it can have different forms. If we choose a standard normal distribution, i.e. $\\boldsymbol{\\theta\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})}$,  we get\n",
        "\n",
        "$\\boldsymbol{\\theta}^{\\star}=\\arg\\min_{\\boldsymbol{\\theta}}\\lambda\\Vert\\boldsymbol{\\theta}\\Vert_{2}^{2}+\\dfrac{1}{N_{D}}\\sum_{i=1}^{N_{D}}\\Vert\\boldsymbol{\\mathbf{y}_{i}-g_{\\boldsymbol{\\theta}}(\\mathbf{x}_{i})}\\Vert_{2}^{2}$,\n",
        "\n",
        "where we have made the same simplifications as for the ML estimation and also introduced the parameter $\\lambda=\\sigma^{2}$, which is used to control the strength of the regularization. This form of regularization is commonly known as $l_{2}$-norm or weight decay regularization. Choosing a prior where the weights follow an i.i.d laplacian distribution, i.e. $p(\\boldsymbol{\\theta})=\\prod_{j}\\dfrac{1}{2}\\mathrm{e}^{\\vert\\theta_{j}\\vert}$, leads to\n",
        "\n",
        "$\\boldsymbol{\\theta}^{\\star}=\\arg\\min_{\\boldsymbol{\\theta}}\\lambda\\Vert\\boldsymbol{\\theta}\\Vert_{1}+\\dfrac{1}{N_{D}}\\sum_{i=1}^{N_{D}}\\Vert\\boldsymbol{\\mathbf{y}_{i}-g_{\\boldsymbol{\\theta}}(\\mathbf{x}_{i})}\\Vert_{2}^{2}$,\n",
        "\n",
        "where the strength of the regularization is again controlled by $\\lambda=\\sigma^{2}$. this type of regularization is known as  $l_{1}$-norm and it has the property to induce sparsity in the parameters of the network.\n",
        "\n",
        "With this theoretical background on regularization we can now implement it and observe it's effects on the regression problem covered in this exercise. For this we will define a model with a high capacity and train it for a extended time to provoke overfitting. For this, we will increase the number of hidden neurons in both hidden layers to $100$ and $50$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2Fv2J-VK_vw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyBigModel(object):\n",
        "    def __init__(self):\n",
        "        # Create model variables\n",
        "        self.W0 = tf.Variable(tf.random.normal([1, 100]), name=\"W0\")\n",
        "        self.b0 = tf.Variable(tf.zeros(100), name=\"b0\")\n",
        "        self.W1 = tf.Variable(tf.random.normal([100, 50]), name=\"W1\")\n",
        "        self.b1 = tf.Variable(tf.zeros(50), name=\"b1\")\n",
        "        self.W2 = tf.Variable(tf.random.normal([50, 1]), name=\"W2\")\n",
        "        self.b2 = tf.Variable(tf.zeros(1), name=\"b2\")\n",
        "        self.trainable_variables = [self.W0, self.b0, self.W1, self.b1, self.W2, self.b2]\n",
        "        \n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        # Compute forward pass\n",
        "        output = tf.reshape(inputs, [-1, 1])\n",
        "        output = tf.nn.tanh(tf.add(tf.matmul(output, self.W0), self.b0))\n",
        "        output = tf.nn.tanh(tf.add(tf.matmul(output, self.W1), self.b1))\n",
        "        output = tf.add(tf.matmul(output, self.W2), self.b2)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1XpaZTNLV-p",
        "colab_type": "text"
      },
      "source": [
        "After creating one instance of this class we can again train it on our data set. We will also create a new optimizer for training this bigger model, since some optimizers adapt the learning rates for individual parameters during a training process and we do not want to train our bigger model with learning rates adopted from an earlier training run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7Yq-a1FLi0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "big_mdl = MyBigModel()\n",
        "big_opt = tf.optimizers.SGD(learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBrf0I68MnMy",
        "colab_type": "text"
      },
      "source": [
        "Now we are ready to train this bigger model using the same training step and training loop. In order to provoke overfitting we also reduce the number of samples in the training data set a lot, increase the batch size and train for a more epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz_lXM8LMwXo",
        "colab_type": "code",
        "outputId": "8a34e120-e26f-4b90-a727-63895eb00667",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "N_train_samples_overfit = 30\n",
        "N_epochs = 1000\n",
        "batch_size = 30\n",
        "\n",
        "sel_idx = np.arange(0, N_train_samples)\n",
        "sel_idx = np.random.choice(sel_idx, N_train_samples_overfit)\n",
        "x_train_overfit = x_train[sel_idx]\n",
        "y_train_overfit = y_train[sel_idx]\n",
        "\n",
        "train_overfit_ds = tf.data.Dataset.from_tensor_slices((x_train_overfit, y_train_overfit)).shuffle(N_train_samples_overfit).batch(batch_size).repeat()\n",
        "\n",
        "epoch = 0\n",
        "train_iters = 0\n",
        "train_loss = 0.0\n",
        "for x_t, y_t in train_overfit_ds:\n",
        "    train_loss += train_step(big_mdl, big_opt, x_t, y_t)\n",
        "    train_iters += 1\n",
        "    if (train_iters > int(N_train_samples/batch_size)):\n",
        "        for x_v, y_v in validation_ds:\n",
        "            y_pred = big_mdl(x_v)\n",
        "            validation_loss = tf.reduce_mean(tf.square(y_v-y_pred))\n",
        "        print(\"Epoch: {} Train loss: {:.5} Validation loss: {:.5}\".format(epoch, train_loss/train_iters, validation_loss))\n",
        "        train_iters = 0\n",
        "        train_loss = 0.0\n",
        "        epoch += 1\n",
        "    if (epoch == N_epochs):\n",
        "        break"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Train loss: 5.7738 Validation loss: 0.082186\n",
            "Epoch: 1 Train loss: 0.25607 Validation loss: 0.0080653\n",
            "Epoch: 2 Train loss: 0.23086 Validation loss: 0.0024606\n",
            "Epoch: 3 Train loss: 0.21058 Validation loss: 0.001497\n",
            "Epoch: 4 Train loss: 0.19845 Validation loss: 0.0008522\n",
            "Epoch: 5 Train loss: 0.18855 Validation loss: 0.00037877\n",
            "Epoch: 6 Train loss: 0.1792 Validation loss: 0.00013214\n",
            "Epoch: 7 Train loss: 0.17033 Validation loss: 2.75e-05\n",
            "Epoch: 8 Train loss: 0.16206 Validation loss: 1.5252e-08\n",
            "Epoch: 9 Train loss: 0.15452 Validation loss: 1.8802e-05\n",
            "Epoch: 10 Train loss: 0.1478 Validation loss: 7.0623e-05\n",
            "Epoch: 11 Train loss: 0.14194 Validation loss: 0.00014904\n",
            "Epoch: 12 Train loss: 0.13691 Validation loss: 0.00024932\n",
            "Epoch: 13 Train loss: 0.13264 Validation loss: 0.00036659\n",
            "Epoch: 14 Train loss: 0.12906 Validation loss: 0.00049501\n",
            "Epoch: 15 Train loss: 0.12606 Validation loss: 0.00062825\n",
            "Epoch: 16 Train loss: 0.12354 Validation loss: 0.00075988\n",
            "Epoch: 17 Train loss: 0.12142 Validation loss: 0.0008841\n",
            "Epoch: 18 Train loss: 0.1196 Validation loss: 0.00099627\n",
            "Epoch: 19 Train loss: 0.11803 Validation loss: 0.0010927\n",
            "Epoch: 20 Train loss: 0.11663 Validation loss: 0.0011714\n",
            "Epoch: 21 Train loss: 0.11538 Validation loss: 0.0012315\n",
            "Epoch: 22 Train loss: 0.11422 Validation loss: 0.0012731\n",
            "Epoch: 23 Train loss: 0.11312 Validation loss: 0.0012973\n",
            "Epoch: 24 Train loss: 0.11207 Validation loss: 0.0013055\n",
            "Epoch: 25 Train loss: 0.11103 Validation loss: 0.0013005\n",
            "Epoch: 26 Train loss: 0.10999 Validation loss: 0.001284\n",
            "Epoch: 27 Train loss: 0.10893 Validation loss: 0.0012589\n",
            "Epoch: 28 Train loss: 0.10782 Validation loss: 0.0012278\n",
            "Epoch: 29 Train loss: 0.10664 Validation loss: 0.0011937\n",
            "Epoch: 30 Train loss: 0.10534 Validation loss: 0.0011601\n",
            "Epoch: 31 Train loss: 0.10386 Validation loss: 0.0011326\n",
            "Epoch: 32 Train loss: 0.10208 Validation loss: 0.0011215\n",
            "Epoch: 33 Train loss: 0.099882 Validation loss: 0.0011519\n",
            "Epoch: 34 Train loss: 0.097264 Validation loss: 0.0012792\n",
            "Epoch: 35 Train loss: 0.094494 Validation loss: 0.0015736\n",
            "Epoch: 36 Train loss: 0.091885 Validation loss: 0.0020634\n",
            "Epoch: 37 Train loss: 0.089543 Validation loss: 0.0027391\n",
            "Epoch: 38 Train loss: 0.087457 Validation loss: 0.0035891\n",
            "Epoch: 39 Train loss: 0.0856 Validation loss: 0.0046006\n",
            "Epoch: 40 Train loss: 0.08395 Validation loss: 0.005744\n",
            "Epoch: 41 Train loss: 0.082488 Validation loss: 0.0069707\n",
            "Epoch: 42 Train loss: 0.081194 Validation loss: 0.0082208\n",
            "Epoch: 43 Train loss: 0.080047 Validation loss: 0.0094343\n",
            "Epoch: 44 Train loss: 0.079025 Validation loss: 0.010565\n",
            "Epoch: 45 Train loss: 0.078111 Validation loss: 0.011583\n",
            "Epoch: 46 Train loss: 0.077287 Validation loss: 0.012473\n",
            "Epoch: 47 Train loss: 0.076539 Validation loss: 0.013235\n",
            "Epoch: 48 Train loss: 0.075854 Validation loss: 0.013874\n",
            "Epoch: 49 Train loss: 0.075223 Validation loss: 0.014403\n",
            "Epoch: 50 Train loss: 0.074637 Validation loss: 0.014832\n",
            "Epoch: 51 Train loss: 0.074092 Validation loss: 0.015175\n",
            "Epoch: 52 Train loss: 0.073581 Validation loss: 0.015443\n",
            "Epoch: 53 Train loss: 0.073099 Validation loss: 0.015646\n",
            "Epoch: 54 Train loss: 0.072644 Validation loss: 0.015792\n",
            "Epoch: 55 Train loss: 0.072213 Validation loss: 0.01589\n",
            "Epoch: 56 Train loss: 0.071803 Validation loss: 0.015945\n",
            "Epoch: 57 Train loss: 0.071413 Validation loss: 0.015963\n",
            "Epoch: 58 Train loss: 0.071039 Validation loss: 0.015949\n",
            "Epoch: 59 Train loss: 0.070682 Validation loss: 0.015907\n",
            "Epoch: 60 Train loss: 0.070339 Validation loss: 0.015841\n",
            "Epoch: 61 Train loss: 0.07001 Validation loss: 0.015752\n",
            "Epoch: 62 Train loss: 0.069693 Validation loss: 0.015644\n",
            "Epoch: 63 Train loss: 0.069387 Validation loss: 0.01552\n",
            "Epoch: 64 Train loss: 0.069093 Validation loss: 0.015383\n",
            "Epoch: 65 Train loss: 0.068808 Validation loss: 0.015232\n",
            "Epoch: 66 Train loss: 0.068533 Validation loss: 0.01507\n",
            "Epoch: 67 Train loss: 0.068267 Validation loss: 0.014899\n",
            "Epoch: 68 Train loss: 0.068009 Validation loss: 0.014721\n",
            "Epoch: 69 Train loss: 0.067759 Validation loss: 0.014535\n",
            "Epoch: 70 Train loss: 0.067516 Validation loss: 0.014345\n",
            "Epoch: 71 Train loss: 0.06728 Validation loss: 0.01415\n",
            "Epoch: 72 Train loss: 0.067051 Validation loss: 0.013951\n",
            "Epoch: 73 Train loss: 0.066828 Validation loss: 0.013749\n",
            "Epoch: 74 Train loss: 0.06661 Validation loss: 0.013546\n",
            "Epoch: 75 Train loss: 0.066398 Validation loss: 0.013342\n",
            "Epoch: 76 Train loss: 0.066191 Validation loss: 0.013137\n",
            "Epoch: 77 Train loss: 0.065989 Validation loss: 0.012931\n",
            "Epoch: 78 Train loss: 0.065791 Validation loss: 0.012727\n",
            "Epoch: 79 Train loss: 0.065598 Validation loss: 0.012523\n",
            "Epoch: 80 Train loss: 0.065409 Validation loss: 0.01232\n",
            "Epoch: 81 Train loss: 0.065223 Validation loss: 0.012119\n",
            "Epoch: 82 Train loss: 0.065041 Validation loss: 0.011919\n",
            "Epoch: 83 Train loss: 0.064863 Validation loss: 0.011722\n",
            "Epoch: 84 Train loss: 0.064688 Validation loss: 0.011527\n",
            "Epoch: 85 Train loss: 0.064515 Validation loss: 0.011334\n",
            "Epoch: 86 Train loss: 0.064346 Validation loss: 0.011143\n",
            "Epoch: 87 Train loss: 0.064179 Validation loss: 0.010956\n",
            "Epoch: 88 Train loss: 0.064015 Validation loss: 0.010772\n",
            "Epoch: 89 Train loss: 0.063853 Validation loss: 0.010591\n",
            "Epoch: 90 Train loss: 0.063693 Validation loss: 0.010413\n",
            "Epoch: 91 Train loss: 0.063535 Validation loss: 0.010238\n",
            "Epoch: 92 Train loss: 0.063379 Validation loss: 0.010067\n",
            "Epoch: 93 Train loss: 0.063224 Validation loss: 0.0098993\n",
            "Epoch: 94 Train loss: 0.063071 Validation loss: 0.0097346\n",
            "Epoch: 95 Train loss: 0.06292 Validation loss: 0.0095734\n",
            "Epoch: 96 Train loss: 0.06277 Validation loss: 0.0094158\n",
            "Epoch: 97 Train loss: 0.062621 Validation loss: 0.0092618\n",
            "Epoch: 98 Train loss: 0.062473 Validation loss: 0.0091111\n",
            "Epoch: 99 Train loss: 0.062327 Validation loss: 0.008964\n",
            "Epoch: 100 Train loss: 0.062181 Validation loss: 0.0088209\n",
            "Epoch: 101 Train loss: 0.062035 Validation loss: 0.0086809\n",
            "Epoch: 102 Train loss: 0.061891 Validation loss: 0.0085443\n",
            "Epoch: 103 Train loss: 0.061747 Validation loss: 0.0084112\n",
            "Epoch: 104 Train loss: 0.061603 Validation loss: 0.0082816\n",
            "Epoch: 105 Train loss: 0.06146 Validation loss: 0.0081553\n",
            "Epoch: 106 Train loss: 0.061317 Validation loss: 0.0080329\n",
            "Epoch: 107 Train loss: 0.061174 Validation loss: 0.0079137\n",
            "Epoch: 108 Train loss: 0.061031 Validation loss: 0.0077986\n",
            "Epoch: 109 Train loss: 0.060887 Validation loss: 0.0076864\n",
            "Epoch: 110 Train loss: 0.060744 Validation loss: 0.0075776\n",
            "Epoch: 111 Train loss: 0.0606 Validation loss: 0.0074722\n",
            "Epoch: 112 Train loss: 0.060456 Validation loss: 0.0073702\n",
            "Epoch: 113 Train loss: 0.060312 Validation loss: 0.0072714\n",
            "Epoch: 114 Train loss: 0.060166 Validation loss: 0.0071758\n",
            "Epoch: 115 Train loss: 0.06002 Validation loss: 0.0070843\n",
            "Epoch: 116 Train loss: 0.059874 Validation loss: 0.0069954\n",
            "Epoch: 117 Train loss: 0.059726 Validation loss: 0.0069101\n",
            "Epoch: 118 Train loss: 0.059577 Validation loss: 0.0068285\n",
            "Epoch: 119 Train loss: 0.059427 Validation loss: 0.0067506\n",
            "Epoch: 120 Train loss: 0.059275 Validation loss: 0.0066759\n",
            "Epoch: 121 Train loss: 0.059122 Validation loss: 0.0066047\n",
            "Epoch: 122 Train loss: 0.058967 Validation loss: 0.0065371\n",
            "Epoch: 123 Train loss: 0.058811 Validation loss: 0.0064731\n",
            "Epoch: 124 Train loss: 0.058653 Validation loss: 0.0064126\n",
            "Epoch: 125 Train loss: 0.058492 Validation loss: 0.0063562\n",
            "Epoch: 126 Train loss: 0.058329 Validation loss: 0.0063035\n",
            "Epoch: 127 Train loss: 0.058164 Validation loss: 0.0062548\n",
            "Epoch: 128 Train loss: 0.057996 Validation loss: 0.0062103\n",
            "Epoch: 129 Train loss: 0.057825 Validation loss: 0.0061701\n",
            "Epoch: 130 Train loss: 0.057651 Validation loss: 0.0061339\n",
            "Epoch: 131 Train loss: 0.057474 Validation loss: 0.0061021\n",
            "Epoch: 132 Train loss: 0.057293 Validation loss: 0.0060749\n",
            "Epoch: 133 Train loss: 0.057108 Validation loss: 0.0060528\n",
            "Epoch: 134 Train loss: 0.056919 Validation loss: 0.0060356\n",
            "Epoch: 135 Train loss: 0.056726 Validation loss: 0.0060238\n",
            "Epoch: 136 Train loss: 0.056527 Validation loss: 0.0060179\n",
            "Epoch: 137 Train loss: 0.056323 Validation loss: 0.006018\n",
            "Epoch: 138 Train loss: 0.056114 Validation loss: 0.0060239\n",
            "Epoch: 139 Train loss: 0.055898 Validation loss: 0.0060368\n",
            "Epoch: 140 Train loss: 0.055676 Validation loss: 0.0060573\n",
            "Epoch: 141 Train loss: 0.055446 Validation loss: 0.0060856\n",
            "Epoch: 142 Train loss: 0.055208 Validation loss: 0.0061222\n",
            "Epoch: 143 Train loss: 0.054961 Validation loss: 0.0061678\n",
            "Epoch: 144 Train loss: 0.054705 Validation loss: 0.0062235\n",
            "Epoch: 145 Train loss: 0.054438 Validation loss: 0.0062896\n",
            "Epoch: 146 Train loss: 0.05416 Validation loss: 0.0063674\n",
            "Epoch: 147 Train loss: 0.053869 Validation loss: 0.0064584\n",
            "Epoch: 148 Train loss: 0.053563 Validation loss: 0.006564\n",
            "Epoch: 149 Train loss: 0.053242 Validation loss: 0.0066853\n",
            "Epoch: 150 Train loss: 0.052904 Validation loss: 0.006824\n",
            "Epoch: 151 Train loss: 0.052545 Validation loss: 0.0069827\n",
            "Epoch: 152 Train loss: 0.052165 Validation loss: 0.0071639\n",
            "Epoch: 153 Train loss: 0.051759 Validation loss: 0.0073708\n",
            "Epoch: 154 Train loss: 0.051324 Validation loss: 0.0076061\n",
            "Epoch: 155 Train loss: 0.050855 Validation loss: 0.0078741\n",
            "Epoch: 156 Train loss: 0.050348 Validation loss: 0.0081805\n",
            "Epoch: 157 Train loss: 0.049796 Validation loss: 0.0085309\n",
            "Epoch: 158 Train loss: 0.04919 Validation loss: 0.0089377\n",
            "Epoch: 159 Train loss: 0.048521 Validation loss: 0.0094972\n",
            "Epoch: 160 Train loss: 0.048008 Validation loss: 0.016194\n",
            "Epoch: 161 Train loss: 0.13675 Validation loss: 0.10678\n",
            "Epoch: 162 Train loss: 0.064426 Validation loss: 0.035223\n",
            "Epoch: 163 Train loss: 0.0648 Validation loss: 0.058664\n",
            "Epoch: 164 Train loss: 0.085777 Validation loss: 0.054639\n",
            "Epoch: 165 Train loss: 0.06858 Validation loss: 0.042553\n",
            "Epoch: 166 Train loss: 0.071474 Validation loss: 0.048432\n",
            "Epoch: 167 Train loss: 0.073137 Validation loss: 0.043975\n",
            "Epoch: 168 Train loss: 0.069766 Validation loss: 0.041903\n",
            "Epoch: 169 Train loss: 0.070825 Validation loss: 0.042433\n",
            "Epoch: 170 Train loss: 0.070572 Validation loss: 0.040804\n",
            "Epoch: 171 Train loss: 0.069929 Validation loss: 0.040267\n",
            "Epoch: 172 Train loss: 0.070176 Validation loss: 0.039937\n",
            "Epoch: 173 Train loss: 0.069956 Validation loss: 0.03927\n",
            "Epoch: 174 Train loss: 0.06985 Validation loss: 0.038948\n",
            "Epoch: 175 Train loss: 0.069867 Validation loss: 0.038574\n",
            "Epoch: 176 Train loss: 0.069774 Validation loss: 0.038202\n",
            "Epoch: 177 Train loss: 0.069754 Validation loss: 0.037908\n",
            "Epoch: 178 Train loss: 0.069731 Validation loss: 0.037594\n",
            "Epoch: 179 Train loss: 0.0697 Validation loss: 0.037307\n",
            "Epoch: 180 Train loss: 0.069689 Validation loss: 0.037031\n",
            "Epoch: 181 Train loss: 0.069672 Validation loss: 0.036757\n",
            "Epoch: 182 Train loss: 0.069663 Validation loss: 0.036497\n",
            "Epoch: 183 Train loss: 0.069655 Validation loss: 0.036241\n",
            "Epoch: 184 Train loss: 0.069651 Validation loss: 0.035992\n",
            "Epoch: 185 Train loss: 0.069651 Validation loss: 0.035749\n",
            "Epoch: 186 Train loss: 0.069652 Validation loss: 0.035512\n",
            "Epoch: 187 Train loss: 0.069656 Validation loss: 0.035282\n",
            "Epoch: 188 Train loss: 0.069664 Validation loss: 0.035058\n",
            "Epoch: 189 Train loss: 0.069675 Validation loss: 0.034843\n",
            "Epoch: 190 Train loss: 0.069689 Validation loss: 0.034633\n",
            "Epoch: 191 Train loss: 0.069706 Validation loss: 0.034432\n",
            "Epoch: 192 Train loss: 0.069725 Validation loss: 0.034239\n",
            "Epoch: 193 Train loss: 0.069748 Validation loss: 0.034056\n",
            "Epoch: 194 Train loss: 0.069776 Validation loss: 0.033883\n",
            "Epoch: 195 Train loss: 0.069806 Validation loss: 0.03372\n",
            "Epoch: 196 Train loss: 0.06984 Validation loss: 0.033567\n",
            "Epoch: 197 Train loss: 0.069877 Validation loss: 0.033425\n",
            "Epoch: 198 Train loss: 0.069919 Validation loss: 0.033298\n",
            "Epoch: 199 Train loss: 0.069965 Validation loss: 0.033181\n",
            "Epoch: 200 Train loss: 0.070014 Validation loss: 0.033078\n",
            "Epoch: 201 Train loss: 0.070068 Validation loss: 0.03299\n",
            "Epoch: 202 Train loss: 0.070128 Validation loss: 0.032916\n",
            "Epoch: 203 Train loss: 0.070191 Validation loss: 0.032856\n",
            "Epoch: 204 Train loss: 0.07026 Validation loss: 0.032813\n",
            "Epoch: 205 Train loss: 0.070335 Validation loss: 0.032783\n",
            "Epoch: 206 Train loss: 0.070414 Validation loss: 0.032771\n",
            "Epoch: 207 Train loss: 0.0705 Validation loss: 0.032774\n",
            "Epoch: 208 Train loss: 0.070591 Validation loss: 0.032795\n",
            "Epoch: 209 Train loss: 0.070691 Validation loss: 0.032832\n",
            "Epoch: 210 Train loss: 0.070796 Validation loss: 0.032886\n",
            "Epoch: 211 Train loss: 0.070909 Validation loss: 0.032958\n",
            "Epoch: 212 Train loss: 0.071031 Validation loss: 0.033049\n",
            "Epoch: 213 Train loss: 0.07116 Validation loss: 0.033156\n",
            "Epoch: 214 Train loss: 0.071299 Validation loss: 0.033283\n",
            "Epoch: 215 Train loss: 0.071448 Validation loss: 0.033428\n",
            "Epoch: 216 Train loss: 0.071608 Validation loss: 0.033593\n",
            "Epoch: 217 Train loss: 0.071779 Validation loss: 0.033777\n",
            "Epoch: 218 Train loss: 0.071962 Validation loss: 0.033982\n",
            "Epoch: 219 Train loss: 0.07216 Validation loss: 0.034206\n",
            "Epoch: 220 Train loss: 0.072371 Validation loss: 0.034453\n",
            "Epoch: 221 Train loss: 0.072599 Validation loss: 0.03472\n",
            "Epoch: 222 Train loss: 0.072845 Validation loss: 0.035011\n",
            "Epoch: 223 Train loss: 0.073111 Validation loss: 0.035327\n",
            "Epoch: 224 Train loss: 0.073398 Validation loss: 0.035667\n",
            "Epoch: 225 Train loss: 0.07371 Validation loss: 0.036036\n",
            "Epoch: 226 Train loss: 0.074048 Validation loss: 0.036431\n",
            "Epoch: 227 Train loss: 0.074417 Validation loss: 0.036857\n",
            "Epoch: 228 Train loss: 0.07482 Validation loss: 0.037317\n",
            "Epoch: 229 Train loss: 0.075262 Validation loss: 0.037813\n",
            "Epoch: 230 Train loss: 0.075748 Validation loss: 0.038349\n",
            "Epoch: 231 Train loss: 0.076285 Validation loss: 0.038927\n",
            "Epoch: 232 Train loss: 0.076882 Validation loss: 0.039555\n",
            "Epoch: 233 Train loss: 0.077548 Validation loss: 0.040239\n",
            "Epoch: 234 Train loss: 0.078297 Validation loss: 0.040987\n",
            "Epoch: 235 Train loss: 0.079143 Validation loss: 0.041807\n",
            "Epoch: 236 Train loss: 0.080108 Validation loss: 0.042714\n",
            "Epoch: 237 Train loss: 0.081217 Validation loss: 0.043723\n",
            "Epoch: 238 Train loss: 0.082508 Validation loss: 0.044858\n",
            "Epoch: 239 Train loss: 0.084028 Validation loss: 0.046145\n",
            "Epoch: 240 Train loss: 0.085837 Validation loss: 0.047621\n",
            "Epoch: 241 Train loss: 0.088024 Validation loss: 0.049336\n",
            "Epoch: 242 Train loss: 0.090691 Validation loss: 0.051346\n",
            "Epoch: 243 Train loss: 0.09393 Validation loss: 0.05368\n",
            "Epoch: 244 Train loss: 0.097609 Validation loss: 0.056177\n",
            "Epoch: 245 Train loss: 0.10038 Validation loss: 0.057682\n",
            "Epoch: 246 Train loss: 0.095898 Validation loss: 0.053515\n",
            "Epoch: 247 Train loss: 0.072117 Validation loss: 0.03878\n",
            "Epoch: 248 Train loss: 0.041408 Validation loss: 0.024982\n",
            "Epoch: 249 Train loss: 0.027257 Validation loss: 0.01906\n",
            "Epoch: 250 Train loss: 0.023817 Validation loss: 0.017802\n",
            "Epoch: 251 Train loss: 0.022876 Validation loss: 0.01825\n",
            "Epoch: 252 Train loss: 0.022305 Validation loss: 0.019014\n",
            "Epoch: 253 Train loss: 0.021833 Validation loss: 0.019692\n",
            "Epoch: 254 Train loss: 0.02142 Validation loss: 0.020249\n",
            "Epoch: 255 Train loss: 0.021049 Validation loss: 0.02072\n",
            "Epoch: 256 Train loss: 0.020709 Validation loss: 0.021139\n",
            "Epoch: 257 Train loss: 0.02039 Validation loss: 0.021524\n",
            "Epoch: 258 Train loss: 0.020087 Validation loss: 0.021889\n",
            "Epoch: 259 Train loss: 0.019797 Validation loss: 0.022239\n",
            "Epoch: 260 Train loss: 0.019517 Validation loss: 0.022577\n",
            "Epoch: 261 Train loss: 0.019245 Validation loss: 0.022907\n",
            "Epoch: 262 Train loss: 0.018979 Validation loss: 0.023229\n",
            "Epoch: 263 Train loss: 0.018718 Validation loss: 0.023546\n",
            "Epoch: 264 Train loss: 0.018462 Validation loss: 0.023854\n",
            "Epoch: 265 Train loss: 0.018211 Validation loss: 0.024156\n",
            "Epoch: 266 Train loss: 0.017963 Validation loss: 0.024451\n",
            "Epoch: 267 Train loss: 0.017719 Validation loss: 0.024739\n",
            "Epoch: 268 Train loss: 0.017479 Validation loss: 0.02502\n",
            "Epoch: 269 Train loss: 0.017242 Validation loss: 0.025294\n",
            "Epoch: 270 Train loss: 0.017008 Validation loss: 0.025561\n",
            "Epoch: 271 Train loss: 0.016777 Validation loss: 0.025822\n",
            "Epoch: 272 Train loss: 0.01655 Validation loss: 0.026075\n",
            "Epoch: 273 Train loss: 0.016326 Validation loss: 0.026321\n",
            "Epoch: 274 Train loss: 0.016105 Validation loss: 0.02656\n",
            "Epoch: 275 Train loss: 0.015886 Validation loss: 0.026791\n",
            "Epoch: 276 Train loss: 0.015672 Validation loss: 0.027017\n",
            "Epoch: 277 Train loss: 0.01546 Validation loss: 0.027233\n",
            "Epoch: 278 Train loss: 0.015251 Validation loss: 0.027443\n",
            "Epoch: 279 Train loss: 0.015045 Validation loss: 0.027646\n",
            "Epoch: 280 Train loss: 0.014842 Validation loss: 0.027842\n",
            "Epoch: 281 Train loss: 0.014642 Validation loss: 0.028031\n",
            "Epoch: 282 Train loss: 0.014446 Validation loss: 0.028211\n",
            "Epoch: 283 Train loss: 0.014252 Validation loss: 0.028384\n",
            "Epoch: 284 Train loss: 0.014061 Validation loss: 0.02855\n",
            "Epoch: 285 Train loss: 0.013873 Validation loss: 0.028708\n",
            "Epoch: 286 Train loss: 0.013688 Validation loss: 0.028857\n",
            "Epoch: 287 Train loss: 0.013506 Validation loss: 0.029\n",
            "Epoch: 288 Train loss: 0.013327 Validation loss: 0.029135\n",
            "Epoch: 289 Train loss: 0.013151 Validation loss: 0.02926\n",
            "Epoch: 290 Train loss: 0.012978 Validation loss: 0.029379\n",
            "Epoch: 291 Train loss: 0.012807 Validation loss: 0.029489\n",
            "Epoch: 292 Train loss: 0.012639 Validation loss: 0.02959\n",
            "Epoch: 293 Train loss: 0.012475 Validation loss: 0.029683\n",
            "Epoch: 294 Train loss: 0.012312 Validation loss: 0.029768\n",
            "Epoch: 295 Train loss: 0.012153 Validation loss: 0.029844\n",
            "Epoch: 296 Train loss: 0.011997 Validation loss: 0.029912\n",
            "Epoch: 297 Train loss: 0.011843 Validation loss: 0.029971\n",
            "Epoch: 298 Train loss: 0.011692 Validation loss: 0.03002\n",
            "Epoch: 299 Train loss: 0.011543 Validation loss: 0.030061\n",
            "Epoch: 300 Train loss: 0.011397 Validation loss: 0.030092\n",
            "Epoch: 301 Train loss: 0.011254 Validation loss: 0.030115\n",
            "Epoch: 302 Train loss: 0.011113 Validation loss: 0.030129\n",
            "Epoch: 303 Train loss: 0.010975 Validation loss: 0.030133\n",
            "Epoch: 304 Train loss: 0.01084 Validation loss: 0.03013\n",
            "Epoch: 305 Train loss: 0.010707 Validation loss: 0.030116\n",
            "Epoch: 306 Train loss: 0.010576 Validation loss: 0.030093\n",
            "Epoch: 307 Train loss: 0.010448 Validation loss: 0.03006\n",
            "Epoch: 308 Train loss: 0.010322 Validation loss: 0.030017\n",
            "Epoch: 309 Train loss: 0.010199 Validation loss: 0.029967\n",
            "Epoch: 310 Train loss: 0.010078 Validation loss: 0.029906\n",
            "Epoch: 311 Train loss: 0.0099589 Validation loss: 0.029835\n",
            "Epoch: 312 Train loss: 0.0098424 Validation loss: 0.029755\n",
            "Epoch: 313 Train loss: 0.0097283 Validation loss: 0.029667\n",
            "Epoch: 314 Train loss: 0.0096163 Validation loss: 0.029569\n",
            "Epoch: 315 Train loss: 0.0095065 Validation loss: 0.029462\n",
            "Epoch: 316 Train loss: 0.0093989 Validation loss: 0.029346\n",
            "Epoch: 317 Train loss: 0.0092934 Validation loss: 0.029222\n",
            "Epoch: 318 Train loss: 0.00919 Validation loss: 0.029088\n",
            "Epoch: 319 Train loss: 0.0090886 Validation loss: 0.028946\n",
            "Epoch: 320 Train loss: 0.0089893 Validation loss: 0.028794\n",
            "Epoch: 321 Train loss: 0.008892 Validation loss: 0.028634\n",
            "Epoch: 322 Train loss: 0.0087967 Validation loss: 0.028466\n",
            "Epoch: 323 Train loss: 0.0087032 Validation loss: 0.028289\n",
            "Epoch: 324 Train loss: 0.0086117 Validation loss: 0.028104\n",
            "Epoch: 325 Train loss: 0.0085221 Validation loss: 0.02791\n",
            "Epoch: 326 Train loss: 0.0084343 Validation loss: 0.027709\n",
            "Epoch: 327 Train loss: 0.0083483 Validation loss: 0.0275\n",
            "Epoch: 328 Train loss: 0.008264 Validation loss: 0.027283\n",
            "Epoch: 329 Train loss: 0.0081815 Validation loss: 0.027059\n",
            "Epoch: 330 Train loss: 0.0081008 Validation loss: 0.026826\n",
            "Epoch: 331 Train loss: 0.0080217 Validation loss: 0.026587\n",
            "Epoch: 332 Train loss: 0.0079442 Validation loss: 0.026341\n",
            "Epoch: 333 Train loss: 0.0078684 Validation loss: 0.026089\n",
            "Epoch: 334 Train loss: 0.0077942 Validation loss: 0.02583\n",
            "Epoch: 335 Train loss: 0.0077215 Validation loss: 0.025564\n",
            "Epoch: 336 Train loss: 0.0076503 Validation loss: 0.025293\n",
            "Epoch: 337 Train loss: 0.0075807 Validation loss: 0.025015\n",
            "Epoch: 338 Train loss: 0.0075125 Validation loss: 0.02473\n",
            "Epoch: 339 Train loss: 0.0074458 Validation loss: 0.024438\n",
            "Epoch: 340 Train loss: 0.0073805 Validation loss: 0.024134\n",
            "Epoch: 341 Train loss: 0.0073166 Validation loss: 0.023813\n",
            "Epoch: 342 Train loss: 0.0072541 Validation loss: 0.023458\n",
            "Epoch: 343 Train loss: 0.0071934 Validation loss: 0.023031\n",
            "Epoch: 344 Train loss: 0.0071364 Validation loss: 0.022435\n",
            "Epoch: 345 Train loss: 0.0070945 Validation loss: 0.02142\n",
            "Epoch: 346 Train loss: 0.0071422 Validation loss: 0.019344\n",
            "Epoch: 347 Train loss: 0.007789 Validation loss: 0.014838\n",
            "Epoch: 348 Train loss: 0.011823 Validation loss: 0.0075793\n",
            "Epoch: 349 Train loss: 0.020787 Validation loss: 0.0046814\n",
            "Epoch: 350 Train loss: 0.01814 Validation loss: 0.0079593\n",
            "Epoch: 351 Train loss: 0.011208 Validation loss: 0.012119\n",
            "Epoch: 352 Train loss: 0.0083407 Validation loss: 0.015138\n",
            "Epoch: 353 Train loss: 0.0073429 Validation loss: 0.016952\n",
            "Epoch: 354 Train loss: 0.0069458 Validation loss: 0.017906\n",
            "Epoch: 355 Train loss: 0.0067535 Validation loss: 0.018318\n",
            "Epoch: 356 Train loss: 0.0066397 Validation loss: 0.018402\n",
            "Epoch: 357 Train loss: 0.0065599 Validation loss: 0.018289\n",
            "Epoch: 358 Train loss: 0.0064967 Validation loss: 0.018058\n",
            "Epoch: 359 Train loss: 0.0064425 Validation loss: 0.017756\n",
            "Epoch: 360 Train loss: 0.0063938 Validation loss: 0.017409\n",
            "Epoch: 361 Train loss: 0.0063491 Validation loss: 0.01703\n",
            "Epoch: 362 Train loss: 0.0063076 Validation loss: 0.016626\n",
            "Epoch: 363 Train loss: 0.0062693 Validation loss: 0.016198\n",
            "Epoch: 364 Train loss: 0.0062348 Validation loss: 0.015742\n",
            "Epoch: 365 Train loss: 0.006205 Validation loss: 0.015252\n",
            "Epoch: 366 Train loss: 0.0061826 Validation loss: 0.014713\n",
            "Epoch: 367 Train loss: 0.0061718 Validation loss: 0.014107\n",
            "Epoch: 368 Train loss: 0.006181 Validation loss: 0.013408\n",
            "Epoch: 369 Train loss: 0.0062261 Validation loss: 0.012582\n",
            "Epoch: 370 Train loss: 0.0063361 Validation loss: 0.011594\n",
            "Epoch: 371 Train loss: 0.0065598 Validation loss: 0.010428\n",
            "Epoch: 372 Train loss: 0.0069623 Validation loss: 0.0091269\n",
            "Epoch: 373 Train loss: 0.0075751 Validation loss: 0.0078549\n",
            "Epoch: 374 Train loss: 0.0082688 Validation loss: 0.0068886\n",
            "Epoch: 375 Train loss: 0.0086973 Validation loss: 0.0064635\n",
            "Epoch: 376 Train loss: 0.0085849 Validation loss: 0.0065877\n",
            "Epoch: 377 Train loss: 0.0080506 Validation loss: 0.0070672\n",
            "Epoch: 378 Train loss: 0.0074155 Validation loss: 0.007667\n",
            "Epoch: 379 Train loss: 0.0068816 Validation loss: 0.0082222\n",
            "Epoch: 380 Train loss: 0.0064933 Validation loss: 0.0086575\n",
            "Epoch: 381 Train loss: 0.0062249 Validation loss: 0.0089544\n",
            "Epoch: 382 Train loss: 0.0060401 Validation loss: 0.0091259\n",
            "Epoch: 383 Train loss: 0.00591 Validation loss: 0.0091949\n",
            "Epoch: 384 Train loss: 0.0058155 Validation loss: 0.0091826\n",
            "Epoch: 385 Train loss: 0.0057445 Validation loss: 0.0091085\n",
            "Epoch: 386 Train loss: 0.0056892 Validation loss: 0.008989\n",
            "Epoch: 387 Train loss: 0.0056453 Validation loss: 0.0088334\n",
            "Epoch: 388 Train loss: 0.00561 Validation loss: 0.0086485\n",
            "Epoch: 389 Train loss: 0.0055819 Validation loss: 0.0084398\n",
            "Epoch: 390 Train loss: 0.0055604 Validation loss: 0.0082107\n",
            "Epoch: 391 Train loss: 0.0055457 Validation loss: 0.0079607\n",
            "Epoch: 392 Train loss: 0.0055385 Validation loss: 0.0076905\n",
            "Epoch: 393 Train loss: 0.0055404 Validation loss: 0.0073992\n",
            "Epoch: 394 Train loss: 0.0055534 Validation loss: 0.0070851\n",
            "Epoch: 395 Train loss: 0.0055809 Validation loss: 0.0067478\n",
            "Epoch: 396 Train loss: 0.0056264 Validation loss: 0.0063885\n",
            "Epoch: 397 Train loss: 0.0056934 Validation loss: 0.0060114\n",
            "Epoch: 398 Train loss: 0.0057843 Validation loss: 0.0056266\n",
            "Epoch: 399 Train loss: 0.0058973 Validation loss: 0.0052489\n",
            "Epoch: 400 Train loss: 0.006024 Validation loss: 0.0049014\n",
            "Epoch: 401 Train loss: 0.006146 Validation loss: 0.0046089\n",
            "Epoch: 402 Train loss: 0.0062382 Validation loss: 0.0043927\n",
            "Epoch: 403 Train loss: 0.0062767 Validation loss: 0.0042653\n",
            "Epoch: 404 Train loss: 0.0062503 Validation loss: 0.0042242\n",
            "Epoch: 405 Train loss: 0.0061648 Validation loss: 0.0042548\n",
            "Epoch: 406 Train loss: 0.0060388 Validation loss: 0.0043361\n",
            "Epoch: 407 Train loss: 0.0058952 Validation loss: 0.0044445\n",
            "Epoch: 408 Train loss: 0.0057526 Validation loss: 0.0045605\n",
            "Epoch: 409 Train loss: 0.0056222 Validation loss: 0.0046693\n",
            "Epoch: 410 Train loss: 0.0055087 Validation loss: 0.0047625\n",
            "Epoch: 411 Train loss: 0.0054129 Validation loss: 0.0048349\n",
            "Epoch: 412 Train loss: 0.0053334 Validation loss: 0.0048858\n",
            "Epoch: 413 Train loss: 0.0052678 Validation loss: 0.004916\n",
            "Epoch: 414 Train loss: 0.0052138 Validation loss: 0.0049267\n",
            "Epoch: 415 Train loss: 0.0051695 Validation loss: 0.0049198\n",
            "Epoch: 416 Train loss: 0.0051331 Validation loss: 0.004898\n",
            "Epoch: 417 Train loss: 0.0051035 Validation loss: 0.0048624\n",
            "Epoch: 418 Train loss: 0.0050798 Validation loss: 0.0048149\n",
            "Epoch: 419 Train loss: 0.0050614 Validation loss: 0.0047554\n",
            "Epoch: 420 Train loss: 0.0050482 Validation loss: 0.0046863\n",
            "Epoch: 421 Train loss: 0.0050399 Validation loss: 0.0046082\n",
            "Epoch: 422 Train loss: 0.0050366 Validation loss: 0.0045202\n",
            "Epoch: 423 Train loss: 0.0050387 Validation loss: 0.0044239\n",
            "Epoch: 424 Train loss: 0.0050461 Validation loss: 0.0043205\n",
            "Epoch: 425 Train loss: 0.0050593 Validation loss: 0.0042109\n",
            "Epoch: 426 Train loss: 0.0050783 Validation loss: 0.0040967\n",
            "Epoch: 427 Train loss: 0.0051028 Validation loss: 0.00398\n",
            "Epoch: 428 Train loss: 0.0051321 Validation loss: 0.0038645\n",
            "Epoch: 429 Train loss: 0.0051643 Validation loss: 0.0037535\n",
            "Epoch: 430 Train loss: 0.0051972 Validation loss: 0.003652\n",
            "Epoch: 431 Train loss: 0.0052274 Validation loss: 0.0035642\n",
            "Epoch: 432 Train loss: 0.005251 Validation loss: 0.0034946\n",
            "Epoch: 433 Train loss: 0.0052645 Validation loss: 0.0034457\n",
            "Epoch: 434 Train loss: 0.0052652 Validation loss: 0.0034199\n",
            "Epoch: 435 Train loss: 0.0052517 Validation loss: 0.0034177\n",
            "Epoch: 436 Train loss: 0.0052245 Validation loss: 0.003436\n",
            "Epoch: 437 Train loss: 0.005186 Validation loss: 0.0034722\n",
            "Epoch: 438 Train loss: 0.0051392 Validation loss: 0.003522\n",
            "Epoch: 439 Train loss: 0.0050875 Validation loss: 0.0035815\n",
            "Epoch: 440 Train loss: 0.0050339 Validation loss: 0.0036469\n",
            "Epoch: 441 Train loss: 0.004981 Validation loss: 0.0037138\n",
            "Epoch: 442 Train loss: 0.0049307 Validation loss: 0.0037802\n",
            "Epoch: 443 Train loss: 0.0048838 Validation loss: 0.003844\n",
            "Epoch: 444 Train loss: 0.0048412 Validation loss: 0.003902\n",
            "Epoch: 445 Train loss: 0.0048029 Validation loss: 0.0039556\n",
            "Epoch: 446 Train loss: 0.0047689 Validation loss: 0.0040031\n",
            "Epoch: 447 Train loss: 0.0047389 Validation loss: 0.0040442\n",
            "Epoch: 448 Train loss: 0.0047128 Validation loss: 0.0040793\n",
            "Epoch: 449 Train loss: 0.0046902 Validation loss: 0.0041082\n",
            "Epoch: 450 Train loss: 0.0046707 Validation loss: 0.0041314\n",
            "Epoch: 451 Train loss: 0.0046543 Validation loss: 0.0041489\n",
            "Epoch: 452 Train loss: 0.0046405 Validation loss: 0.0041609\n",
            "Epoch: 453 Train loss: 0.0046295 Validation loss: 0.0041677\n",
            "Epoch: 454 Train loss: 0.0046208 Validation loss: 0.0041706\n",
            "Epoch: 455 Train loss: 0.0046145 Validation loss: 0.0041688\n",
            "Epoch: 456 Train loss: 0.0046104 Validation loss: 0.0041629\n",
            "Epoch: 457 Train loss: 0.0046085 Validation loss: 0.004154\n",
            "Epoch: 458 Train loss: 0.0046086 Validation loss: 0.0041419\n",
            "Epoch: 459 Train loss: 0.0046106 Validation loss: 0.0041272\n",
            "Epoch: 460 Train loss: 0.0046143 Validation loss: 0.0041113\n",
            "Epoch: 461 Train loss: 0.0046193 Validation loss: 0.0040945\n",
            "Epoch: 462 Train loss: 0.0046254 Validation loss: 0.0040782\n",
            "Epoch: 463 Train loss: 0.0046321 Validation loss: 0.0040631\n",
            "Epoch: 464 Train loss: 0.0046386 Validation loss: 0.0040515\n",
            "Epoch: 465 Train loss: 0.0046445 Validation loss: 0.0040434\n",
            "Epoch: 466 Train loss: 0.004649 Validation loss: 0.0040412\n",
            "Epoch: 467 Train loss: 0.0046514 Validation loss: 0.0040452\n",
            "Epoch: 468 Train loss: 0.0046511 Validation loss: 0.0040565\n",
            "Epoch: 469 Train loss: 0.0046477 Validation loss: 0.0040761\n",
            "Epoch: 470 Train loss: 0.0046408 Validation loss: 0.0041036\n",
            "Epoch: 471 Train loss: 0.0046304 Validation loss: 0.0041395\n",
            "Epoch: 472 Train loss: 0.0046165 Validation loss: 0.0041839\n",
            "Epoch: 473 Train loss: 0.0045996 Validation loss: 0.0042349\n",
            "Epoch: 474 Train loss: 0.0045803 Validation loss: 0.0042927\n",
            "Epoch: 475 Train loss: 0.0045591 Validation loss: 0.0043553\n",
            "Epoch: 476 Train loss: 0.0045367 Validation loss: 0.0044226\n",
            "Epoch: 477 Train loss: 0.0045136 Validation loss: 0.0044922\n",
            "Epoch: 478 Train loss: 0.0044905 Validation loss: 0.0045647\n",
            "Epoch: 479 Train loss: 0.0044677 Validation loss: 0.0046377\n",
            "Epoch: 480 Train loss: 0.0044456 Validation loss: 0.0047115\n",
            "Epoch: 481 Train loss: 0.0044244 Validation loss: 0.0047845\n",
            "Epoch: 482 Train loss: 0.0044043 Validation loss: 0.0048572\n",
            "Epoch: 483 Train loss: 0.0043854 Validation loss: 0.0049287\n",
            "Epoch: 484 Train loss: 0.0043677 Validation loss: 0.0049979\n",
            "Epoch: 485 Train loss: 0.0043516 Validation loss: 0.0050649\n",
            "Epoch: 486 Train loss: 0.0043367 Validation loss: 0.0051301\n",
            "Epoch: 487 Train loss: 0.0043231 Validation loss: 0.0051937\n",
            "Epoch: 488 Train loss: 0.0043107 Validation loss: 0.0052542\n",
            "Epoch: 489 Train loss: 0.0042995 Validation loss: 0.0053126\n",
            "Epoch: 490 Train loss: 0.0042894 Validation loss: 0.0053693\n",
            "Epoch: 491 Train loss: 0.0042803 Validation loss: 0.0054232\n",
            "Epoch: 492 Train loss: 0.0042722 Validation loss: 0.0054756\n",
            "Epoch: 493 Train loss: 0.004265 Validation loss: 0.005526\n",
            "Epoch: 494 Train loss: 0.0042588 Validation loss: 0.0055741\n",
            "Epoch: 495 Train loss: 0.0042535 Validation loss: 0.0056204\n",
            "Epoch: 496 Train loss: 0.0042488 Validation loss: 0.0056658\n",
            "Epoch: 497 Train loss: 0.004245 Validation loss: 0.0057097\n",
            "Epoch: 498 Train loss: 0.0042418 Validation loss: 0.0057523\n",
            "Epoch: 499 Train loss: 0.0042392 Validation loss: 0.0057941\n",
            "Epoch: 500 Train loss: 0.004237 Validation loss: 0.0058359\n",
            "Epoch: 501 Train loss: 0.0042354 Validation loss: 0.0058773\n",
            "Epoch: 502 Train loss: 0.004234 Validation loss: 0.0059189\n",
            "Epoch: 503 Train loss: 0.0042327 Validation loss: 0.0059618\n",
            "Epoch: 504 Train loss: 0.0042316 Validation loss: 0.0060046\n",
            "Epoch: 505 Train loss: 0.0042305 Validation loss: 0.0060496\n",
            "Epoch: 506 Train loss: 0.0042293 Validation loss: 0.0060955\n",
            "Epoch: 507 Train loss: 0.0042277 Validation loss: 0.0061443\n",
            "Epoch: 508 Train loss: 0.0042257 Validation loss: 0.0061948\n",
            "Epoch: 509 Train loss: 0.0042234 Validation loss: 0.0062482\n",
            "Epoch: 510 Train loss: 0.0042204 Validation loss: 0.0063052\n",
            "Epoch: 511 Train loss: 0.0042166 Validation loss: 0.0063654\n",
            "Epoch: 512 Train loss: 0.0042121 Validation loss: 0.0064293\n",
            "Epoch: 513 Train loss: 0.0042069 Validation loss: 0.0064966\n",
            "Epoch: 514 Train loss: 0.0042007 Validation loss: 0.0065683\n",
            "Epoch: 515 Train loss: 0.0041939 Validation loss: 0.0066431\n",
            "Epoch: 516 Train loss: 0.0041863 Validation loss: 0.0067209\n",
            "Epoch: 517 Train loss: 0.004178 Validation loss: 0.0068031\n",
            "Epoch: 518 Train loss: 0.0041691 Validation loss: 0.0068883\n",
            "Epoch: 519 Train loss: 0.0041597 Validation loss: 0.0069767\n",
            "Epoch: 520 Train loss: 0.00415 Validation loss: 0.0070675\n",
            "Epoch: 521 Train loss: 0.0041399 Validation loss: 0.0071607\n",
            "Epoch: 522 Train loss: 0.0041297 Validation loss: 0.0072565\n",
            "Epoch: 523 Train loss: 0.0041194 Validation loss: 0.007354\n",
            "Epoch: 524 Train loss: 0.0041091 Validation loss: 0.0074535\n",
            "Epoch: 525 Train loss: 0.0040988 Validation loss: 0.0075532\n",
            "Epoch: 526 Train loss: 0.0040887 Validation loss: 0.0076548\n",
            "Epoch: 527 Train loss: 0.0040788 Validation loss: 0.0077562\n",
            "Epoch: 528 Train loss: 0.0040692 Validation loss: 0.007858\n",
            "Epoch: 529 Train loss: 0.00406 Validation loss: 0.0079605\n",
            "Epoch: 530 Train loss: 0.004051 Validation loss: 0.0080634\n",
            "Epoch: 531 Train loss: 0.0040424 Validation loss: 0.0081663\n",
            "Epoch: 532 Train loss: 0.0040341 Validation loss: 0.0082691\n",
            "Epoch: 533 Train loss: 0.0040261 Validation loss: 0.0083718\n",
            "Epoch: 534 Train loss: 0.0040185 Validation loss: 0.0084741\n",
            "Epoch: 535 Train loss: 0.0040113 Validation loss: 0.0085764\n",
            "Epoch: 536 Train loss: 0.0040044 Validation loss: 0.0086781\n",
            "Epoch: 537 Train loss: 0.0039978 Validation loss: 0.00878\n",
            "Epoch: 538 Train loss: 0.0039916 Validation loss: 0.0088813\n",
            "Epoch: 539 Train loss: 0.0039857 Validation loss: 0.0089816\n",
            "Epoch: 540 Train loss: 0.0039801 Validation loss: 0.0090826\n",
            "Epoch: 541 Train loss: 0.0039748 Validation loss: 0.0091829\n",
            "Epoch: 542 Train loss: 0.0039697 Validation loss: 0.0092828\n",
            "Epoch: 543 Train loss: 0.0039649 Validation loss: 0.009383\n",
            "Epoch: 544 Train loss: 0.0039603 Validation loss: 0.0094823\n",
            "Epoch: 545 Train loss: 0.0039559 Validation loss: 0.0095819\n",
            "Epoch: 546 Train loss: 0.0039518 Validation loss: 0.0096811\n",
            "Epoch: 547 Train loss: 0.0039478 Validation loss: 0.0097802\n",
            "Epoch: 548 Train loss: 0.0039441 Validation loss: 0.0098792\n",
            "Epoch: 549 Train loss: 0.0039405 Validation loss: 0.0099783\n",
            "Epoch: 550 Train loss: 0.0039371 Validation loss: 0.010078\n",
            "Epoch: 551 Train loss: 0.0039338 Validation loss: 0.010177\n",
            "Epoch: 552 Train loss: 0.0039306 Validation loss: 0.010277\n",
            "Epoch: 553 Train loss: 0.0039276 Validation loss: 0.010377\n",
            "Epoch: 554 Train loss: 0.0039247 Validation loss: 0.010477\n",
            "Epoch: 555 Train loss: 0.0039219 Validation loss: 0.010578\n",
            "Epoch: 556 Train loss: 0.003919 Validation loss: 0.01068\n",
            "Epoch: 557 Train loss: 0.0039162 Validation loss: 0.010783\n",
            "Epoch: 558 Train loss: 0.0039135 Validation loss: 0.010886\n",
            "Epoch: 559 Train loss: 0.0039108 Validation loss: 0.01099\n",
            "Epoch: 560 Train loss: 0.0039081 Validation loss: 0.011095\n",
            "Epoch: 561 Train loss: 0.0039055 Validation loss: 0.0112\n",
            "Epoch: 562 Train loss: 0.0039029 Validation loss: 0.011307\n",
            "Epoch: 563 Train loss: 0.0039001 Validation loss: 0.011416\n",
            "Epoch: 564 Train loss: 0.0038974 Validation loss: 0.011526\n",
            "Epoch: 565 Train loss: 0.0038946 Validation loss: 0.011637\n",
            "Epoch: 566 Train loss: 0.0038916 Validation loss: 0.01175\n",
            "Epoch: 567 Train loss: 0.0038887 Validation loss: 0.011864\n",
            "Epoch: 568 Train loss: 0.0038856 Validation loss: 0.01198\n",
            "Epoch: 569 Train loss: 0.0038824 Validation loss: 0.012098\n",
            "Epoch: 570 Train loss: 0.0038791 Validation loss: 0.012218\n",
            "Epoch: 571 Train loss: 0.0038758 Validation loss: 0.012338\n",
            "Epoch: 572 Train loss: 0.0038722 Validation loss: 0.012461\n",
            "Epoch: 573 Train loss: 0.0038687 Validation loss: 0.012586\n",
            "Epoch: 574 Train loss: 0.003865 Validation loss: 0.012713\n",
            "Epoch: 575 Train loss: 0.0038613 Validation loss: 0.012841\n",
            "Epoch: 576 Train loss: 0.0038574 Validation loss: 0.01297\n",
            "Epoch: 577 Train loss: 0.0038535 Validation loss: 0.013102\n",
            "Epoch: 578 Train loss: 0.0038496 Validation loss: 0.013235\n",
            "Epoch: 579 Train loss: 0.0038456 Validation loss: 0.01337\n",
            "Epoch: 580 Train loss: 0.0038415 Validation loss: 0.013507\n",
            "Epoch: 581 Train loss: 0.0038374 Validation loss: 0.013645\n",
            "Epoch: 582 Train loss: 0.0038333 Validation loss: 0.013784\n",
            "Epoch: 583 Train loss: 0.0038291 Validation loss: 0.013925\n",
            "Epoch: 584 Train loss: 0.0038249 Validation loss: 0.014067\n",
            "Epoch: 585 Train loss: 0.0038208 Validation loss: 0.01421\n",
            "Epoch: 586 Train loss: 0.0038166 Validation loss: 0.014356\n",
            "Epoch: 587 Train loss: 0.0038124 Validation loss: 0.014501\n",
            "Epoch: 588 Train loss: 0.0038084 Validation loss: 0.014648\n",
            "Epoch: 589 Train loss: 0.0038043 Validation loss: 0.014797\n",
            "Epoch: 590 Train loss: 0.0038002 Validation loss: 0.014946\n",
            "Epoch: 591 Train loss: 0.0037962 Validation loss: 0.015097\n",
            "Epoch: 592 Train loss: 0.0037923 Validation loss: 0.015246\n",
            "Epoch: 593 Train loss: 0.0037885 Validation loss: 0.015397\n",
            "Epoch: 594 Train loss: 0.0037847 Validation loss: 0.01555\n",
            "Epoch: 595 Train loss: 0.003781 Validation loss: 0.015703\n",
            "Epoch: 596 Train loss: 0.0037773 Validation loss: 0.015857\n",
            "Epoch: 597 Train loss: 0.0037737 Validation loss: 0.016011\n",
            "Epoch: 598 Train loss: 0.0037701 Validation loss: 0.016167\n",
            "Epoch: 599 Train loss: 0.0037666 Validation loss: 0.016322\n",
            "Epoch: 600 Train loss: 0.0037631 Validation loss: 0.01648\n",
            "Epoch: 601 Train loss: 0.0037597 Validation loss: 0.016637\n",
            "Epoch: 602 Train loss: 0.0037564 Validation loss: 0.016795\n",
            "Epoch: 603 Train loss: 0.0037531 Validation loss: 0.016954\n",
            "Epoch: 604 Train loss: 0.0037499 Validation loss: 0.017113\n",
            "Epoch: 605 Train loss: 0.0037468 Validation loss: 0.017272\n",
            "Epoch: 606 Train loss: 0.0037437 Validation loss: 0.017432\n",
            "Epoch: 607 Train loss: 0.0037408 Validation loss: 0.017592\n",
            "Epoch: 608 Train loss: 0.0037379 Validation loss: 0.017752\n",
            "Epoch: 609 Train loss: 0.003735 Validation loss: 0.017914\n",
            "Epoch: 610 Train loss: 0.0037322 Validation loss: 0.018075\n",
            "Epoch: 611 Train loss: 0.0037295 Validation loss: 0.018237\n",
            "Epoch: 612 Train loss: 0.0037268 Validation loss: 0.018399\n",
            "Epoch: 613 Train loss: 0.0037242 Validation loss: 0.018562\n",
            "Epoch: 614 Train loss: 0.0037216 Validation loss: 0.018725\n",
            "Epoch: 615 Train loss: 0.0037191 Validation loss: 0.018888\n",
            "Epoch: 616 Train loss: 0.0037166 Validation loss: 0.019052\n",
            "Epoch: 617 Train loss: 0.0037142 Validation loss: 0.019216\n",
            "Epoch: 618 Train loss: 0.0037118 Validation loss: 0.019381\n",
            "Epoch: 619 Train loss: 0.0037094 Validation loss: 0.019547\n",
            "Epoch: 620 Train loss: 0.0037071 Validation loss: 0.019713\n",
            "Epoch: 621 Train loss: 0.0037049 Validation loss: 0.019878\n",
            "Epoch: 622 Train loss: 0.0037026 Validation loss: 0.020045\n",
            "Epoch: 623 Train loss: 0.0037005 Validation loss: 0.020211\n",
            "Epoch: 624 Train loss: 0.0036983 Validation loss: 0.020378\n",
            "Epoch: 625 Train loss: 0.0036962 Validation loss: 0.020546\n",
            "Epoch: 626 Train loss: 0.0036941 Validation loss: 0.020714\n",
            "Epoch: 627 Train loss: 0.0036921 Validation loss: 0.020884\n",
            "Epoch: 628 Train loss: 0.0036901 Validation loss: 0.021051\n",
            "Epoch: 629 Train loss: 0.0036881 Validation loss: 0.021221\n",
            "Epoch: 630 Train loss: 0.0036862 Validation loss: 0.021391\n",
            "Epoch: 631 Train loss: 0.0036843 Validation loss: 0.021561\n",
            "Epoch: 632 Train loss: 0.0036824 Validation loss: 0.02173\n",
            "Epoch: 633 Train loss: 0.0036806 Validation loss: 0.021901\n",
            "Epoch: 634 Train loss: 0.0036788 Validation loss: 0.022071\n",
            "Epoch: 635 Train loss: 0.003677 Validation loss: 0.022244\n",
            "Epoch: 636 Train loss: 0.0036752 Validation loss: 0.022415\n",
            "Epoch: 637 Train loss: 0.0036735 Validation loss: 0.022587\n",
            "Epoch: 638 Train loss: 0.0036718 Validation loss: 0.022759\n",
            "Epoch: 639 Train loss: 0.0036701 Validation loss: 0.022932\n",
            "Epoch: 640 Train loss: 0.0036684 Validation loss: 0.023105\n",
            "Epoch: 641 Train loss: 0.0036668 Validation loss: 0.023277\n",
            "Epoch: 642 Train loss: 0.0036652 Validation loss: 0.023452\n",
            "Epoch: 643 Train loss: 0.0036636 Validation loss: 0.023626\n",
            "Epoch: 644 Train loss: 0.0036621 Validation loss: 0.0238\n",
            "Epoch: 645 Train loss: 0.0036606 Validation loss: 0.023974\n",
            "Epoch: 646 Train loss: 0.0036591 Validation loss: 0.024148\n",
            "Epoch: 647 Train loss: 0.0036576 Validation loss: 0.024323\n",
            "Epoch: 648 Train loss: 0.0036562 Validation loss: 0.024498\n",
            "Epoch: 649 Train loss: 0.0036547 Validation loss: 0.024674\n",
            "Epoch: 650 Train loss: 0.0036533 Validation loss: 0.024849\n",
            "Epoch: 651 Train loss: 0.0036519 Validation loss: 0.025024\n",
            "Epoch: 652 Train loss: 0.0036505 Validation loss: 0.025198\n",
            "Epoch: 653 Train loss: 0.0036492 Validation loss: 0.025374\n",
            "Epoch: 654 Train loss: 0.0036479 Validation loss: 0.025549\n",
            "Epoch: 655 Train loss: 0.0036466 Validation loss: 0.025724\n",
            "Epoch: 656 Train loss: 0.0036453 Validation loss: 0.0259\n",
            "Epoch: 657 Train loss: 0.003644 Validation loss: 0.026075\n",
            "Epoch: 658 Train loss: 0.0036427 Validation loss: 0.026251\n",
            "Epoch: 659 Train loss: 0.0036415 Validation loss: 0.026426\n",
            "Epoch: 660 Train loss: 0.0036403 Validation loss: 0.026601\n",
            "Epoch: 661 Train loss: 0.0036391 Validation loss: 0.026776\n",
            "Epoch: 662 Train loss: 0.0036379 Validation loss: 0.026951\n",
            "Epoch: 663 Train loss: 0.0036368 Validation loss: 0.027127\n",
            "Epoch: 664 Train loss: 0.0036356 Validation loss: 0.027301\n",
            "Epoch: 665 Train loss: 0.0036345 Validation loss: 0.027477\n",
            "Epoch: 666 Train loss: 0.0036334 Validation loss: 0.027651\n",
            "Epoch: 667 Train loss: 0.0036323 Validation loss: 0.027826\n",
            "Epoch: 668 Train loss: 0.0036312 Validation loss: 0.028001\n",
            "Epoch: 669 Train loss: 0.0036301 Validation loss: 0.028176\n",
            "Epoch: 670 Train loss: 0.0036291 Validation loss: 0.02835\n",
            "Epoch: 671 Train loss: 0.003628 Validation loss: 0.028524\n",
            "Epoch: 672 Train loss: 0.003627 Validation loss: 0.028697\n",
            "Epoch: 673 Train loss: 0.003626 Validation loss: 0.028871\n",
            "Epoch: 674 Train loss: 0.003625 Validation loss: 0.029044\n",
            "Epoch: 675 Train loss: 0.003624 Validation loss: 0.029217\n",
            "Epoch: 676 Train loss: 0.003623 Validation loss: 0.029391\n",
            "Epoch: 677 Train loss: 0.0036221 Validation loss: 0.029564\n",
            "Epoch: 678 Train loss: 0.0036211 Validation loss: 0.029736\n",
            "Epoch: 679 Train loss: 0.0036202 Validation loss: 0.029907\n",
            "Epoch: 680 Train loss: 0.0036192 Validation loss: 0.030078\n",
            "Epoch: 681 Train loss: 0.0036183 Validation loss: 0.03025\n",
            "Epoch: 682 Train loss: 0.0036174 Validation loss: 0.030422\n",
            "Epoch: 683 Train loss: 0.0036165 Validation loss: 0.030592\n",
            "Epoch: 684 Train loss: 0.0036155 Validation loss: 0.030761\n",
            "Epoch: 685 Train loss: 0.0036147 Validation loss: 0.030931\n",
            "Epoch: 686 Train loss: 0.0036138 Validation loss: 0.0311\n",
            "Epoch: 687 Train loss: 0.0036129 Validation loss: 0.031269\n",
            "Epoch: 688 Train loss: 0.003612 Validation loss: 0.031439\n",
            "Epoch: 689 Train loss: 0.0036111 Validation loss: 0.031606\n",
            "Epoch: 690 Train loss: 0.0036103 Validation loss: 0.031774\n",
            "Epoch: 691 Train loss: 0.0036094 Validation loss: 0.031943\n",
            "Epoch: 692 Train loss: 0.0036086 Validation loss: 0.032111\n",
            "Epoch: 693 Train loss: 0.0036077 Validation loss: 0.032277\n",
            "Epoch: 694 Train loss: 0.0036069 Validation loss: 0.032444\n",
            "Epoch: 695 Train loss: 0.0036061 Validation loss: 0.03261\n",
            "Epoch: 696 Train loss: 0.0036052 Validation loss: 0.032775\n",
            "Epoch: 697 Train loss: 0.0036044 Validation loss: 0.032942\n",
            "Epoch: 698 Train loss: 0.0036036 Validation loss: 0.033109\n",
            "Epoch: 699 Train loss: 0.0036028 Validation loss: 0.033273\n",
            "Epoch: 700 Train loss: 0.003602 Validation loss: 0.033438\n",
            "Epoch: 701 Train loss: 0.0036012 Validation loss: 0.033603\n",
            "Epoch: 702 Train loss: 0.0036004 Validation loss: 0.033767\n",
            "Epoch: 703 Train loss: 0.0035996 Validation loss: 0.033929\n",
            "Epoch: 704 Train loss: 0.0035988 Validation loss: 0.034092\n",
            "Epoch: 705 Train loss: 0.0035981 Validation loss: 0.034255\n",
            "Epoch: 706 Train loss: 0.0035973 Validation loss: 0.034418\n",
            "Epoch: 707 Train loss: 0.0035965 Validation loss: 0.034579\n",
            "Epoch: 708 Train loss: 0.0035957 Validation loss: 0.034742\n",
            "Epoch: 709 Train loss: 0.003595 Validation loss: 0.034902\n",
            "Epoch: 710 Train loss: 0.0035942 Validation loss: 0.035062\n",
            "Epoch: 711 Train loss: 0.0035935 Validation loss: 0.035222\n",
            "Epoch: 712 Train loss: 0.0035927 Validation loss: 0.035382\n",
            "Epoch: 713 Train loss: 0.003592 Validation loss: 0.035541\n",
            "Epoch: 714 Train loss: 0.0035913 Validation loss: 0.035701\n",
            "Epoch: 715 Train loss: 0.0035905 Validation loss: 0.03586\n",
            "Epoch: 716 Train loss: 0.0035898 Validation loss: 0.036018\n",
            "Epoch: 717 Train loss: 0.0035891 Validation loss: 0.036176\n",
            "Epoch: 718 Train loss: 0.0035884 Validation loss: 0.036333\n",
            "Epoch: 719 Train loss: 0.0035876 Validation loss: 0.036491\n",
            "Epoch: 720 Train loss: 0.0035869 Validation loss: 0.036649\n",
            "Epoch: 721 Train loss: 0.0035862 Validation loss: 0.036806\n",
            "Epoch: 722 Train loss: 0.0035855 Validation loss: 0.036961\n",
            "Epoch: 723 Train loss: 0.0035848 Validation loss: 0.037118\n",
            "Epoch: 724 Train loss: 0.0035841 Validation loss: 0.037275\n",
            "Epoch: 725 Train loss: 0.0035834 Validation loss: 0.037432\n",
            "Epoch: 726 Train loss: 0.0035827 Validation loss: 0.037588\n",
            "Epoch: 727 Train loss: 0.003582 Validation loss: 0.037744\n",
            "Epoch: 728 Train loss: 0.0035813 Validation loss: 0.037899\n",
            "Epoch: 729 Train loss: 0.0035807 Validation loss: 0.038052\n",
            "Epoch: 730 Train loss: 0.00358 Validation loss: 0.038206\n",
            "Epoch: 731 Train loss: 0.0035793 Validation loss: 0.038359\n",
            "Epoch: 732 Train loss: 0.0035786 Validation loss: 0.038511\n",
            "Epoch: 733 Train loss: 0.003578 Validation loss: 0.038664\n",
            "Epoch: 734 Train loss: 0.0035773 Validation loss: 0.038815\n",
            "Epoch: 735 Train loss: 0.0035766 Validation loss: 0.038967\n",
            "Epoch: 736 Train loss: 0.003576 Validation loss: 0.039118\n",
            "Epoch: 737 Train loss: 0.0035753 Validation loss: 0.039268\n",
            "Epoch: 738 Train loss: 0.0035747 Validation loss: 0.039419\n",
            "Epoch: 739 Train loss: 0.003574 Validation loss: 0.039569\n",
            "Epoch: 740 Train loss: 0.0035734 Validation loss: 0.039717\n",
            "Epoch: 741 Train loss: 0.0035727 Validation loss: 0.039866\n",
            "Epoch: 742 Train loss: 0.0035721 Validation loss: 0.040014\n",
            "Epoch: 743 Train loss: 0.0035715 Validation loss: 0.040161\n",
            "Epoch: 744 Train loss: 0.0035708 Validation loss: 0.040308\n",
            "Epoch: 745 Train loss: 0.0035702 Validation loss: 0.040455\n",
            "Epoch: 746 Train loss: 0.0035696 Validation loss: 0.040601\n",
            "Epoch: 747 Train loss: 0.0035689 Validation loss: 0.040747\n",
            "Epoch: 748 Train loss: 0.0035683 Validation loss: 0.040892\n",
            "Epoch: 749 Train loss: 0.0035677 Validation loss: 0.041038\n",
            "Epoch: 750 Train loss: 0.0035671 Validation loss: 0.041183\n",
            "Epoch: 751 Train loss: 0.0035665 Validation loss: 0.041327\n",
            "Epoch: 752 Train loss: 0.0035658 Validation loss: 0.041471\n",
            "Epoch: 753 Train loss: 0.0035652 Validation loss: 0.041613\n",
            "Epoch: 754 Train loss: 0.0035646 Validation loss: 0.041756\n",
            "Epoch: 755 Train loss: 0.003564 Validation loss: 0.041899\n",
            "Epoch: 756 Train loss: 0.0035634 Validation loss: 0.042042\n",
            "Epoch: 757 Train loss: 0.0035628 Validation loss: 0.042183\n",
            "Epoch: 758 Train loss: 0.0035622 Validation loss: 0.042325\n",
            "Epoch: 759 Train loss: 0.0035616 Validation loss: 0.042466\n",
            "Epoch: 760 Train loss: 0.003561 Validation loss: 0.042605\n",
            "Epoch: 761 Train loss: 0.0035604 Validation loss: 0.042745\n",
            "Epoch: 762 Train loss: 0.0035599 Validation loss: 0.042884\n",
            "Epoch: 763 Train loss: 0.0035593 Validation loss: 0.043023\n",
            "Epoch: 764 Train loss: 0.0035587 Validation loss: 0.043162\n",
            "Epoch: 765 Train loss: 0.0035581 Validation loss: 0.0433\n",
            "Epoch: 766 Train loss: 0.0035575 Validation loss: 0.043437\n",
            "Epoch: 767 Train loss: 0.0035569 Validation loss: 0.043574\n",
            "Epoch: 768 Train loss: 0.0035564 Validation loss: 0.043711\n",
            "Epoch: 769 Train loss: 0.0035558 Validation loss: 0.043845\n",
            "Epoch: 770 Train loss: 0.0035552 Validation loss: 0.043981\n",
            "Epoch: 771 Train loss: 0.0035547 Validation loss: 0.044115\n",
            "Epoch: 772 Train loss: 0.0035541 Validation loss: 0.04425\n",
            "Epoch: 773 Train loss: 0.0035535 Validation loss: 0.044383\n",
            "Epoch: 774 Train loss: 0.003553 Validation loss: 0.044516\n",
            "Epoch: 775 Train loss: 0.0035524 Validation loss: 0.044649\n",
            "Epoch: 776 Train loss: 0.0035518 Validation loss: 0.044781\n",
            "Epoch: 777 Train loss: 0.0035513 Validation loss: 0.044912\n",
            "Epoch: 778 Train loss: 0.0035507 Validation loss: 0.045044\n",
            "Epoch: 779 Train loss: 0.0035502 Validation loss: 0.045174\n",
            "Epoch: 780 Train loss: 0.0035496 Validation loss: 0.045304\n",
            "Epoch: 781 Train loss: 0.0035491 Validation loss: 0.045433\n",
            "Epoch: 782 Train loss: 0.0035485 Validation loss: 0.045561\n",
            "Epoch: 783 Train loss: 0.003548 Validation loss: 0.045688\n",
            "Epoch: 784 Train loss: 0.0035474 Validation loss: 0.045817\n",
            "Epoch: 785 Train loss: 0.0035469 Validation loss: 0.045943\n",
            "Epoch: 786 Train loss: 0.0035464 Validation loss: 0.046071\n",
            "Epoch: 787 Train loss: 0.0035458 Validation loss: 0.046197\n",
            "Epoch: 788 Train loss: 0.0035453 Validation loss: 0.046322\n",
            "Epoch: 789 Train loss: 0.0035447 Validation loss: 0.046447\n",
            "Epoch: 790 Train loss: 0.0035442 Validation loss: 0.046572\n",
            "Epoch: 791 Train loss: 0.0035437 Validation loss: 0.046696\n",
            "Epoch: 792 Train loss: 0.0035432 Validation loss: 0.046819\n",
            "Epoch: 793 Train loss: 0.0035426 Validation loss: 0.046942\n",
            "Epoch: 794 Train loss: 0.0035421 Validation loss: 0.047065\n",
            "Epoch: 795 Train loss: 0.0035416 Validation loss: 0.047188\n",
            "Epoch: 796 Train loss: 0.003541 Validation loss: 0.04731\n",
            "Epoch: 797 Train loss: 0.0035405 Validation loss: 0.047431\n",
            "Epoch: 798 Train loss: 0.00354 Validation loss: 0.047552\n",
            "Epoch: 799 Train loss: 0.0035395 Validation loss: 0.047673\n",
            "Epoch: 800 Train loss: 0.003539 Validation loss: 0.047792\n",
            "Epoch: 801 Train loss: 0.0035384 Validation loss: 0.047913\n",
            "Epoch: 802 Train loss: 0.0035379 Validation loss: 0.048032\n",
            "Epoch: 803 Train loss: 0.0035374 Validation loss: 0.048151\n",
            "Epoch: 804 Train loss: 0.0035369 Validation loss: 0.04827\n",
            "Epoch: 805 Train loss: 0.0035364 Validation loss: 0.048388\n",
            "Epoch: 806 Train loss: 0.0035359 Validation loss: 0.048505\n",
            "Epoch: 807 Train loss: 0.0035354 Validation loss: 0.048622\n",
            "Epoch: 808 Train loss: 0.0035349 Validation loss: 0.048739\n",
            "Epoch: 809 Train loss: 0.0035343 Validation loss: 0.048855\n",
            "Epoch: 810 Train loss: 0.0035339 Validation loss: 0.048972\n",
            "Epoch: 811 Train loss: 0.0035333 Validation loss: 0.049087\n",
            "Epoch: 812 Train loss: 0.0035328 Validation loss: 0.049202\n",
            "Epoch: 813 Train loss: 0.0035323 Validation loss: 0.049317\n",
            "Epoch: 814 Train loss: 0.0035318 Validation loss: 0.04943\n",
            "Epoch: 815 Train loss: 0.0035313 Validation loss: 0.049544\n",
            "Epoch: 816 Train loss: 0.0035308 Validation loss: 0.049657\n",
            "Epoch: 817 Train loss: 0.0035303 Validation loss: 0.04977\n",
            "Epoch: 818 Train loss: 0.0035298 Validation loss: 0.049882\n",
            "Epoch: 819 Train loss: 0.0035294 Validation loss: 0.049994\n",
            "Epoch: 820 Train loss: 0.0035289 Validation loss: 0.050107\n",
            "Epoch: 821 Train loss: 0.0035284 Validation loss: 0.050217\n",
            "Epoch: 822 Train loss: 0.0035279 Validation loss: 0.050327\n",
            "Epoch: 823 Train loss: 0.0035274 Validation loss: 0.050439\n",
            "Epoch: 824 Train loss: 0.0035269 Validation loss: 0.050547\n",
            "Epoch: 825 Train loss: 0.0035264 Validation loss: 0.050658\n",
            "Epoch: 826 Train loss: 0.0035259 Validation loss: 0.050767\n",
            "Epoch: 827 Train loss: 0.0035254 Validation loss: 0.050875\n",
            "Epoch: 828 Train loss: 0.003525 Validation loss: 0.050983\n",
            "Epoch: 829 Train loss: 0.0035245 Validation loss: 0.05109\n",
            "Epoch: 830 Train loss: 0.003524 Validation loss: 0.051198\n",
            "Epoch: 831 Train loss: 0.0035235 Validation loss: 0.051305\n",
            "Epoch: 832 Train loss: 0.003523 Validation loss: 0.051411\n",
            "Epoch: 833 Train loss: 0.0035225 Validation loss: 0.051517\n",
            "Epoch: 834 Train loss: 0.0035221 Validation loss: 0.051622\n",
            "Epoch: 835 Train loss: 0.0035216 Validation loss: 0.051726\n",
            "Epoch: 836 Train loss: 0.0035211 Validation loss: 0.051831\n",
            "Epoch: 837 Train loss: 0.0035206 Validation loss: 0.051935\n",
            "Epoch: 838 Train loss: 0.0035201 Validation loss: 0.052039\n",
            "Epoch: 839 Train loss: 0.0035197 Validation loss: 0.052142\n",
            "Epoch: 840 Train loss: 0.0035192 Validation loss: 0.052245\n",
            "Epoch: 841 Train loss: 0.0035187 Validation loss: 0.052349\n",
            "Epoch: 842 Train loss: 0.0035183 Validation loss: 0.05245\n",
            "Epoch: 843 Train loss: 0.0035178 Validation loss: 0.052551\n",
            "Epoch: 844 Train loss: 0.0035173 Validation loss: 0.052652\n",
            "Epoch: 845 Train loss: 0.0035169 Validation loss: 0.052753\n",
            "Epoch: 846 Train loss: 0.0035164 Validation loss: 0.052854\n",
            "Epoch: 847 Train loss: 0.0035159 Validation loss: 0.052953\n",
            "Epoch: 848 Train loss: 0.0035155 Validation loss: 0.053055\n",
            "Epoch: 849 Train loss: 0.003515 Validation loss: 0.053153\n",
            "Epoch: 850 Train loss: 0.0035145 Validation loss: 0.053254\n",
            "Epoch: 851 Train loss: 0.0035141 Validation loss: 0.053351\n",
            "Epoch: 852 Train loss: 0.0035136 Validation loss: 0.05345\n",
            "Epoch: 853 Train loss: 0.0035131 Validation loss: 0.053547\n",
            "Epoch: 854 Train loss: 0.0035127 Validation loss: 0.053645\n",
            "Epoch: 855 Train loss: 0.0035122 Validation loss: 0.053743\n",
            "Epoch: 856 Train loss: 0.0035118 Validation loss: 0.053839\n",
            "Epoch: 857 Train loss: 0.0035113 Validation loss: 0.053935\n",
            "Epoch: 858 Train loss: 0.0035108 Validation loss: 0.054032\n",
            "Epoch: 859 Train loss: 0.0035104 Validation loss: 0.054127\n",
            "Epoch: 860 Train loss: 0.0035099 Validation loss: 0.054223\n",
            "Epoch: 861 Train loss: 0.0035095 Validation loss: 0.054318\n",
            "Epoch: 862 Train loss: 0.003509 Validation loss: 0.054413\n",
            "Epoch: 863 Train loss: 0.0035085 Validation loss: 0.054507\n",
            "Epoch: 864 Train loss: 0.0035081 Validation loss: 0.054602\n",
            "Epoch: 865 Train loss: 0.0035076 Validation loss: 0.054696\n",
            "Epoch: 866 Train loss: 0.0035072 Validation loss: 0.05479\n",
            "Epoch: 867 Train loss: 0.0035067 Validation loss: 0.054884\n",
            "Epoch: 868 Train loss: 0.0035063 Validation loss: 0.054978\n",
            "Epoch: 869 Train loss: 0.0035058 Validation loss: 0.05507\n",
            "Epoch: 870 Train loss: 0.0035054 Validation loss: 0.055163\n",
            "Epoch: 871 Train loss: 0.0035049 Validation loss: 0.055256\n",
            "Epoch: 872 Train loss: 0.0035045 Validation loss: 0.055348\n",
            "Epoch: 873 Train loss: 0.003504 Validation loss: 0.05544\n",
            "Epoch: 874 Train loss: 0.0035036 Validation loss: 0.05553\n",
            "Epoch: 875 Train loss: 0.0035031 Validation loss: 0.055622\n",
            "Epoch: 876 Train loss: 0.0035027 Validation loss: 0.055712\n",
            "Epoch: 877 Train loss: 0.0035022 Validation loss: 0.055803\n",
            "Epoch: 878 Train loss: 0.0035018 Validation loss: 0.055894\n",
            "Epoch: 879 Train loss: 0.0035013 Validation loss: 0.055983\n",
            "Epoch: 880 Train loss: 0.0035009 Validation loss: 0.056073\n",
            "Epoch: 881 Train loss: 0.0035005 Validation loss: 0.056163\n",
            "Epoch: 882 Train loss: 0.0035 Validation loss: 0.056252\n",
            "Epoch: 883 Train loss: 0.0034996 Validation loss: 0.056342\n",
            "Epoch: 884 Train loss: 0.0034991 Validation loss: 0.056431\n",
            "Epoch: 885 Train loss: 0.0034987 Validation loss: 0.056519\n",
            "Epoch: 886 Train loss: 0.0034982 Validation loss: 0.056606\n",
            "Epoch: 887 Train loss: 0.0034978 Validation loss: 0.056694\n",
            "Epoch: 888 Train loss: 0.0034974 Validation loss: 0.056782\n",
            "Epoch: 889 Train loss: 0.0034969 Validation loss: 0.05687\n",
            "Epoch: 890 Train loss: 0.0034965 Validation loss: 0.056956\n",
            "Epoch: 891 Train loss: 0.003496 Validation loss: 0.057043\n",
            "Epoch: 892 Train loss: 0.0034956 Validation loss: 0.057129\n",
            "Epoch: 893 Train loss: 0.0034952 Validation loss: 0.057216\n",
            "Epoch: 894 Train loss: 0.0034947 Validation loss: 0.057302\n",
            "Epoch: 895 Train loss: 0.0034943 Validation loss: 0.057388\n",
            "Epoch: 896 Train loss: 0.0034939 Validation loss: 0.057473\n",
            "Epoch: 897 Train loss: 0.0034934 Validation loss: 0.057558\n",
            "Epoch: 898 Train loss: 0.003493 Validation loss: 0.057643\n",
            "Epoch: 899 Train loss: 0.0034926 Validation loss: 0.057728\n",
            "Epoch: 900 Train loss: 0.0034921 Validation loss: 0.057813\n",
            "Epoch: 901 Train loss: 0.0034917 Validation loss: 0.057897\n",
            "Epoch: 902 Train loss: 0.0034913 Validation loss: 0.057981\n",
            "Epoch: 903 Train loss: 0.0034908 Validation loss: 0.058065\n",
            "Epoch: 904 Train loss: 0.0034904 Validation loss: 0.058148\n",
            "Epoch: 905 Train loss: 0.00349 Validation loss: 0.058232\n",
            "Epoch: 906 Train loss: 0.0034895 Validation loss: 0.058315\n",
            "Epoch: 907 Train loss: 0.0034891 Validation loss: 0.058398\n",
            "Epoch: 908 Train loss: 0.0034887 Validation loss: 0.058482\n",
            "Epoch: 909 Train loss: 0.0034882 Validation loss: 0.058564\n",
            "Epoch: 910 Train loss: 0.0034878 Validation loss: 0.058646\n",
            "Epoch: 911 Train loss: 0.0034874 Validation loss: 0.05873\n",
            "Epoch: 912 Train loss: 0.003487 Validation loss: 0.058812\n",
            "Epoch: 913 Train loss: 0.0034865 Validation loss: 0.058893\n",
            "Epoch: 914 Train loss: 0.0034861 Validation loss: 0.058975\n",
            "Epoch: 915 Train loss: 0.0034857 Validation loss: 0.059057\n",
            "Epoch: 916 Train loss: 0.0034852 Validation loss: 0.059138\n",
            "Epoch: 917 Train loss: 0.0034848 Validation loss: 0.05922\n",
            "Epoch: 918 Train loss: 0.0034844 Validation loss: 0.059301\n",
            "Epoch: 919 Train loss: 0.003484 Validation loss: 0.059382\n",
            "Epoch: 920 Train loss: 0.0034835 Validation loss: 0.059465\n",
            "Epoch: 921 Train loss: 0.0034831 Validation loss: 0.059545\n",
            "Epoch: 922 Train loss: 0.0034827 Validation loss: 0.059624\n",
            "Epoch: 923 Train loss: 0.0034823 Validation loss: 0.059705\n",
            "Epoch: 924 Train loss: 0.0034818 Validation loss: 0.059785\n",
            "Epoch: 925 Train loss: 0.0034814 Validation loss: 0.059865\n",
            "Epoch: 926 Train loss: 0.003481 Validation loss: 0.059945\n",
            "Epoch: 927 Train loss: 0.0034806 Validation loss: 0.060025\n",
            "Epoch: 928 Train loss: 0.0034802 Validation loss: 0.060104\n",
            "Epoch: 929 Train loss: 0.0034797 Validation loss: 0.060184\n",
            "Epoch: 930 Train loss: 0.0034793 Validation loss: 0.060263\n",
            "Epoch: 931 Train loss: 0.0034789 Validation loss: 0.060342\n",
            "Epoch: 932 Train loss: 0.0034785 Validation loss: 0.060419\n",
            "Epoch: 933 Train loss: 0.003478 Validation loss: 0.060498\n",
            "Epoch: 934 Train loss: 0.0034776 Validation loss: 0.060577\n",
            "Epoch: 935 Train loss: 0.0034772 Validation loss: 0.060655\n",
            "Epoch: 936 Train loss: 0.0034768 Validation loss: 0.060732\n",
            "Epoch: 937 Train loss: 0.0034764 Validation loss: 0.060808\n",
            "Epoch: 938 Train loss: 0.003476 Validation loss: 0.060886\n",
            "Epoch: 939 Train loss: 0.0034755 Validation loss: 0.060963\n",
            "Epoch: 940 Train loss: 0.0034751 Validation loss: 0.06104\n",
            "Epoch: 941 Train loss: 0.0034747 Validation loss: 0.061117\n",
            "Epoch: 942 Train loss: 0.0034743 Validation loss: 0.061194\n",
            "Epoch: 943 Train loss: 0.0034739 Validation loss: 0.061271\n",
            "Epoch: 944 Train loss: 0.0034734 Validation loss: 0.061348\n",
            "Epoch: 945 Train loss: 0.003473 Validation loss: 0.061425\n",
            "Epoch: 946 Train loss: 0.0034726 Validation loss: 0.061502\n",
            "Epoch: 947 Train loss: 0.0034722 Validation loss: 0.061577\n",
            "Epoch: 948 Train loss: 0.0034718 Validation loss: 0.061653\n",
            "Epoch: 949 Train loss: 0.0034714 Validation loss: 0.061729\n",
            "Epoch: 950 Train loss: 0.003471 Validation loss: 0.061805\n",
            "Epoch: 951 Train loss: 0.0034705 Validation loss: 0.061881\n",
            "Epoch: 952 Train loss: 0.0034701 Validation loss: 0.061957\n",
            "Epoch: 953 Train loss: 0.0034697 Validation loss: 0.062032\n",
            "Epoch: 954 Train loss: 0.0034693 Validation loss: 0.062107\n",
            "Epoch: 955 Train loss: 0.0034689 Validation loss: 0.062183\n",
            "Epoch: 956 Train loss: 0.0034685 Validation loss: 0.062257\n",
            "Epoch: 957 Train loss: 0.003468 Validation loss: 0.062334\n",
            "Epoch: 958 Train loss: 0.0034676 Validation loss: 0.062409\n",
            "Epoch: 959 Train loss: 0.0034672 Validation loss: 0.062483\n",
            "Epoch: 960 Train loss: 0.0034668 Validation loss: 0.062557\n",
            "Epoch: 961 Train loss: 0.0034664 Validation loss: 0.062632\n",
            "Epoch: 962 Train loss: 0.003466 Validation loss: 0.062706\n",
            "Epoch: 963 Train loss: 0.0034656 Validation loss: 0.062779\n",
            "Epoch: 964 Train loss: 0.0034652 Validation loss: 0.062854\n",
            "Epoch: 965 Train loss: 0.0034647 Validation loss: 0.062926\n",
            "Epoch: 966 Train loss: 0.0034643 Validation loss: 0.063001\n",
            "Epoch: 967 Train loss: 0.0034639 Validation loss: 0.063072\n",
            "Epoch: 968 Train loss: 0.0034635 Validation loss: 0.063146\n",
            "Epoch: 969 Train loss: 0.0034631 Validation loss: 0.06322\n",
            "Epoch: 970 Train loss: 0.0034627 Validation loss: 0.063292\n",
            "Epoch: 971 Train loss: 0.0034623 Validation loss: 0.063367\n",
            "Epoch: 972 Train loss: 0.0034619 Validation loss: 0.063438\n",
            "Epoch: 973 Train loss: 0.0034615 Validation loss: 0.063512\n",
            "Epoch: 974 Train loss: 0.0034611 Validation loss: 0.063586\n",
            "Epoch: 975 Train loss: 0.0034607 Validation loss: 0.063658\n",
            "Epoch: 976 Train loss: 0.0034602 Validation loss: 0.063731\n",
            "Epoch: 977 Train loss: 0.0034598 Validation loss: 0.063803\n",
            "Epoch: 978 Train loss: 0.0034594 Validation loss: 0.063876\n",
            "Epoch: 979 Train loss: 0.003459 Validation loss: 0.063949\n",
            "Epoch: 980 Train loss: 0.0034586 Validation loss: 0.064021\n",
            "Epoch: 981 Train loss: 0.0034582 Validation loss: 0.064094\n",
            "Epoch: 982 Train loss: 0.0034578 Validation loss: 0.064166\n",
            "Epoch: 983 Train loss: 0.0034574 Validation loss: 0.064238\n",
            "Epoch: 984 Train loss: 0.003457 Validation loss: 0.06431\n",
            "Epoch: 985 Train loss: 0.0034566 Validation loss: 0.06438\n",
            "Epoch: 986 Train loss: 0.0034562 Validation loss: 0.064452\n",
            "Epoch: 987 Train loss: 0.0034558 Validation loss: 0.064523\n",
            "Epoch: 988 Train loss: 0.0034554 Validation loss: 0.064594\n",
            "Epoch: 989 Train loss: 0.003455 Validation loss: 0.064665\n",
            "Epoch: 990 Train loss: 0.0034545 Validation loss: 0.064736\n",
            "Epoch: 991 Train loss: 0.0034541 Validation loss: 0.064809\n",
            "Epoch: 992 Train loss: 0.0034537 Validation loss: 0.064878\n",
            "Epoch: 993 Train loss: 0.0034533 Validation loss: 0.064949\n",
            "Epoch: 994 Train loss: 0.0034529 Validation loss: 0.06502\n",
            "Epoch: 995 Train loss: 0.0034525 Validation loss: 0.06509\n",
            "Epoch: 996 Train loss: 0.0034521 Validation loss: 0.06516\n",
            "Epoch: 997 Train loss: 0.0034517 Validation loss: 0.065229\n",
            "Epoch: 998 Train loss: 0.0034513 Validation loss: 0.0653\n",
            "Epoch: 999 Train loss: 0.0034509 Validation loss: 0.065371\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkaUDmA8Ps6w",
        "colab_type": "text"
      },
      "source": [
        "Predicting with this model shows overfitting. For recognizing overfitting a comparison of the validation and training loss is very useful. If the training loss decreases during training while the validation loss consistently increases, the model you are training is probably overfitting. Plotting the models prediction and the target also shows that there is a significant discrepancy between the target and the prediction of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Sq-9xhWPxI4",
        "colab_type": "code",
        "outputId": "fe97d3db-e7d2-43fd-8ea9-3c1530aeec89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "y_pred = big_mdl(x)\n",
        "plt.scatter(x_train_overfit, y_train_overfit)\n",
        "plt.plot(x, y_true)\n",
        "plt.plot(x, y_pred.numpy())\n",
        "plt.legend([\"Target\", \"Prediction\", \"Training samples\"])\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdd1hUV/rA8e+ZYehVQEA6YgcFxd5L1HQ1cZO4iTGbuptks8nGrOa3m03Z3SRrkk1PNoluYnqxpGlMYu+KgmJDqgpIEel9Zu7vjwsIAiplmBnmfJ6HZ+DeO9x3FF7OnPIeoSgKkiRJUs+nMXcAkiRJUveQCV+SJMlGyIQvSZJkI2TClyRJshEy4UuSJNkIO3MH0BYfHx8lLCzM3GFIkiRZlQMHDpxTFMW3tXMWm/DDwsKIj483dxiSJElWRQhxqq1zsktHkiTJRsiEL0mSZCNkwpckSbIRMuFLkiTZCJnwJUmSbIRM+JIkSTZCJnxJkiQb0fMSflURbHkRsg+aOxJJkiSLYrELrzpMaGHLv0CjhcDh5o5GkiTJYvS8Fr6jO3iGQt4Rc0ciSZJkUXpewgfwi4K8o+aOQpIkyaL00IQ/BApToa7K3JFIkiRZjJ6b8BUjFCSbOxJJkiSL0TMTfq9w9bEo06xhSJIkWZKemfC9wtTH4jarhEqSJNmcnpnwHT3AyUu28CVJkpromQkf1Fa+TPiSJEmNZMKXJEmyET1vpW0DzxA4/gMYjaAx79+1tQnZLNuQTE5xFX08nVg8awBzYgPNGpMkSban5yZ890Aw1kFlIbi2up9vt1ibkM3S1UlU1RkAyC6uYunqJACZ9CVJ6lY9t0vHLUB9LMsxaxjLNiQ3JvsGVXUGlm2QawQkSepePTfhu9e3nkvNm/Bziltf7dvWcUmSJFPpwQm/voVv5oTfx9OpXcclSZJMpef24bv6qaWSzZzwF88a0KwPH8BJp2XxrAGQsQ2SvoHCNLB3gfBJMPwOdR2BJElSF+u5CV+jVZN+2VmzhtEwMNt0ls6TU/259sRiSP4R7N3AP1qdQpqyAbYtg2tfhuibzRq3JEk9T5ckfCHECuA6IF9RlKhWzgvgNeAaoBJYpCiK6bekcu8Dpdkmv83lzIkNvDAjp7wAVsyC4tMw42kY8wewc1DPnT0E656AVXdDbhLMeJq1iTlySqckmZitTJ3uqhb+h8CbwMo2zl8N9Kv/GA28U/9oWu4BUHDS5Le5YoY6+PpO9Y/Qnd9B6Ljm5wOGwaIfYf1i2PkqaWfPsTTlOqrqjICc0ilJpmBLU6e7JOErirJNCBF2iUtuBFYqiqIAe4QQnkKIAEVRTNvf4h4IaVtMeot2+WkpnNoJ8z5omewbaO3g2lfAzom+e95iodHAf7m+8XTDlM4r/UGsrjOQWVhBTnEVOcXVnC2p4mxJNaVVeipq9FTU6qmqNaDVCOy0Aq1Gg7ujHd4u9ni7OtDH04nI3q5E9nalj4cj6ps1SbIuDS347OIq3Bzs8PNwxF6rIczHmT1p59ucOi0TfscEAmeafJ1Vf6xZwhdC3AfcBxASEtL5u7oFQG0ZVJeqWx+a08GVsP99GPcwDJ1/6WuFgJn/4LudB1mq+5wMxZ+fjSMbT7c1pTO3pJqE00UcP1vKybxyTuaVkVlYgVG5cI1WI/Bzc8DdSYergx1ezvYEemoxGBUMRoVag5Gyaj2ZhRWcK6tt9ovg6awjLsSLq/xKGeuaR7CLAeHsDX1iL8yKkiQLc3ELvqxGT3lBOYP83YnPLOJ8ZW2rz+uJU6ctatBWUZT3gPcA4uLilMtcfnnufdTH8jzzJvwz++DHP0PEVJj+9JU9R6PhP85/IqQqj5d0/+Xa2hDOKH6AOqWzVm/kcFYxB08XkXimmITTxZwtqVafKiDM24X+fm5cNzSASD83gryc6OPhhK+bA1rN5VvpaxOy+fdPJ6gqMeDjas/Vkc6MKfqO4adWE5CZ3/IJAcNg5D0wbIH6LkWSLMQ76+NZYPyZybpDOIpajhjD+dowmZKqgexeOp0Rz/1CcVVdi+f1xKnT3fWbmQ0EN/k6qP6Yabn5q49lZ8Gnn8lv16rSs/DlHeofn5tXtCsZPjI7msdXP8oq8Rfe1L3B/Nq/o2js8XDSEfvsz1TUqi2W4F5OjAzrRWyIJ7EhXgz0d8NRp+1wyM1bRAqjKrfxx+Mf4StKKPAZTWnMYvbUhLMxvYqMUxkMVZK5PX8nYd89jHHPO2iufw2CR3X4/pLUZdI2s7Lmj/jpijluDKYUF36r3cid2g38r/xqtMoknr5hCH9ZdZgavbHxaY1Tp3uY7kr43wEPCSG+QB2sLTF5/z00Ka+Qa/JbtUpfA1/eDjVlcMcacO7VrqePj/Th69D+LE6/n/fs/8NSu894Rn8nlbV65g4PZEKkD3FhvfBxdejSsBvKQejQ84zd/1hgt5nDxnDuqf0zJ/MG8LxLNHMmBDJzOpRU1rEmIYv79v6G8HNbeDr/Y/xWXI1h+tPoxj+kdk9Jkjmc+BHjV3dSrPhxT+3jJCkRALhTzmK7r7jHbh18dQdz5n8EDOUfPx7jXHktDnYa/jU3qsf130PXTcv8HJgC+AghsoC/AzoARVHeBdahTslMRZ2WeVdX3PeyGlr4Jlx81eZ0LkXh1Mr7Cc2O54HaP5H0v1wWz/K45A9RVa2BfZnn2ZFSwPaUc5zILas/M5IV+tn8zu4n9oloZs34nUl/GHOKq3CnnOX2LzFSc5K39Dfwin4+BrRw0WCWh7OORePDuXNcGDtTo1jy6wRuy3mB2b/+lfT044Tf/gZC0/F3G5LUIWcPoXzzO44aw3jU8SmyjDqob8GX4spT+t+RogTybPJHnProXub87kPmxAby4c4Mnv7+GNVNWvs9SVfN0rntMucV4MGuuFe7OLiBvavJWviXms7VP/ldBp9ew2v6efxkHAWtTPUyGhWO5pSyPbWAHSnniM8sotZgxF6rIS7MC3dHO0qr9QC8oL+NUZoT/EvzLnf9NIA5sb8xyWsC6Odh5N9VLzBInOah2of5wTi22fnWBrOEEEzo58OEfjPYnRrDmm8eZ276p2z6dz597vyAE7kVNjHPWbIAtRXw1UKKceOeuj/z4f1TSM4ta5ylIwAFWGmYRS9Rxp/OrCZx7X+ImfsYC8eG8dPRXF5Yf4Kro/zxdLY396vpUj1/dM3N32SrbduqhJnx48vM0S9nlWEC/9Hf1Ozc8+uOU1VnYEfqOXalnqOoUh0sGujvxp3jQpnQz5dRYb1wstcSvuTHxufWouPhuof5wf5Jlla9DMab1NXEXa2mjM9dXsa9+hQP1P2JjcYRLS653GDW2EgfjItXcPQLf6alvMP/3r6ffxoWNjSwevQ8Z8kCbP4XFGVyf83fWDA9jkEB7gwKcGdObCDjX9hEdpMGy2v6eQwXKYw69DxMuhGNd1+evmEI17y2ndc2pvD364eY8YV0PRtI+AEma+Ff3NJ1o5Kldp+xQL+JDYY4ltTdBzTvw84rq2Hp6iT83B2YNtCPif18GB/pg69by374Pp5OzX44M5QA/lZ3F6/Yv6uWYJiypGtfUG0lfHYr3kWH2TvqFQ4e6AOVzWcvXOlglkarYciC56n+wcBdB94jx9iL97mu8XxPnecsmVlBMsqet/nJfhYpmqFk7T/Nq7+ebHxXefHvrIKGx+se4FfN4ziufwJ++w0D/d25ZWQIH+8+xcKxYYT7uJjpxXQ9G0j4/uq0yE5o6KfPKy7nPted3OGTTICuig+dFbJrXajCAT9xnimaQzhTw0pxAy9rbqOOljNLPZx0rPr9OPr6ulx2EVNrhdfWa6fyaFA2wVtfhLCJEDa+U6+tUV01fHEbnN4F895ndPTNJFzbySXnQuB47Yv8uO8QS+0+56QSzFbjsMbTPXGes2Rmm/+JXuPIk6VzKdfqG99BN7yr9HTWNR5rkI8XH+pu5Y+p/4OTG2DAbB67qj+rD2bx7pY0Xrx5qDleiUn06IS/NiGbymO13KTPYdrzG1k8e2C7W5QN/fRKXSWf2P+bMfrjZJ4NoMA3mBiXIoYYTuBIDYWKO+sNo/jQMIujSjga0TLZO+m0PHPDECJ7u17RvVsrvLZ41gCCB78D/50Eq++FB3a0e/ZPC/oa+OoOSN8Kc95pVritWR2gjtBoeNn5USKqFvOq7i2uq/kn2ag7kAV4OHYubklq6uwhOPYtn+t+Q7nWgzpD89/BqjoDDnYanHTaFtVrQ6/+E+zeBRuehMgZ+Lo5cMvIYD7fd5pHr+qPfw/5We2xCb8hUS8wuuGgq6O85FyH+o0b+ulf1i1nlDjBn2sfYJVxIg65WtwcdZyrqWm81sFOw4woP6b6uPDB9vRmI/0CuGlE+5Nnmwn35hXwwVXw7UNw66cdn/5oqINvfgcpP8P1r0HMJcffO+SPs4fy6Oo/85VYytv2rzG/9u/UoiOklzN1BiM6bc/dlkHqRtuWUatz56Wyq1p9dw1QUlXHf26JadGIujE2EJyfgi9/C0e+gWG3cu/ECD7de5oPtqfz1+sGd/OLMY0em/AbEnWexgsAP1HEyTrXy/YbG4wK2UVVpJ0rJ72gguziKkaL49yk3cFr+rmsMk4CoEZv5Nr+PgwP8WJkWC/69XZFU7+CdfwLm1pM61KAzScKuu4F9omFq56FDUth47Mw4+/t/x76WvjmLjjxA1y9DEYs6rr4mlD/vWfyr3W5vFD3Ak+5rGFb6MP8fCyPe1fG89aC4bg49NgfRak7FJ1COfEjq+3nEuDnj2t1HTn1K8+b6uPp1HYjauC14Betjo9Fzye4lzPXDQ3gy/1neGxmf5ztrf9n1PpfQRsaBjtzlSYJXwkmu7iKzcn5lFTWUVxZS25pDbklVeSUVJNb/1FraNIyF/CQ3RpyFS/e1t/YeDzQ04lXfhPT6r27bVvDMb+Hc8mw4xV1Je+oe6/8ufoa+GohnPxJTfaj7+va2C6i/pIthe9zuf3AR9w++QE+GxDNX9cmceeKfXz4u1G4yqQvddS+9wDBayVT+PPMCOw0ou2Nh9oiBEx+Qu3ePLoGom/m9jGhfJuYw/eHcrhlZBfU9zKzHvcbdq68hnHPb2r8Oo8LCb/BXf/b3/i5Tivw93AkwN2J2BBP/D0cifBxIcLXlb6+rhw8sIeJm47w77pbqEGdk3u5H5yLZ9c0Pd6lhIBrXoayPFj3OChGGH3/5Z9XUagm+1M71MqcI+/u2rgu4Xv/Bxku1qNfvpAVTq+ycGwYH+85xSKZ9KWOqq2Agx+z33kitZoArhsa0FhapN0TDgZeB96RsEcdy4oL9aK/nyuf7j0tE74lcrbXcvfEcNILytl4PJ98o5rwe1OETiu4a3w4s6P88XDS4emkw8vZvrErpjUzDDtREGxznY0o4Yp+cC65rWFX09rB/A/VTVPWP6Ful3jVM6Br449L5g5Y+3v1j8S892Go6RZwXWxtQjZLv0snSn8/X9o/x10Vy/nH/vu4Y0woH+85xZ0r9rHyd6Nk947UPse+g5oSXiqbxG2TQxqTfYcmHGg0MOp+dU+KrAOIoBEsGBXC098f40h2CVGB1r39aI/7zXK2t+MvswcCF6YUFle50NexjGXXDWv/D8Dx7xGh4/jhrnlX/JS2ZteYbM65zhHmfwS//A32vA1pG2HSYhhwjVol1GiE7AOw77+Q9DV4hcFd6yAozjTxtKFhXGU/A3nfcA332/3ItzXj+OWYPW/eFsuDnx3k958eZPmdcXIgV7pyiZ9y3iGIA7UDeW1MF7TCY25Tx8X2/ReC3mNubBD/WneC1QezrT7hC7XqgeWJi4tT4uPju+abvTUGvPuqs1naozAN3hgOs19Q+8utQdom+OlJKDgOQgPO3lBTDvoq0Lmo/fyTn1A3Te9m4Ut+bJw74UgNv9g/QTX2XFP7PL093ZgQ6cOX8WeYGxvIy/OHXfKdlyQBUHQKXhvKW+JWjvS9j3dub7kyvEPW/wX2L4dHj4KbH/etjCfhTDF7lk6/ovLi5iSEOKAoSqutOdtoRrn5d2y17cmf1MeB1136OkvSdxr8fhcsWgeTnlBb+SPvVnfZeuyY2t1jhmQPzccwqnHgb/pF9NNkc5/2B7KLq/juUA7XRPmzJiGbFzecMEuMkpU5/CUAn1WN5Tcjgy9zcTuMvBeMdXDoMwBujAmkoKyGPemFXXcPM+hxXTqtcguAcyntf17mTugVAZ5d+IPUHTQadQVuV63C7SIXj21sMcbyg2E0f7Rbww/GMZyq8yfxTDG3jwnhv1vT6d/bjZtGBJk5asmiHVlFsuNQ6nRBTIz06brv6xMJIWMp2/MRs7cPJbukGgG8vjGF8V15n25mOy388ly1L/tKGY1qmYG29p6V2m1ObCDPz4smsElL/9m6hdRgx3N2/wMUzpZU8/frhzA2wpula5JIOF3U9jeUbFv+CSg4wWflI5g3PAi7Lh73Seh1DW7lGfQuVRdsKsDejPN8E3/m0k+0YDaS8APAqIfKdrwdKzgOVUUQalmtZGs3JzaQnUumNSb9fLx4Wf8bJmmTmKmJp4+nEzqthrd/Oxw/dwfu//gAua0soJEkjn+HgmC9Po6bTfBO8InjfalUHLhZu63Z8X+tt97uRhtJ+E22OrxSp3apj7KFbxKLZw3AqX763CeGGSQbg/ir7jP+MiMcAC8Xez5YOJLyGj1//DwBvaFnbkghdcKxbzlqN4iA4PArrk/VHqklsN44kuu1u3Hgwkbn5yta3/TcGthIwu/AVodZ8eDqD56hponJxjXt3jGi5W2HewgRedxQvbbxmgH+bvxzbhT7Ms/z2sYOjMFIPc7ahGzGv7CJaUs/gLwjrKoawQ3D+pjkXn08nVhlmIS7qGS65mDjcY1QS7BYIxtJ+B1o4ecmgX+03JPVhBq6dzJeuJbX/u9RdUbRtpea/WGeGxvE/BFBvLk5lZ2p58wYrWRuDQURs4urmK1RS57/ZBiF1kS/ootnDeCQNpoCxZ1rtXsAdWW+UYHEM9Y5tmQbCd/VT3280ha+vkatUeMfbbqYpJZm/kP9t9/4XLPDz9w4hL6+rjzyRSIFZTVtPFnq6ZruMDdNm8BhYzhn8eb97Rkmud+c2ED+OW8Y2+zGMU2TSKQHPHPDEOw0gp+P5ZnknqZmGwnfzh6cfa68hV+QrA7y+keZNi6pOe++6gK3xE8g+8JbaGd7O95aMJyy6jr+suowlrpYUDKthuKDnpQRK1LYbIxtdtwU5sQGctMdf8RJ1PLr9dUsGB3KmAhvfpUJ38K1Z6vDXHUaFv49Z6cbqzFpMbj4wk9LoUliH+DvxhOzB7LpRD6rDmabMUDJXBoW7k3SJKEVCpsNMc2Om0zIGHU878hqAK4a7EdaQQUZ5ypMe18TsKGE347NzPOOgJ2TuuhK6l6O7jD1/+DMHjjxY7NTd40LY1RYL575/ihnS+T2iLamYWbXZG0ihYobh5UI0xUlbEqjhSFzIOUXqClj6oDeAGxNzjftfU3AxhJ+O1r4foPV/2ip+8XeAT794denwaBvPKzRCJbNH4reoPCXVUmya8fGzIkN5Pm5Q5imPcw241DcnBx4fl606YoSNjVkLhhqIHk9Id7OhHk7sy3F+iYR2FDCD4CK/GYJpFWKcmGGjmQeWjuY8TQUpkDCymanQr1dWHrNQLadLOArK17xKHXMnN75eFHKZkMM6x6Z2D3JHiBolJpDTvwAwKT+vuxOK6RGb7jMEy2LDSV8f3WDkIrLvA0rzYbqYvCTA7ZmNeAaCB4DW15Qq302cfvoUEaF9+Jf605wrlzO2rEpKT9jRJDfe3yzEh0mp9FA/9mQuhH0NUzs50tVnYEDp6xreqbtJHyP+qXXJVmXvk4O2FoGIWDmc1CeB7vfanZKoxH8a24UlbV6/rXuuJkClMxBn7yBBGMko4f06/6bD7wWasshYztj+3pjpxFsO2ld3Tq2k/A96zdGKD596etyj6iPfj1jl3qrFjwKBl0Pu16H8ubvzCJ7u3HfpAhWH8xmV5p1/dJJHVRVhCb3EDuM0Vw12K/77x82Ud1TInkdrg52jAj1YtvJgu6PoxNsJ+F71Jc4vmzCP6zOznFwM31M0uVNfxrqqmDriy1OPTytHyG9nPnr2iNW15cqdUDmTjQYOeEUy5A+7t1/f50jRE6D5PWgKEzq78uxs6VW1a1oOwnfwRWcel0+4ecdkf33lsQnEkYsggMfwrnUZqccdVqevXEI6QUVfGCi1ZaS5dCnbaVKscdv0ESEuUqeDLgWynLgbCLj+noDsDf9vHli6QDbSfigdutcKuHXlMH5dDlDx9JMWQJae9jyr5anBvRm1hA/3tqcSl6pLKPck1Wf3Mx+4wCmRplxU5x+M9WtQ0+sIyrQAxd7rVXtgiUTflN5x9RHmfAti2tvteTCkVUXBtWbePKaQegNCss2JJshOKlblOfjWppCvIhmTEQv88Xh4g0hYyF5PTqthpHhvdgtE76F8gyBkjPNluw3k1efTGSXjuUZ9zA4esCmf7Q4Fertwl0TwvjmQBZJWSVmCE4yNSVD3YSkOng8DnZmXhA54Go1VxSfZkyEN6n55VZT1M/GEn4o6Kuhoo2R9dwkcPS8MIVTshxOXjD+EXVj+TP7Wpx+aGokPq72PPvDUbkCtwcqPbaRUsWZ8CgL2JCo3yz1MfVXxkTU9+NnWEcr38YSfv1MnaJTrZ/PPSJr4Fuy0Q+ohdU2PtviXZqbo47HrhrA/swifkxqx74HknXI3M5e4yAmDvQ3dyTg0w88QiB1I1F93HF1sGN3mg0lfCHEbCFEshAiVQixpJXzi4QQBUKIxPqPe7rivu3mpW6fx/n0lueMBsg/JrtzLJm9C0x8HDK3Q/qWFqdvGRnMQH83lm1Ipk5uidhzFJ/Go+oMyc7DCfJyNnc0aoOw3wxI34KdomdkmJfVDNx2OuELIbTAW8DVwGDgNiFEa6uWvlQUJab+44PO3rdDeoWrI+yFrWyXdy4F6iohYFj3xyVdubi7wD0INj3XopWv1QgWzxrAqcJKvtwv6+z0FDUpmwGwi5xs5kiaiJyhrro9s5cxEd6kFVSQX2b5s8S6ooU/CkhVFCVdUZRa4Avgxi74vl3PzkHtxz/XSsI/m6g+9onp3pik9rFzgCl/gewDkLyuxelpA3szItSL1zemUFUrF2P1BOePbqZQcWPw0NHmDuWC8Emg0UHqL4y1ovn4XZHwA4Gmzams+mMXu0kIcVgI8Y0QIrgL7tsxPv2gMLXl8ZxE0DmrZXklyzZsAfTqq87YMTbvuhFC8JfZA8kvq+Gj3ZlmCU/qWg45eznAQEbVD5BaBAc3dWOU1I0MDnDH2V5LfKZtJPwr8T0QpijKUOAX4KPWLhJC3CeEiBdCxBcUmKhGhU9/NeFflCg4m6gO2Moa+JZPawfT/k8dczmyqsXpUeG9mDLAl3e2pFFSVWeGAKUuU3qWXrU5FPYajqPOwn43+10FeUewq8glJtiTeCuonNkVCT8baNpiD6o/1khRlEJFURomqn4AjGjtGymK8p6iKHGKosT5+vp2QWit8B2oTs08n3bhmNEAZw9DgOzOsRqD54JfNGz+JxhaJvXHZw6gpKqO97e1MkAvWY1zx7YC4NpvopkjaUXkDPUxdSNxoV4cP1tKRc1l9tsws65I+PuBfkKIcCGEPXAr8F3TC4QQAU2+vAEwX03bhj76nMQLxwqSoa5C9t9bE40Gpv0VijIg8dMWp6MCPbhuaAArdmZQaEXFraTmzh3bQqXiQNQIC0z4vQeDWx9I/YURYb0wKpB4ptjcUV1SpxO+oih64CFgA2oi/0pRlKNCiGeFEDfUX/ZHIcRRIcQh4I/Aos7et8N8B4LW4cIgLcCpnepjqAUs6pCuXP9ZEBgH214CfW2L03+a0Z+qOgMf7JCF1ayVS+4+jmkHENbbw9yhtCQERE6HtC3EBrkiBMRnWna3Tpf04SuKsk5RlP6KovRVFOWf9ceeUhTlu/rPlyqKMkRRlGGKokxVFOVEV9y3Q7Q68I+CnIQLxzK3q1P9PEPNFpbUAULA1KVquYyEj1ucjuztynVD+7ByVyZFFS3/IEiWzVBZTGBtOkU+cearjnk5kTOgpgT3c4cY4OdG/CnLHri1rZW2DULHqcvza8rU/t+M7RA2Qa6wtUZ9p6v7jW5/GfQtu24enhZJRa2BFTtlK9/anD60GQ0Kzv3GmzuUtoVPAgSkb2FEqBcJp4sxGC23tIdtJvx+s8BYB+lb1Y+q8zD4hss/T7I8QsDUJ9W9iA+ubHG6v58b10T78+HOTDljx8oUH99KnaKl3/Cp5g6lbc69oE8spG8hLsyL8ho9ybll5o6qTbaZ8EPGqJUXk76GxE/AwePCiLtkfSKmqCVrt78CdS1XOz40tR9lNXo+3JnZ3ZFJneCcF0+qti+9vS1o/n1rIqZA1n5GBtgDcMCCu3VsM+FrdTDyXji2Fo6ugVH3qCs4JeskBExZqu5EdLDlEo/Bfdy5arAfy3ekU1YtW/nWoK6mirDqE5z3Hm7uUC4vYgoY9QSWHKC3m4NFz8e3zYQPMGmxWmN93B9hcot6b5K1CZ8EoePrW/lVLU7/cVo/Sqv1rNzdRqVUyaKkHdqGg6jDse8Ec4dyecGjwc4Jkb6VuDAvi56pY7sJX+cIM/8BM58DO3tzRyN1VkMrvzxX3f/2ItFBHkzq78v/dmZSXSdr7Fi688fUDU8iRkw3cyRXQOcIoWMhfTPDQ7zILq4i30K327TdhC/1POETIWwi7PhPq638+ydFcK68hjUJ2a08WbIkDrnxZGkC8fLtY+5QrkzEFCg4wSgfdaaYpS7Akglf6lmmPgnleRC/osWpcX29iQp05/1t6RgteOqcrauu1RNedZRCLysqVR6hziQaWJWAnUbIhC9J3SJ0HIRPVlv5tRXNTgkhuH9SX9LPVfDL8TwzBShdzrFjh+klyrAPG2PuUK6cXxQ4e2OfuZWBAW4cypIJX5K6x9Qn1X2L9y9vcerqKH+Ceznx361prTxRsgR5R9X+++Chk8wcSTtoNGpDI30LMUEeHD5TYpHvImXCl3qekDHqW+ydr7Vo5dtpNdwzIYKDp4uton65LRLZ8VThiGvwUApHUCcAACAASURBVHOH0j59p0J5LhO9zlNWoyetoNzcEbUgE77UM019EirPwb73W5yaHxeEl7OOd7fK0smWprJWT2D5EfLdh1jf3hQRUwAYXqfW6bLEfnyZ8KWeKXiUWmdn1xstWvnO9nYsHBvGr8fzSM233GXwtuhgag4DxWn1/8/aeIZArwh88nfj5mgnE74kdavJT6it/Fbm5S8cG4q9nYb/yXILFiUzaSc6YcB/sBUsuGpNxFTEqZ0MD3SVCV+SulXIGHVe/s7XW9TY8XZ1YE5MH1YfzKakUpZbsBT603sBcLCmGTpNRUyB2nJmeZ7hRG6ZxS3ykwlf6tkmP6Guvm2lXv6iceFU1Rn4Yv9pMwQmXay0uo6A0iMUOwaBi4+5w+mY8IkgNIxWkjAYFY5kl5g7omZkwpd6trCJEDwGdrzaYleswX3cGR3ei5W7T6E3GNv4BlJ32Z9eSKwmhdqAOHOH0nFOXhAQQ3DJfsDyBm5lwpd6NiFg8mIozYJDn7U4fdf4cLKLq/hVLsQyu6PHj9JbFOPV38q3Go2Ygv3ZA/TzkAlfkrpf3+nQZ7haSdPQvL/+qsF+BHo6sUIO3ppdZfoeAHSho80cSSdFTAajnrnemTLhS1K3E0Ltyy8+pW5604RWI7hzXCj7Ms5zNMey+lttSVFFLb1LDlOncQC/IeYOp3OCx4CdIxO1R8kqqqKwvOXWm+YiE75kG/rPBv9ode9bY/OZE7fEheCk08odscxob0YhsZpUqn2HqhsUWTOdI4SMIaIsHoAkCxq4lQlfsg1CqJveFKaqu5w14eGs46YRgXx7KMeiWmO2ZF9KDlEiA+eIseYOpWuET8alOBlfUUJSlkz4ktT9Bl4PvoNg20tgbD4rZ9G4cGr1Rj7fJ6dodre1CdkkxW9HJwz8Nd6JtT1hv4KIKQDc6JHKYdnClyQz0Ghg0uNQcBxOfN/sVGRvV8ZHevPZ3tMYLLDKYU+1NiGbJasOM5STAPxaFsLS1UlWnfTXJmQzcWUhxYoLgyoPsi/Dcor0yYQv2ZYhc8E7ErYtA6V5Yr99dCg5JdVsOpFvpuBsz7INyVTrjcRqUslSfCjAi6o6A8s2JJs7tA5Zm5DN0tVJnCmpZbdxMKNFEiVVtazclWnu0ACZ8CVbo9HCxD9DbhKc/KnZqRmD/fBzd+CTPXKj8+6SU6xuRRmrSeWgsV+L49Zm2YZkqurLKew0RhEkzhEq8nhtY4qZI1PJhC/Znuj5amXDrf9u1srXaTXcOjKEbSkFnC6sNGOAtqOPpxN+nCdQFJJgjGx23Bo1/UO1wxgFwATNEQoratt6SreSCV+yPVodTHgMcg5C+pZmp24bFYJGCD7dJ1v53eGeCeHEalIBGlv4Tjoti2cNMGdYHdb0D1Wm4k+24s14zREc7Swj1VpGFJLU3WIWgKs/7Hil2WF/D0dmDOrN1/FZFlfpsCdyc9IxXJNCDTqOK2EEejrx/Lxo5sQGmju0Dlk8awBOuoaNWwQ7DVGM1RzD0c6sYTWSCV+yTXYOMPZByNgGWQeanbp9TCjnK2pZf+SsmYKzHbvTChlpl4p90HBOvnAjO5dMs9pkDzAnNpDn50UT6OmEAI45DcdLlBNUk0peafVln29qMuFLtivuLnD0bNHKH9/XhzBvZz7ZI+fkm5KiKOxPPUuUSEcEjzR3OF1mTmwgO5dMI+OFa3n6j78H1H78wxawAEsmfMl2ObjBqPvgxA+Qf6LxsEYj+O3oUA6cKuL42VIzBtiznT5fiWfZSXRKHQRZcUnkS3Hzw+g7iPHaoyRlmb+Qmkz4km0b/QDonGHnq80O3zwiCHs7jZyiaUK70goZrqmfrhhkhXvYXiFN36mM0iRzPKvA3KHIhC/ZOBdvGH6nWkWz+EIXjpeLPdcNDWBtQjblNXozBthz7U4rZIx9OopbH/Cw3n77ywqfjAO1aLP2oSjmXcUtE74kjXsIELDrjWaHfzs6lIpaA98fyjFPXD2YoijsSiskzi6tR/XftypsPEahJao2kbMl5h247ZKEL4SYLYRIFkKkCiGWtHLeQQjxZf35vUKIsK64ryR1CY8gGHoLHFwJ5Rfedg8P8aS/nytfyIJqXS6toBzK8/GpOwtBPTzhO7hR2TvWIgZuO53whRBa4C3gamAwcJsQYvBFl90NFCmKEgn8B3ixs/eVpC414U+gr4G97zQeEkJwy8gQDmWVcCxHDt52pV1p6v61QI/uv2/g2G8q0SKdk6fOmDWOrmjhjwJSFUVJVxSlFvgCuPGia24EPqr//BtguhBCdMG9Jalr+PSDQdfDvg+g+kJynxcbiL1Wwxf7ZSu/K+1KLWSSUwaKRgcBQ80djsnZ9ZuGVigY07ebNY6uSPiBQNM/W1n1x1q9RlEUPVACeF/8jYQQ9wkh4oUQ8QUF5h/RlmzMxMegpgTilzce8nKxZ3aUP2sSsuXK2y5iNCrsyShknEMGwj8adNZZN6ddAuOo0TjhX7jHrAO3FjVoqyjKe4qixCmKEufr62vucCRb0ycWIqbC7reh7kIRrFtHBVNWrWddklx52xWO55ZSVllNaE0yBPf87hwA7Ow55z2CkcbDZBWZrxJoVyT8bCC4yddB9cdavUYIYQd4AIVdcG9J6loTH4OKfEj8tPHQ2Ahvwryd+WKfeftfe4rdaYUMEGewM1T1/AHbJkTEFPpqzpKSYr5a/12R8PcD/YQQ4UIIe+BW4LuLrvkOuLP+85uBTYq5J6RKUmvCJkJgHOx8HQzq/PuGwdt9medJzS83c4DWb3daITPc68dEeuoK21b4DJ0FQGXyRrPF0OmEX98n/xCwATgOfKUoylEhxLNCiBvqL1sOeAshUoHHgBZTNyXJIgihtvKLT8HR1Y2HbxoRiJ1G8KUcvO0UvcHIvozzTHHOBBdf8Aw1d0jdxj4gimLhgWfuLrPF0CV9+IqirFMUpb+iKH0VRfln/bGnFEX5rv7zakVR5iuKEqkoyihFUdK74r6SZBL9rwbfgbDjP42bnfd2c2T6oN6sOphNjV4O3nbUkZxSymr09K87oU7HtKXJehoNpzzi6F95EKX+56rbQzDLXSXJkmk0MOFRyD8GKT83Hr51VAjnK2r59Zjc87ajdqaew5MyXMszbao7p0FN8CR6U0RO6iGz3F8mfElqTdRN4BGilk6uH26a1M+XQE8nOSe/E3anFXKdd32pChsasG3gGTUDgPNHfr7MlaYhE74ktUarg3EPw5m9cHq3ekgjmB8XxPaUc5w5L/e8ba8avYH9mee5yv00CI06DdbGhEcO5pTih8Np8yzAkglfktoSezs4e8P2Cxuk/CYuGI2AL/fLKZpXam1CNuNf2MSAv/5Ejd5IcMVR8BsCDq7mDq3b6bQajjsNJ7A4nknP/0L4kh8Z/8Im1iZcPJPdNGTCl6S22DvD6N9D6i+QmwSom1RP7u/L1wfOoDeYZ+DNmqxNyGbp6iSyi9XFRgIjviVJZDheXG7Ldpx0HoELVfiUHkEBsourWLo6qVuSvkz4knQpo+4Be1d1xk69W0aGkFdaw+ZkWf7jcpZtSKaqSUmKviIHN1HFmye9urVla0lWnw/HqAjGaY42HquqM7Bsg+kXZMmEL0mX4uSl7n17dA2cV2cTTx/UGx9XB9mtcwVyipuXEWjY4SpBiezWlq0lyax24qgSygTtkWbHL/63MgWZ8CXpcsY8CBq7xg1SdFoNN48IYnNyPnml5t3QwtL18WxeGC1WpFKsuJCh+APd17K1JH08HNlpjGa4OIkLF5L8xf9WpiATviRdjnsADLsNEj6FsjwAbhkZjMGo8M2BLDMHZ9kWzxqAk07b+HWsJpUEYyRKk9TTHS1bS/LE7IFsMw7FXhgau3WcdFoWzxpg8nvLhC9JV2L8I2Csgz1vAxDu48KYiF58uf8MRqMsC9WWObGBPD8vGp1W4Eol/UUWCcZ+za7pjpatJZkTG4hDxDjKFUcmaw4R6OnE8/OimRNr+n19ZcKXpCvh3RcG3wj7l0NVMQC3jgzh9PlK9qTLwq+XMrm/L3qjwr0RRWiEQoIS2Xiuu1q2luaGEWHsNEZxi2cyO/8ytVuSPciEL0lXbsKjUFvWuEHK7Ch/3B3t+EIO3l7SnvRCFAVu9lP3E8h3i0JAt7ZsLU10oCdbjcPQlWfBuZPddl+7bruTJFm7gGHQdzrseQfG/AFHnRPzhgfx2d7TFFXU4uVib+4ILdKutEKc7bUElB0B34FsePB6c4dkdhE+LuzTDgeWQ+qv4Ns973JkC1+S2mPiY1BRAAmfAOrgba3ByBobm1rYHjvTzjEqzAtNdrxNFkxrjUYj8A7sy2ltCKT80n337bY7SVJPEDpeLfq1S90gZVCAO8OCPPhy/xmz7lVqqXJLqkkvqGB2n0qoOm+TBdPaMjTIg1/rolFO7YTaim65p0z4ktQeQsCEx6D4dOMGKbeMDCE5r4zEM8VmDs7y7Ew9B8CxfeouT3dsUGxuoVVbooM82agfijDUQuaObrmnTPiS1F79ZzfbIEUjQABz395ls+UC2vLZPrWU9IDaY5QqTuws9bHJ1bWtGRrowX7jQPRap27r1pEJX5Laq8kGKbs3fM4z3x+joTPHVssFtMZoVEg4XQRAnCaZg8b+GNHY5Ora1oR6O+Pg6ESKS6xaoK8bugRlwpekjoi6CTyCcdn3OlV1+manZEJTHc8txaiAO+UM0GQRb+zfeM7WVte2RgjB0CAPthiGQVFmY60mU5IJX5I6on6DlKHKCUaKlsldJjTYnqL23zcUTDugXEj4tra6ti3RgZ58XVw/JbMbunVkwpekjoq9gyLc+YPdty1OyYQG21MKCPBwZKxdCnWKlkRjX8B2V9e2ZmiQB+mG3tR4RDTbP9lUZMKXpI6yd+bswEVM1R5ikDjVeFirETaf0KpqDezPKOK6oQHc5JtFiiacahxtenVta6IDPQBI85oAmduhptyk95MJX5I6YfCNj1GndeZRp3UI1Narg52G2VH+5g7NrPZmFFJrMDIpwgOfkiMMHj2TjBeuZeeSaTLZNxHk5YSXs45tIg4MtZC2yaT3kwlfkjrDyQvd6LuZqewk44lBvL8wjspaAxuO5po7MrPadvIc9nYaRjmdAX01BI82d0gWSQhBdJAn3xeFgqMnJK836f1kwpekTlibkM2NB2KoMWpY89YS8suqCe7lxBf7bLug2vaUAkaH98IhZ796IGSMeQOyYEMDPTiRX4khciac/AmMhss/qYNkwpekDmrYoPtQiROrDBO5Rr+J/6zZQXSgB7vTC8k81z3L5S3N2ZIqUvLLmdTPF07vAa8wcLPtLq5LiQ7ywGBUOOUzSS0/cWafye4lE74kdVDTDbr/a7geO/QsUH7g4KliNAK+irfNVn7DdMyJ/bzVhB8sW/eXMjRIHbjdI2JAo4PkdSa7l0z4ktRBTefan1L8WW8cze3aX6ksLWTqgN58fSALvcFoxgjNY9vJAnzdHBigK4DKc7I75zL83R3xcXUgPs8AYRNM2o8vE74kddDFc+3f0d+Am6jiD65buXVUCAVlNWw6kW+m6MxDbzCy7WQBk/v7Is7sVQ/KhH9JQgiGBXlwOKsEBlwDhSlwLtUk95IJX5I66OINuo8qYWxXhrFIu56pEa70dnPgSxvbDevAqSJKq/VMH9gbTu8GRw/wse01CVciNsST1PxySkNmqAdOmqaVLxO+JHVQwwbdgZ5OjVv2MeExHGsKsTv8GTePCGJzcj65JdXmDrXbbErOR6cVTOjnU99/P1otNiddUmyIFwAJZW7gF2Wybh25xaEkdcKc2MDmC4kUBU6pG6TcsmA+b29J45sDZ3hoWj/zBdmNNp/IZ1R4L9zqCtWuieF3mDskqzAs2BMhIOF0EZMnLTbZfeSfXknqSk02SAnNWce4vt58GX8Go7Hn74Z15nwlJ/PKmTqg94UNPcImmDcoK+HqYMcAPzcOni6GIXPUDxOQCV+SutqAq8E/Grb+m1tHBHDmfBW70grNHZXJbU5WB6inDaxP+PZu4D/MzFFZj9gQTxJPF5m0cSATviR1NSFgylIoymC2sh1PZx1f7D9t7qhMbuPxfMJ9XIjwdVUTfuhY0Mpe4ysVG+JFabWedBMu2OtUwhdC9BJC/CKESKl/9GrjOoMQIrH+47vO3FOSrMKAa8A/GvsdLzEvxo+fj+ZxvqLW3FGZTGWtnt3p6voDynLV/vuwieYOy6oMD/EE4GD9LmGm0NkW/hJgo6Io/YCN9V+3pkpRlJj6jxs6eU9JsnxNWvn3ehyg1mBk1YEsc0dlMrtSC6nVG5k+qDec2qkelP337RLh44q7ox0Jp4tNdo/OJvwbgY/qP/8IMM1IgyRZo/pWfkDi64wOdefjPad67ODtxhP5uDrYMTKsl9qd4+AO/kPNHZZV0WgEMSFejfsAm+QenXy+n6IoZ+s/zwX82rjOUQgRL4TYI4Ro84+CEOK++uviCwoKOhmaJJlZk1b+ksAkTp+vZOvJnvdzbTQq/Ho8j4n9fLC306gJP0T233dEbLAnJ/PKKK/RX/7iDrhswhdC/CqEONLKx41Nr1MURQHaar6EKooSBywAXhVC9G3tIkVR3lMUJU5RlDhfX9/2vhZJsjz1rfyYjPfwd7Vj5e5Mc0fU5RLOFFFQVqNu+lKWB+dOyu6cDooN8cSowOEzpunWuWzCVxRlhqIoUa18fAvkCSECAOofWy0coihKdv1jOrAFiO2yVyBJlqy+lS+KMng6/ChbThZwurDS3FF1qZ+O5KLTCqYO7A2n5Pz7zogNrl9xa66EfxnfAXfWf34n0GI3ZyGElxDCof5zH2A8cKyT95Uk61Hfyp+R/xE6YeSTvacu/xwroSgKPx3NZXykD+6OOkjfAg4esv++gzycdfT1dTFZP35nO9leAL4SQtwNnAJ+AyCEiAMeUBTlHmAQ8F8hhBH1D8wLiqJ0KOHX1dWRlZVFdbXt1CaxJY6OjgQFBaHT6cwdStcSAqY8id0Xt/FUUAIvxTvw2FX9cWxSeM1aHTtbypnzVTw4JVItK5G2GSImyf77TrgmOoCKGtPsetWp/xVFUQqB6a0cjwfuqf98FxDdmfs0yMrKws3NjbCwMIQQXfEtJQuhKAqFhYVkZWURHh5u7nC63oCrIWgk889/ynOV0Xx3KIffxAWbO6pO23AkF42AGYP9oDAVSs7AxMfMHZZV+/NM01UXtaqVttXV1Xh7e8tk3wMJIfD29u65796EgBlP41CZy589t7JydybqPAfr9tPRXEaG9cLH1QHSNqkH+04zb1BSm6wq4QMy2fdgPf7/NmwCRM7gTsMqTmefNdnAXHdJKyjnZF65OjsH1ITfK0Ldw1aySFaX8CXJqk1/Coe6Uh5yWM+HOzPNHU2nrE9Sl+DMHOIP+lrI2C5b9xZOJvx2KCwsJCYmhpiYGPz9/QkMDGz8ura2a+ukFBcX8/bbb3fp95QsQMAwiLqJRdp17E06TnaTfXGtzXeHcogL9VI3fsnaB3UVMuFbOJnw28Hb25vExEQSExN54IEHePTRRxu/tre3b/N5en37V83JhN+DTf0/dOh5ULuGj3ZlmjuaDjmRW8rJvHJuiOmjHkjbBEIrC6ZZOKudO/XM90c5llPapd9zcB93/n79kHY95/333+e9996jtraWyMhIPv74Y5ydnVm0aBGOjo4kJCQwfvx4HnzwQX77299SUVHBjTfeyKuvvkp5eTkAy5Yt46uvvqKmpoa5c+fyzDPPsGTJEtLS0oiJieGqq65i2bJlXfpaJTPy7osYvpAF8Su5fu91lE2LxM3RuqaifpuYg1YjuCY6QD2Q+isEjwJHd/MGJl2SbOF30rx589i/fz+HDh1i0KBBLF++vPFcVlYWu3bt4pVXXuGRRx7hkUceISkpiaCgoMZrfv75Z1JSUti3bx+JiYkcOHCAbdu28cILL9C3b18SExNlsu+JJv8FjZ2Oh4yfWN1G54qi8F1iDhMifdTZOSXZcPYQ9J9l7tCky7DaFn57W+KmcuTIEf76179SXFxMeXk5s2Zd+KGfP38+Wq26uGb37t2sXbsWgAULFvD4448DasL/+eefiY1Vq02Ul5eTkpJCSEhIN78SqVu5+aOZ+Geu3fwP/rD9R/TjHsROax3tr4Oni8guruKxq/qrB07+pD72v9p8QUlXxDp+wizYokWLePPNN0lKSuLvf/97s3nkLi4ul32+oigsXbq0cSwgNTWVu+++25QhS5Zi3ENUOQXwh+oPWJ+Ube5orti3iTk42GmYOaS+OO7Jn9SpmL6mWzAkdQ2Z8DuprKyMgIAA6urq+PTTT9u8bsyYMaxatQqAL774ovH4rFmzWLFiRWN/fnZ2Nvn5+bi5uVFWVmba4CXz0jnhcPVzRGkyyfj1A6tYiFWjN/DdoRxmDPZTxx1qKyB9q9q67+nrKHoAmfA76bnnnmP06NGMHz+egQMHtnndq6++yiuvvMLQoUNJTU3Fw8MDgJkzZ7JgwQLGjh1LdHQ0N998M2VlZXh7ezN+/HiioqJYvHhxd70cqZtpom+mwHMot5Z9yN4Tll9U7eejeRRX1nFLQ1mI9C1gqIEBs80al3RlhKW2KuLi4pT4+Phmx44fP86gQYPMFFHnVFZW4uTkhBCCL774gs8//5xvv21RXNTmWfP/cUfVZO7F4cOZrHa5lXmL/2vucC7pjuV7SS+oYNsTU9FqBHz7IBz7Hp5IA611zTTqqYQQB+r3H2nBagdtrc2BAwd46KGHUBQFT09PVqxYYe6QJAvhEDaaVP9rufbsKg4deoBhwyxzu4isokp2pJ7j4Wn91GRvqIPjP6izc2Sytwoy4XeTiRMncujQIXOHIVmowPkvUvfGJlj/OAz91SL7w7+p34R9/oj6acXpW6G6GKLmmTEqqT1kH74kWQAn72AS+z3EsOp4MrZ/Zu5wWjAYFb6Oz2J8Xx+CezmrB4+uUTcrl+UUrIZM+JJkIWLmPc4xwvHa+jeo7tpV5J21+UQ+2cVV3Daqfn2IvhZOfA8DrwU7B/MGJ10xmfAlyUK4OjmSFPM07vrz5H/3N3OH08yHuzIJ8HC8MPc+fTNUl8CQueYNTGoXmfAlyYJce/V1fKWZjc+xlSjZB80dDgApeWXsSD3H7WNC0TWsBk76Ghw9IGKqeYOT2kUm/HbSarXExMQQFRXF/Pnzqays7PD3WrRoEd988w0A99xzD8eOtb3V75YtW9i1a1fj1++++y4rV67s8L0ly+TqYIdh8pMUKO5UfP17tevEzD7anYm9nYZbR9bPva8qhuPfQ/R8sGu7SqxkeWTCbycnJycSExM5cuQI9vb2vPvuu83Od6QUMsAHH3zA4MGD2zx/ccJ/4IEHWLhwYYfuJVm2+ROieMXxD7gWn0DZ8qJZYzlfUcuqA9ncMKwP3q71ffVHVoG+GmJvN2tsUvtZ77TM9UsgN6lrv6d/NFz9whVfPnHiRA4fPsyWLVv429/+hpeXFydOnOD48eMsWbKELVu2UFNTw4MPPsj999+Poig8/PDD/PLLLwQHBzeroT9lyhReeukl4uLi+Omnn3jyyScxGAz4+PiwfPly3n33XbRaLZ988glvvPEGGzduxNXVlccff7yxPn9lZSV9+/ZlxYoVeHl5MWXKFEaPHs3mzZspLi5m+fLlTJwo65VbsrUJ2SzbkEx2SRRxdpOYt+M/aAddA4EjzBLP/3ZmUK038MDkiAsHEz4BvygIiDFLTFLHyRZ+B+n1etavX090dDQABw8e5LXXXuPkyZMsX74cDw8P9u/fz/79+3n//ffJyMhgzZo1JCcnc+zYMVauXNmsxd6goKCAe++9l1WrVnHo0CG+/vprwsLCmm24cnHSXrhwIS+++CKHDx8mOjqaZ555plmc+/bt49VXX212XLI8axOyWbo6qXEXrOf0d5CneFDy+T1Q1/2bu5dV1/HhrkxmDfYnsrebejDvKOQcVFv3FrhWQLo0623ht6Ml3pWqqqqIiVFbNhMnTuTuu+9m165djBo1ivDwcEAteXz48OHG/vmSkhJSUlLYtm0bt912G1qtlj59+jBtWsv5y3v27GHSpEmN36tXr16XjKekpITi4mImT54MwJ133sn8+fMbz8+bpy6KGTFiBJmZmZ178ZJJLduQTFWdofHrUlxYUncvK8tfhE3Pwax/dms8H+85RVm1ngenRl44eOBD0NpD9G+6NRapa1hvwjeThj78izUthawoCm+88Uaz2vgA69atM3l8F3NwUPtdtVpth8cXpO6R08r+ttuMw1ipv4qFu9+E8EndtslIaXUd729LZ1J/X6KD1EJ/VBVDwqcQdTO4eHdLHFLXkl06JjBr1izeeecd6urqADh58iQVFRVMmjSJL7/8EoPBwNmzZ9m8eXOL544ZM4Zt27aRkZEBwPnz5wHaLJfs4eGBl5cX27dvB+Djjz9ubO1L1qWPp1Orx/+p/y2ndX1R1jyg7i7VDf67NY2iyjoWz2xS4z7hY3Wj8jEPdEsMUteTCd8E7rnnHgYPHszw4cOJiori/vvvR6/XM3fuXPr168fgwYNZuHAhY8eObfFcX19f3nvvPebNm8ewYcO45ZZbALj++utZs2YNMTExjcm9wUcffcTixYsZOnQoiYmJPPXUU93yOqWutXjWAJx02mbHnHRaZg8LY1H57zHUVsHqe8Fg2ndquSXVLN+RwQ3D+lxo3RvqYO97EDoBAoaZ9P6S6cjyyJJFsfX/44ZZOjnFVfTxdGLxrAFcP6wPN7+7i8EF6/in8iaMfahd/fmtfc85sYFtXv/EN4dYk5DNpj9PuVA35+BK+O5huO1LWfvewsnyyJJkJebEBraajJfdPIxrXi9leq8spu1+E/yGQMyCy36/hpk/DYPB2cVVLF2d1Hivi+3LOM9X8VncPyniQrI31MG2ZdAnVm5UbuVkl44kWYHI3q4snjmA+/LnFgRCuQAADeVJREFUkes9Gr5/BM7sv+zzLp75A1BVZ2DZhuQW19boDSxdfZhATycemdHvwonET6H4NExZKqdiWjmZ8CXJSvxuQjhj+/lzQ9491LgEwBe3wfn0Sz6ntZk/bR1/a3MaaQUV/GNuFM729W/+q0tg0z8gaBT0m9np1yCZl0z4kmQltBrBq7fEoHXx5ne1j2M0GmDlHCg92+Zz2pr5c/HxvemFvLkphXmxgUwd0PvCia3/hopzcM2/Zeu+B5AJX5KsiLerA2/9djj7y31Z4vgUSmUhfDIPKgpbvb6tmT+LZ12YbplfVs0jXyQS6u3Cs3OiLlyYkwB734Xhd6j995LVkwlfkqzM8BAvXr81hq9ze/Nyr6dQzqfDh9dAWW6La+fEBvL8vGgCPZ0QQKCnE8/Pi24csK2s1XP3h/GUVNXxxm2xuDrUd+XUVsKqe8GlN1z1bDe+OsmU5CyddigsLGT69OkA5ObmotVq8fX1BWDfvn3NiqFdLD4+npUrV/L6669f8h7jxo1rtcaOpXN1daW8vNzcYdiM2VEBPHtjFH9bC7Uh/2Bp8d8RK2bDwrXgFdbs2rZm/lTVGrj/4wMczSnhvTviiAqsn3OvKLBuMRSmwMJvwcmrG16R1B16dMJv7/zjy/H29m4sq/D00083VqtsoNfrsbNr/Z80Li6OuLhWp8Y2Y43JXjKPO8aEYq8VLF0NRX7P8WLls2jenwbzP4LwS1dFLSyv4YFPDhB/qogXbxrKjMF+F07u+A8kfgKTFkPEFJO+Bql7dapLRwgxXwhxVAhhFEK0mc2EELOFEMlCiFQhxJLO3PNKNa08qHBh/vHahK5dmr5o0SIeeOABRo8ezRNPPMG+ffsY+//t3XtQVNcdwPHvb3HltTZGTKKID+gwSPABQRC1ade3IxZUTBuFFJvpxFozWCc1D6K1acwkMzb9w5iIjzAkppNoY7UkkImQ6OAjBq1JrCgJhBJdmxiC7fqIiZqe/nFX5LHAkl1YLpzPzM7s7j3sPb89y2/vnnvuORMmkJCQwMSJE/n4Y2P42759+5gzZw5gfFncf//92O12oqKimhz122y2hvJ2u50FCxYwcuRIMjMzuXGRXHFxMSNHjiQxMZGcnJyG122soqKC5ORk4uPjGTNmDFVVVQDMnTuXxMRE4uLi2Lx5c5P9rly5kri4OKZNm0Z5eXlD/QoLCwEoKCggPT0du91OdHR0q7Nvrlu3jqSkJMaMGcOaNWsAuHz5MqmpqYwdO5ZRo0axfft2r953zfDzpGHkZSXy1n8iSPv2CZyWW1Avp0PZn1q9Irfskzpmr9/PR2ecPLcwgZ+NG3pz46Hn4J0nYFQGTH68i6LQuoq3R/gngPnAptYKiEgA8DwwHXAAR0SkUCnV+vJOPtDW+GNvjvLdcTgcHDp0iICAAC5cuMD+/fvp06cPpaWl5ObmsnPnzhZ/U1lZyd69e7l48SIxMTEsXboUq9XapMwHH3xARUUF4eHhTJo0iYMHDzJu3DiWLFlCWVkZkZGRLFy40G2d8vLyWL58OZmZmVy9epXvvjPei/z8fAYMGMCVK1dISkoiIyODsLAwLl++zJQpU1i3bh3z5s1j1apVlJSUcPLkSbKzs0lLSwOMrqsTJ04QEhJCUlISqampTX657Nmzh6qqKsrLy1FKkZaWRllZGXV1dYSHh1NUVAQYs3xqvjEjbhBFOf1Ysf1DJp3O5fl+Bfzk3Sf5tuJN+v70WSQikQvfXGP/J1/xavlpDlR/RdTAUPIXJxEX7urGufo1lPwejmyBO+fC3I16VE4P5FXCV0qdApC2PxjJQLVSqsZV9jUgHejUhN+R8cfeuueeewgIMEZCOJ1OsrOzqaqqQkQaJlBrLjU1lcDAQAIDA7n99ts5d+4cERERTcokJyc3PBcfH09tbS02m42oqKiG6ZMXLlzY5Ej9hgkTJvDUU0/hcDiYP38+0dHGhTTr169n165dAJw5c4aqqirCwsLo27cvs2YZl8yPHj2awMBArFYro0ePbjKt8vTp0wkLM2ZKnD9/PgcOHGiR8Pfs2UNCgjGq49KlS1RVVXH33Xfz0EMP8cgjjzBnzhy9EIuPDQ8L5fVfT+SN4//m6X138NcvE/jDFy8xcOsU3lFJbL02g8P/i2Vgv2BWpcaSlTKcIGuA8SugYhfsexrOfwopy2DGk2AJaH+nmul0RR/+EOBMo8cOYLy7giLyAPAAwLBhw7zaaXj/4IaFJJo/72uNp0ZevXo1kydPZteuXdTW1mK3293+zY1pi6H1qYs9KdOaRYsWMX78eIqKipg9ezabNm3CYrFQWlrKe++9R0hICHa7nW++MRbWsFqtDV/cFoulYd8Wi6XJfpt/uTd/rJTiscceY8mSJS3qdOzYMYqLi1m1ahVTp07Vk7z5mMUipMcPIW1sOJVfxFP66SIGn3yRSedeY6oc4VrgAPoMT0YuDIUSAacDPjtoXFw1MAbu2w0/vLkoua/PgWn+127CF5FSYJCbTY8rpf7uy8oopTYDm8GYPM2b11o5M6bJHCLQcvxxZ3A6nQwZYvxTFBQU+Pz1Y2JiqKmpoba2lhEjRrTaF15TU0NUVBQ5OTmcPn2a48ePExkZya233kpISAiVlZUcPny4w/svKSnh/PnzBAcHs3v3bvLz85tsnzlzJqtXryYzMxObzcbZs2exWq1cv36dAQMGkJWVRf/+/dm6dev3il9rn4gQO/gHxA6+E370LFxbC5VFWKvfgbP/gNOHAQW2OyA2DUbOMa6itdw8pdfROXg0c2g34Sulpnm5j7NAo7NCRLie61Q3PpRdfYTy8MMPk52dzdq1a0lNTfX56wcHB/PCCy8wa9YsQkNDSUpKcltux44dbNu2DavVyqBBg8jNzSU0NJS8vDxiY2OJiYkhJSWlw/tPTk4mIyMDh8NBVlZWi5FHM2bM4NSpUw1TP9tsNl555RWqq6tZuXIlFosFq9XKxo0bOx689v1Yg2H0AuPmoa48B6Z1HZ9Mjywi+4DfKaWOutnWB/gEmIqR6I8Ai5RSFW29pp4euXWXLl3CZrOhlGLZsmVER0ezYsWKTt9vQUEBR48eZcOGDZ22D93G3UPko0W4ywwC/OsZ3x/IaL7T1vTI3g7LnCciDmACUCQib7ueDxeRYgCl1HXgQeBt4BSwo71kr7Vty5YtxMfHExcXh9PpdNtfrmne8HQOHs1c9AIoWrei27h7aN6HD8Y5sMbTMmjdU49aAEUp1d4wUM2kuuvBR2/kr3NgWucyVcIPCgqivr6esLAwnfR7GKUU9fX1BAUF+bsqmktrc/Bo5mWqhB8REYHD4aCurs7fVdE6QVBQUIuLzzRN8x1TJXyr1dpwhammaZrWMXo+fE3TtF5CJ3xN07ReQid8TdO0XqLbjsMXkTrgMy9eYiDwlY+q4089JQ7QsXRXPSWWnhIHeBfLcKXUbe42dNuE7y0ROdraxQdm0lPiAB1Ld9VTYukpcUDnxaK7dDRN03oJnfA1TdN6iZ6c8FsuA2VOPSUO0LF0Vz0llp4SB3RSLD22D1/TNE1rqicf4WuapmmN6ISvaZrWS5g64YvILBH5WESqReRRN9sDRWS7a/v7IjKi62vpGQ9iWSwidSLyoev2K3/Usz0iki8iX4rIiVa2i4isd8V5XETu6uo6esqDWOwi4mzUJt1yVXYRGSoie0XkpIhUiMhyN2VM0S4exmKWdgkSkXIR+cgVyxNuyvg2hymlTHkDAoBPgSigL/ARcGezMr8B8lz37wW2+7veXsSyGNjg77p6EMuPgbuAE61snw28hbFaXgrwvr/r7EUsduBNf9fTgzgGA3e57vfDWHK0+efLFO3iYSxmaRcBbK77VuB9IKVZGZ/mMDMf4ScD1UqpGqXUVeA1IL1ZmXTgJdf914Gp0j0n0vckFlNQSpUB59sokg68rAyHgf4iMrhratcxHsRiCkqpz5VSx1z3L2IsNdp8ontTtIuHsZiC672+5Hpodd2aj6LxaQ4zc8IfApxp9NhBy4ZvKKOMtXWdQFiX1K5jPIkFIMP1c/t1ERnaNVXzOU9jNYsJrp/kb4lInL8r0x5Xl0ACxtFkY6ZrlzZiAZO0i4gEiMiHwJdAiVKq1XbxRQ4zc8Lvbd4ARiilxgAl3PzW1/znGMa8JWOB54Ddfq5Pm0TEBuwEfquUuuDv+nijnVhM0y5Kqe+UUvFABJAsIqM6c39mTvhngcZHuRGu59yWEZE+wC1AfZfUrmPajUUpVa+U+tb1cCuQ2EV18zVP2s0UlFIXbvwkV0oVA1YRGejnarklIlaMBPkXpdTf3BQxTbu0F4uZ2uUGpdR/gb3ArGabfJrDzJzwjwDRIhIpIn0xTmgUNitTCGS77i8A3lWusx/dTLuxNOtPTcPouzSjQuAXrlEhKYBTKfW5vyv1fYjIoBv9qSKSjPH/1O0OKFx1fBE4pZT6cyvFTNEunsRiona5TUT6u+4HA9OBymbFfJrDTLXEYWNKqesi8iDwNsYol3ylVIWI/BE4qpQqxPhgbBORaoyTb/f6r8at8zCWHBFJA65jxLLYbxVug4i8ijFKYqCIOIA1GCejUErlAcUYI0Kqga+BX/qnpu3zIJYFwFIRuQ5cAe7tpgcUk4D7gH+6+osBcoFhYLp28SQWs7TLYOAlEQnA+FLaoZR6szNzmJ5aQdM0rZcwc5eOpmma1gE64WuapvUSOuFrmqb1Ejrha5qm9RI64WuapvUSOuFrmqb1Ejrha5qm9RL/BxiQqssfXKDTAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZ36J2EN85b9",
        "colab_type": "text"
      },
      "source": [
        "In order to implement a regularization we need to modify the loss function. Since the loss function in this exercise is computed during the training step, we define a new training step with a regularization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLbcWwlt9Jwl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def regularized_train_step(model, optimizer, x, y, lmbd):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(model.trainable_variables)\n",
        "        y_pred = model(x)\n",
        "        loss_val = tf.reduce_mean(tf.square(y-y_pred))\n",
        "        regul_val = tf.reduce_sum([tf.reduce_sum(tf.square(w)) for w in model.trainable_variables[::2]])\n",
        "        total_loss = tf.add(loss_val, lmbd*regul_val)\n",
        "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    return loss_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExK4FIw89s0M",
        "colab_type": "text"
      },
      "source": [
        "We can now set the strength of the regularization and retrain the big model with a regularization. We create another instance of the big model in order to compare the big model with and without regularization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PUUBGi496Vt",
        "colab_type": "code",
        "outputId": "456091d2-2cea-4b32-9967-e1ded1aad3d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "lmbd = 0.005\n",
        "\n",
        "big_reg_mdl = MyBigModel()\n",
        "big_opt = tf.optimizers.SGD(learning_rate)\n",
        "\n",
        "epoch = 0\n",
        "train_iters = 0\n",
        "train_loss = 0.0\n",
        "for x_t, y_t in train_overfit_ds:\n",
        "    train_loss += regularized_train_step(big_reg_mdl, big_opt, x_t, y_t, lmbd)\n",
        "    train_iters += 1\n",
        "    if (train_iters > int(N_train_samples/batch_size)):\n",
        "        for x_v, y_v in validation_ds:\n",
        "            y_pred = big_reg_mdl(x_v)\n",
        "            validation_loss = tf.reduce_mean(tf.square(y_v-y_pred))\n",
        "        print(\"Epoch: {} Train loss: {:.5} Validation loss: {:.5}\".format(epoch, train_loss/train_iters, validation_loss))\n",
        "        train_iters = 0\n",
        "        train_loss = 0.0\n",
        "        train_reg = 0.0\n",
        "        epoch += 1\n",
        "    if (epoch == N_epochs):\n",
        "        break"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Train loss: 12.418 Validation loss: 0.52514\n",
            "Epoch: 1 Train loss: 0.285 Validation loss: 0.34479\n",
            "Epoch: 2 Train loss: 0.27377 Validation loss: 0.48222\n",
            "Epoch: 3 Train loss: 0.26969 Validation loss: 0.60992\n",
            "Epoch: 4 Train loss: 0.2674 Validation loss: 0.67693\n",
            "Epoch: 5 Train loss: 0.26592 Validation loss: 0.69615\n",
            "Epoch: 6 Train loss: 0.26483 Validation loss: 0.68718\n",
            "Epoch: 7 Train loss: 0.26396 Validation loss: 0.66364\n",
            "Epoch: 8 Train loss: 0.26323 Validation loss: 0.63349\n",
            "Epoch: 9 Train loss: 0.2626 Validation loss: 0.60103\n",
            "Epoch: 10 Train loss: 0.26204 Validation loss: 0.56855\n",
            "Epoch: 11 Train loss: 0.26154 Validation loss: 0.53715\n",
            "Epoch: 12 Train loss: 0.26108 Validation loss: 0.50735\n",
            "Epoch: 13 Train loss: 0.26065 Validation loss: 0.47933\n",
            "Epoch: 14 Train loss: 0.26024 Validation loss: 0.4531\n",
            "Epoch: 15 Train loss: 0.25985 Validation loss: 0.4286\n",
            "Epoch: 16 Train loss: 0.25947 Validation loss: 0.40569\n",
            "Epoch: 17 Train loss: 0.2591 Validation loss: 0.38426\n",
            "Epoch: 18 Train loss: 0.25872 Validation loss: 0.36416\n",
            "Epoch: 19 Train loss: 0.25833 Validation loss: 0.34525\n",
            "Epoch: 20 Train loss: 0.25793 Validation loss: 0.3274\n",
            "Epoch: 21 Train loss: 0.25749 Validation loss: 0.31049\n",
            "Epoch: 22 Train loss: 0.25702 Validation loss: 0.29436\n",
            "Epoch: 23 Train loss: 0.25647 Validation loss: 0.27887\n",
            "Epoch: 24 Train loss: 0.25583 Validation loss: 0.26386\n",
            "Epoch: 25 Train loss: 0.25504 Validation loss: 0.24912\n",
            "Epoch: 26 Train loss: 0.25399 Validation loss: 0.23436\n",
            "Epoch: 27 Train loss: 0.25247 Validation loss: 0.21915\n",
            "Epoch: 28 Train loss: 0.25003 Validation loss: 0.20295\n",
            "Epoch: 29 Train loss: 0.24549 Validation loss: 0.18566\n",
            "Epoch: 30 Train loss: 0.2367 Validation loss: 0.16972\n",
            "Epoch: 31 Train loss: 0.22445 Validation loss: 0.15806\n",
            "Epoch: 32 Train loss: 0.21363 Validation loss: 0.14895\n",
            "Epoch: 33 Train loss: 0.20422 Validation loss: 0.1419\n",
            "Epoch: 34 Train loss: 0.19532 Validation loss: 0.13613\n",
            "Epoch: 35 Train loss: 0.18679 Validation loss: 0.13029\n",
            "Epoch: 36 Train loss: 0.17861 Validation loss: 0.12381\n",
            "Epoch: 37 Train loss: 0.17072 Validation loss: 0.11695\n",
            "Epoch: 38 Train loss: 0.16303 Validation loss: 0.11029\n",
            "Epoch: 39 Train loss: 0.15548 Validation loss: 0.10447\n",
            "Epoch: 40 Train loss: 0.14812 Validation loss: 0.10028\n",
            "Epoch: 41 Train loss: 0.14117 Validation loss: 0.098359\n",
            "Epoch: 42 Train loss: 0.13489 Validation loss: 0.098399\n",
            "Epoch: 43 Train loss: 0.1294 Validation loss: 0.098907\n",
            "Epoch: 44 Train loss: 0.15723 Validation loss: 0.031474\n",
            "Epoch: 45 Train loss: 0.15233 Validation loss: 0.085653\n",
            "Epoch: 46 Train loss: 0.12404 Validation loss: 0.039155\n",
            "Epoch: 47 Train loss: 0.14784 Validation loss: 0.04881\n",
            "Epoch: 48 Train loss: 0.12255 Validation loss: 0.041787\n",
            "Epoch: 49 Train loss: 0.13233 Validation loss: 0.034483\n",
            "Epoch: 50 Train loss: 0.12094 Validation loss: 0.042601\n",
            "Epoch: 51 Train loss: 0.12223 Validation loss: 0.033078\n",
            "Epoch: 52 Train loss: 0.11801 Validation loss: 0.041723\n",
            "Epoch: 53 Train loss: 0.11526 Validation loss: 0.035935\n",
            "Epoch: 54 Train loss: 0.11394 Validation loss: 0.039262\n",
            "Epoch: 55 Train loss: 0.11045 Validation loss: 0.038371\n",
            "Epoch: 56 Train loss: 0.1092 Validation loss: 0.037173\n",
            "Epoch: 57 Train loss: 0.10682 Validation loss: 0.038138\n",
            "Epoch: 58 Train loss: 0.10481 Validation loss: 0.036723\n",
            "Epoch: 59 Train loss: 0.10329 Validation loss: 0.036227\n",
            "Epoch: 60 Train loss: 0.10137 Validation loss: 0.036053\n",
            "Epoch: 61 Train loss: 0.099731 Validation loss: 0.035104\n",
            "Epoch: 62 Train loss: 0.098289 Validation loss: 0.034502\n",
            "Epoch: 63 Train loss: 0.096755 Validation loss: 0.034209\n",
            "Epoch: 64 Train loss: 0.095305 Validation loss: 0.033749\n",
            "Epoch: 65 Train loss: 0.093997 Validation loss: 0.033336\n",
            "Epoch: 66 Train loss: 0.09273 Validation loss: 0.033185\n",
            "Epoch: 67 Train loss: 0.091486 Validation loss: 0.033207\n",
            "Epoch: 68 Train loss: 0.090305 Validation loss: 0.033317\n",
            "Epoch: 69 Train loss: 0.089197 Validation loss: 0.033549\n",
            "Epoch: 70 Train loss: 0.088143 Validation loss: 0.033962\n",
            "Epoch: 71 Train loss: 0.087137 Validation loss: 0.034573\n",
            "Epoch: 72 Train loss: 0.08618 Validation loss: 0.035366\n",
            "Epoch: 73 Train loss: 0.08528 Validation loss: 0.036326\n",
            "Epoch: 74 Train loss: 0.084445 Validation loss: 0.03742\n",
            "Epoch: 75 Train loss: 0.083682 Validation loss: 0.038581\n",
            "Epoch: 76 Train loss: 0.082993 Validation loss: 0.039686\n",
            "Epoch: 77 Train loss: 0.082373 Validation loss: 0.040573\n",
            "Epoch: 78 Train loss: 0.081805 Validation loss: 0.041129\n",
            "Epoch: 79 Train loss: 0.081269 Validation loss: 0.041373\n",
            "Epoch: 80 Train loss: 0.080751 Validation loss: 0.04143\n",
            "Epoch: 81 Train loss: 0.080245 Validation loss: 0.041423\n",
            "Epoch: 82 Train loss: 0.07975 Validation loss: 0.041411\n",
            "Epoch: 83 Train loss: 0.079265 Validation loss: 0.041413\n",
            "Epoch: 84 Train loss: 0.07879 Validation loss: 0.041429\n",
            "Epoch: 85 Train loss: 0.078324 Validation loss: 0.041457\n",
            "Epoch: 86 Train loss: 0.077866 Validation loss: 0.041496\n",
            "Epoch: 87 Train loss: 0.077416 Validation loss: 0.041543\n",
            "Epoch: 88 Train loss: 0.076974 Validation loss: 0.041595\n",
            "Epoch: 89 Train loss: 0.07654 Validation loss: 0.041651\n",
            "Epoch: 90 Train loss: 0.076114 Validation loss: 0.041707\n",
            "Epoch: 91 Train loss: 0.075695 Validation loss: 0.041763\n",
            "Epoch: 92 Train loss: 0.075283 Validation loss: 0.041815\n",
            "Epoch: 93 Train loss: 0.074879 Validation loss: 0.041864\n",
            "Epoch: 94 Train loss: 0.074481 Validation loss: 0.041905\n",
            "Epoch: 95 Train loss: 0.07409 Validation loss: 0.04194\n",
            "Epoch: 96 Train loss: 0.073706 Validation loss: 0.041967\n",
            "Epoch: 97 Train loss: 0.073329 Validation loss: 0.041983\n",
            "Epoch: 98 Train loss: 0.072959 Validation loss: 0.041991\n",
            "Epoch: 99 Train loss: 0.072595 Validation loss: 0.041989\n",
            "Epoch: 100 Train loss: 0.072238 Validation loss: 0.041976\n",
            "Epoch: 101 Train loss: 0.071887 Validation loss: 0.041952\n",
            "Epoch: 102 Train loss: 0.071543 Validation loss: 0.041918\n",
            "Epoch: 103 Train loss: 0.071204 Validation loss: 0.041874\n",
            "Epoch: 104 Train loss: 0.070872 Validation loss: 0.041819\n",
            "Epoch: 105 Train loss: 0.070546 Validation loss: 0.041754\n",
            "Epoch: 106 Train loss: 0.070226 Validation loss: 0.04168\n",
            "Epoch: 107 Train loss: 0.069912 Validation loss: 0.041597\n",
            "Epoch: 108 Train loss: 0.069604 Validation loss: 0.041506\n",
            "Epoch: 109 Train loss: 0.069302 Validation loss: 0.041406\n",
            "Epoch: 110 Train loss: 0.069005 Validation loss: 0.041299\n",
            "Epoch: 111 Train loss: 0.068714 Validation loss: 0.041185\n",
            "Epoch: 112 Train loss: 0.068428 Validation loss: 0.041066\n",
            "Epoch: 113 Train loss: 0.068147 Validation loss: 0.04094\n",
            "Epoch: 114 Train loss: 0.067872 Validation loss: 0.040808\n",
            "Epoch: 115 Train loss: 0.067601 Validation loss: 0.040673\n",
            "Epoch: 116 Train loss: 0.067336 Validation loss: 0.040534\n",
            "Epoch: 117 Train loss: 0.067075 Validation loss: 0.040391\n",
            "Epoch: 118 Train loss: 0.066819 Validation loss: 0.040246\n",
            "Epoch: 119 Train loss: 0.066568 Validation loss: 0.040096\n",
            "Epoch: 120 Train loss: 0.066321 Validation loss: 0.039945\n",
            "Epoch: 121 Train loss: 0.066079 Validation loss: 0.039792\n",
            "Epoch: 122 Train loss: 0.065841 Validation loss: 0.039638\n",
            "Epoch: 123 Train loss: 0.065606 Validation loss: 0.039483\n",
            "Epoch: 124 Train loss: 0.065376 Validation loss: 0.039327\n",
            "Epoch: 125 Train loss: 0.06515 Validation loss: 0.039169\n",
            "Epoch: 126 Train loss: 0.064927 Validation loss: 0.039012\n",
            "Epoch: 127 Train loss: 0.064708 Validation loss: 0.038854\n",
            "Epoch: 128 Train loss: 0.064492 Validation loss: 0.038696\n",
            "Epoch: 129 Train loss: 0.064279 Validation loss: 0.038539\n",
            "Epoch: 130 Train loss: 0.064069 Validation loss: 0.038382\n",
            "Epoch: 131 Train loss: 0.063863 Validation loss: 0.038225\n",
            "Epoch: 132 Train loss: 0.063659 Validation loss: 0.038069\n",
            "Epoch: 133 Train loss: 0.063457 Validation loss: 0.037914\n",
            "Epoch: 134 Train loss: 0.063258 Validation loss: 0.037759\n",
            "Epoch: 135 Train loss: 0.063061 Validation loss: 0.037605\n",
            "Epoch: 136 Train loss: 0.062865 Validation loss: 0.037452\n",
            "Epoch: 137 Train loss: 0.062672 Validation loss: 0.0373\n",
            "Epoch: 138 Train loss: 0.06248 Validation loss: 0.037149\n",
            "Epoch: 139 Train loss: 0.062289 Validation loss: 0.036999\n",
            "Epoch: 140 Train loss: 0.062099 Validation loss: 0.03685\n",
            "Epoch: 141 Train loss: 0.06191 Validation loss: 0.036702\n",
            "Epoch: 142 Train loss: 0.061722 Validation loss: 0.036555\n",
            "Epoch: 143 Train loss: 0.061533 Validation loss: 0.036408\n",
            "Epoch: 144 Train loss: 0.061344 Validation loss: 0.036263\n",
            "Epoch: 145 Train loss: 0.061155 Validation loss: 0.036119\n",
            "Epoch: 146 Train loss: 0.060965 Validation loss: 0.035976\n",
            "Epoch: 147 Train loss: 0.060773 Validation loss: 0.035834\n",
            "Epoch: 148 Train loss: 0.06058 Validation loss: 0.035692\n",
            "Epoch: 149 Train loss: 0.060384 Validation loss: 0.035551\n",
            "Epoch: 150 Train loss: 0.060186 Validation loss: 0.03541\n",
            "Epoch: 151 Train loss: 0.059984 Validation loss: 0.03527\n",
            "Epoch: 152 Train loss: 0.059779 Validation loss: 0.035129\n",
            "Epoch: 153 Train loss: 0.05957 Validation loss: 0.034988\n",
            "Epoch: 154 Train loss: 0.059356 Validation loss: 0.034846\n",
            "Epoch: 155 Train loss: 0.059137 Validation loss: 0.034702\n",
            "Epoch: 156 Train loss: 0.058913 Validation loss: 0.034555\n",
            "Epoch: 157 Train loss: 0.058683 Validation loss: 0.034405\n",
            "Epoch: 158 Train loss: 0.058447 Validation loss: 0.034252\n",
            "Epoch: 159 Train loss: 0.058205 Validation loss: 0.034093\n",
            "Epoch: 160 Train loss: 0.057958 Validation loss: 0.033928\n",
            "Epoch: 161 Train loss: 0.057706 Validation loss: 0.033756\n",
            "Epoch: 162 Train loss: 0.057449 Validation loss: 0.033577\n",
            "Epoch: 163 Train loss: 0.057188 Validation loss: 0.033389\n",
            "Epoch: 164 Train loss: 0.056924 Validation loss: 0.033192\n",
            "Epoch: 165 Train loss: 0.056658 Validation loss: 0.032987\n",
            "Epoch: 166 Train loss: 0.056391 Validation loss: 0.032774\n",
            "Epoch: 167 Train loss: 0.056124 Validation loss: 0.032553\n",
            "Epoch: 168 Train loss: 0.055857 Validation loss: 0.032325\n",
            "Epoch: 169 Train loss: 0.055592 Validation loss: 0.032092\n",
            "Epoch: 170 Train loss: 0.055329 Validation loss: 0.031853\n",
            "Epoch: 171 Train loss: 0.055068 Validation loss: 0.031611\n",
            "Epoch: 172 Train loss: 0.054809 Validation loss: 0.031365\n",
            "Epoch: 173 Train loss: 0.054553 Validation loss: 0.031117\n",
            "Epoch: 174 Train loss: 0.054299 Validation loss: 0.030866\n",
            "Epoch: 175 Train loss: 0.054048 Validation loss: 0.030613\n",
            "Epoch: 176 Train loss: 0.0538 Validation loss: 0.030357\n",
            "Epoch: 177 Train loss: 0.053554 Validation loss: 0.030098\n",
            "Epoch: 178 Train loss: 0.053312 Validation loss: 0.029834\n",
            "Epoch: 179 Train loss: 0.053072 Validation loss: 0.029566\n",
            "Epoch: 180 Train loss: 0.052836 Validation loss: 0.029291\n",
            "Epoch: 181 Train loss: 0.052603 Validation loss: 0.029009\n",
            "Epoch: 182 Train loss: 0.052376 Validation loss: 0.028721\n",
            "Epoch: 183 Train loss: 0.052152 Validation loss: 0.028423\n",
            "Epoch: 184 Train loss: 0.051935 Validation loss: 0.028116\n",
            "Epoch: 185 Train loss: 0.051723 Validation loss: 0.0278\n",
            "Epoch: 186 Train loss: 0.051517 Validation loss: 0.027476\n",
            "Epoch: 187 Train loss: 0.051317 Validation loss: 0.027143\n",
            "Epoch: 188 Train loss: 0.051123 Validation loss: 0.026802\n",
            "Epoch: 189 Train loss: 0.050936 Validation loss: 0.026455\n",
            "Epoch: 190 Train loss: 0.050756 Validation loss: 0.026103\n",
            "Epoch: 191 Train loss: 0.050582 Validation loss: 0.025746\n",
            "Epoch: 192 Train loss: 0.050413 Validation loss: 0.025388\n",
            "Epoch: 193 Train loss: 0.05025 Validation loss: 0.025027\n",
            "Epoch: 194 Train loss: 0.050093 Validation loss: 0.024668\n",
            "Epoch: 195 Train loss: 0.04994 Validation loss: 0.02431\n",
            "Epoch: 196 Train loss: 0.049793 Validation loss: 0.023955\n",
            "Epoch: 197 Train loss: 0.049649 Validation loss: 0.023604\n",
            "Epoch: 198 Train loss: 0.049509 Validation loss: 0.023258\n",
            "Epoch: 199 Train loss: 0.049373 Validation loss: 0.022917\n",
            "Epoch: 200 Train loss: 0.04924 Validation loss: 0.022583\n",
            "Epoch: 201 Train loss: 0.049111 Validation loss: 0.022255\n",
            "Epoch: 202 Train loss: 0.048984 Validation loss: 0.021935\n",
            "Epoch: 203 Train loss: 0.048859 Validation loss: 0.021621\n",
            "Epoch: 204 Train loss: 0.048737 Validation loss: 0.021316\n",
            "Epoch: 205 Train loss: 0.048617 Validation loss: 0.021018\n",
            "Epoch: 206 Train loss: 0.048498 Validation loss: 0.020727\n",
            "Epoch: 207 Train loss: 0.048382 Validation loss: 0.020444\n",
            "Epoch: 208 Train loss: 0.048267 Validation loss: 0.020169\n",
            "Epoch: 209 Train loss: 0.048154 Validation loss: 0.019901\n",
            "Epoch: 210 Train loss: 0.048042 Validation loss: 0.01964\n",
            "Epoch: 211 Train loss: 0.047932 Validation loss: 0.019385\n",
            "Epoch: 212 Train loss: 0.047822 Validation loss: 0.019137\n",
            "Epoch: 213 Train loss: 0.047714 Validation loss: 0.018897\n",
            "Epoch: 214 Train loss: 0.047607 Validation loss: 0.018662\n",
            "Epoch: 215 Train loss: 0.047501 Validation loss: 0.018432\n",
            "Epoch: 216 Train loss: 0.047396 Validation loss: 0.018209\n",
            "Epoch: 217 Train loss: 0.047291 Validation loss: 0.017991\n",
            "Epoch: 218 Train loss: 0.047188 Validation loss: 0.017778\n",
            "Epoch: 219 Train loss: 0.047085 Validation loss: 0.01757\n",
            "Epoch: 220 Train loss: 0.046983 Validation loss: 0.017367\n",
            "Epoch: 221 Train loss: 0.046882 Validation loss: 0.017169\n",
            "Epoch: 222 Train loss: 0.046781 Validation loss: 0.016975\n",
            "Epoch: 223 Train loss: 0.046681 Validation loss: 0.016785\n",
            "Epoch: 224 Train loss: 0.046581 Validation loss: 0.016599\n",
            "Epoch: 225 Train loss: 0.046482 Validation loss: 0.016416\n",
            "Epoch: 226 Train loss: 0.046384 Validation loss: 0.016238\n",
            "Epoch: 227 Train loss: 0.046286 Validation loss: 0.016062\n",
            "Epoch: 228 Train loss: 0.046188 Validation loss: 0.01589\n",
            "Epoch: 229 Train loss: 0.046091 Validation loss: 0.015721\n",
            "Epoch: 230 Train loss: 0.045994 Validation loss: 0.015556\n",
            "Epoch: 231 Train loss: 0.045898 Validation loss: 0.015393\n",
            "Epoch: 232 Train loss: 0.045802 Validation loss: 0.015233\n",
            "Epoch: 233 Train loss: 0.045707 Validation loss: 0.015076\n",
            "Epoch: 234 Train loss: 0.045612 Validation loss: 0.014921\n",
            "Epoch: 235 Train loss: 0.045517 Validation loss: 0.01477\n",
            "Epoch: 236 Train loss: 0.045422 Validation loss: 0.01462\n",
            "Epoch: 237 Train loss: 0.045328 Validation loss: 0.014473\n",
            "Epoch: 238 Train loss: 0.045234 Validation loss: 0.014329\n",
            "Epoch: 239 Train loss: 0.04514 Validation loss: 0.014187\n",
            "Epoch: 240 Train loss: 0.045047 Validation loss: 0.014047\n",
            "Epoch: 241 Train loss: 0.044953 Validation loss: 0.01391\n",
            "Epoch: 242 Train loss: 0.044861 Validation loss: 0.013775\n",
            "Epoch: 243 Train loss: 0.044768 Validation loss: 0.013642\n",
            "Epoch: 244 Train loss: 0.044675 Validation loss: 0.013511\n",
            "Epoch: 245 Train loss: 0.044583 Validation loss: 0.013383\n",
            "Epoch: 246 Train loss: 0.044491 Validation loss: 0.013257\n",
            "Epoch: 247 Train loss: 0.044399 Validation loss: 0.013133\n",
            "Epoch: 248 Train loss: 0.044308 Validation loss: 0.013011\n",
            "Epoch: 249 Train loss: 0.044216 Validation loss: 0.012891\n",
            "Epoch: 250 Train loss: 0.044125 Validation loss: 0.012773\n",
            "Epoch: 251 Train loss: 0.044034 Validation loss: 0.012658\n",
            "Epoch: 252 Train loss: 0.043943 Validation loss: 0.012544\n",
            "Epoch: 253 Train loss: 0.043852 Validation loss: 0.012432\n",
            "Epoch: 254 Train loss: 0.043762 Validation loss: 0.012323\n",
            "Epoch: 255 Train loss: 0.043671 Validation loss: 0.012215\n",
            "Epoch: 256 Train loss: 0.043581 Validation loss: 0.01211\n",
            "Epoch: 257 Train loss: 0.043491 Validation loss: 0.012006\n",
            "Epoch: 258 Train loss: 0.043401 Validation loss: 0.011904\n",
            "Epoch: 259 Train loss: 0.043311 Validation loss: 0.011804\n",
            "Epoch: 260 Train loss: 0.043221 Validation loss: 0.011706\n",
            "Epoch: 261 Train loss: 0.043132 Validation loss: 0.01161\n",
            "Epoch: 262 Train loss: 0.043042 Validation loss: 0.011516\n",
            "Epoch: 263 Train loss: 0.042953 Validation loss: 0.011423\n",
            "Epoch: 264 Train loss: 0.042864 Validation loss: 0.011333\n",
            "Epoch: 265 Train loss: 0.042775 Validation loss: 0.011243\n",
            "Epoch: 266 Train loss: 0.042686 Validation loss: 0.011156\n",
            "Epoch: 267 Train loss: 0.042597 Validation loss: 0.01107\n",
            "Epoch: 268 Train loss: 0.042508 Validation loss: 0.010986\n",
            "Epoch: 269 Train loss: 0.042419 Validation loss: 0.010903\n",
            "Epoch: 270 Train loss: 0.04233 Validation loss: 0.010822\n",
            "Epoch: 271 Train loss: 0.042242 Validation loss: 0.010742\n",
            "Epoch: 272 Train loss: 0.042153 Validation loss: 0.010664\n",
            "Epoch: 273 Train loss: 0.042064 Validation loss: 0.010587\n",
            "Epoch: 274 Train loss: 0.041976 Validation loss: 0.010511\n",
            "Epoch: 275 Train loss: 0.041887 Validation loss: 0.010437\n",
            "Epoch: 276 Train loss: 0.041799 Validation loss: 0.010364\n",
            "Epoch: 277 Train loss: 0.04171 Validation loss: 0.010292\n",
            "Epoch: 278 Train loss: 0.041622 Validation loss: 0.010221\n",
            "Epoch: 279 Train loss: 0.041534 Validation loss: 0.010152\n",
            "Epoch: 280 Train loss: 0.041445 Validation loss: 0.010084\n",
            "Epoch: 281 Train loss: 0.041357 Validation loss: 0.010017\n",
            "Epoch: 282 Train loss: 0.041268 Validation loss: 0.0099502\n",
            "Epoch: 283 Train loss: 0.041179 Validation loss: 0.0098849\n",
            "Epoch: 284 Train loss: 0.041091 Validation loss: 0.0098207\n",
            "Epoch: 285 Train loss: 0.041002 Validation loss: 0.0097572\n",
            "Epoch: 286 Train loss: 0.040913 Validation loss: 0.0096949\n",
            "Epoch: 287 Train loss: 0.040824 Validation loss: 0.0096333\n",
            "Epoch: 288 Train loss: 0.040735 Validation loss: 0.0095725\n",
            "Epoch: 289 Train loss: 0.040646 Validation loss: 0.0095125\n",
            "Epoch: 290 Train loss: 0.040556 Validation loss: 0.0094532\n",
            "Epoch: 291 Train loss: 0.040467 Validation loss: 0.0093946\n",
            "Epoch: 292 Train loss: 0.040377 Validation loss: 0.0093368\n",
            "Epoch: 293 Train loss: 0.040287 Validation loss: 0.0092796\n",
            "Epoch: 294 Train loss: 0.040197 Validation loss: 0.009223\n",
            "Epoch: 295 Train loss: 0.040106 Validation loss: 0.0091671\n",
            "Epoch: 296 Train loss: 0.040015 Validation loss: 0.0091118\n",
            "Epoch: 297 Train loss: 0.039924 Validation loss: 0.0090568\n",
            "Epoch: 298 Train loss: 0.039833 Validation loss: 0.0090026\n",
            "Epoch: 299 Train loss: 0.039741 Validation loss: 0.0089488\n",
            "Epoch: 300 Train loss: 0.039649 Validation loss: 0.0088954\n",
            "Epoch: 301 Train loss: 0.039557 Validation loss: 0.0088424\n",
            "Epoch: 302 Train loss: 0.039464 Validation loss: 0.0087899\n",
            "Epoch: 303 Train loss: 0.03937 Validation loss: 0.0087378\n",
            "Epoch: 304 Train loss: 0.039276 Validation loss: 0.008686\n",
            "Epoch: 305 Train loss: 0.039182 Validation loss: 0.0086347\n",
            "Epoch: 306 Train loss: 0.039087 Validation loss: 0.0085836\n",
            "Epoch: 307 Train loss: 0.038991 Validation loss: 0.008533\n",
            "Epoch: 308 Train loss: 0.038894 Validation loss: 0.0084825\n",
            "Epoch: 309 Train loss: 0.038797 Validation loss: 0.0084324\n",
            "Epoch: 310 Train loss: 0.038699 Validation loss: 0.0083826\n",
            "Epoch: 311 Train loss: 0.038601 Validation loss: 0.0083331\n",
            "Epoch: 312 Train loss: 0.038501 Validation loss: 0.0082837\n",
            "Epoch: 313 Train loss: 0.038401 Validation loss: 0.0082347\n",
            "Epoch: 314 Train loss: 0.038299 Validation loss: 0.0081858\n",
            "Epoch: 315 Train loss: 0.038196 Validation loss: 0.0081373\n",
            "Epoch: 316 Train loss: 0.038093 Validation loss: 0.008089\n",
            "Epoch: 317 Train loss: 0.037988 Validation loss: 0.0080408\n",
            "Epoch: 318 Train loss: 0.037881 Validation loss: 0.0079931\n",
            "Epoch: 319 Train loss: 0.037774 Validation loss: 0.0079453\n",
            "Epoch: 320 Train loss: 0.037665 Validation loss: 0.0078978\n",
            "Epoch: 321 Train loss: 0.037554 Validation loss: 0.0078506\n",
            "Epoch: 322 Train loss: 0.037442 Validation loss: 0.0078035\n",
            "Epoch: 323 Train loss: 0.037328 Validation loss: 0.0077568\n",
            "Epoch: 324 Train loss: 0.037213 Validation loss: 0.0077102\n",
            "Epoch: 325 Train loss: 0.037095 Validation loss: 0.0076639\n",
            "Epoch: 326 Train loss: 0.036976 Validation loss: 0.0076177\n",
            "Epoch: 327 Train loss: 0.036855 Validation loss: 0.0075717\n",
            "Epoch: 328 Train loss: 0.036732 Validation loss: 0.0075261\n",
            "Epoch: 329 Train loss: 0.036606 Validation loss: 0.0074806\n",
            "Epoch: 330 Train loss: 0.036479 Validation loss: 0.0074355\n",
            "Epoch: 331 Train loss: 0.036349 Validation loss: 0.0073907\n",
            "Epoch: 332 Train loss: 0.036218 Validation loss: 0.0073461\n",
            "Epoch: 333 Train loss: 0.036084 Validation loss: 0.0073017\n",
            "Epoch: 334 Train loss: 0.035949 Validation loss: 0.0072575\n",
            "Epoch: 335 Train loss: 0.035812 Validation loss: 0.0072137\n",
            "Epoch: 336 Train loss: 0.035674 Validation loss: 0.0071701\n",
            "Epoch: 337 Train loss: 0.035534 Validation loss: 0.0071268\n",
            "Epoch: 338 Train loss: 0.035393 Validation loss: 0.007084\n",
            "Epoch: 339 Train loss: 0.035252 Validation loss: 0.0070415\n",
            "Epoch: 340 Train loss: 0.03511 Validation loss: 0.0069996\n",
            "Epoch: 341 Train loss: 0.034969 Validation loss: 0.0069582\n",
            "Epoch: 342 Train loss: 0.034828 Validation loss: 0.0069175\n",
            "Epoch: 343 Train loss: 0.034688 Validation loss: 0.0068776\n",
            "Epoch: 344 Train loss: 0.03455 Validation loss: 0.0068386\n",
            "Epoch: 345 Train loss: 0.034414 Validation loss: 0.0068008\n",
            "Epoch: 346 Train loss: 0.03428 Validation loss: 0.0067644\n",
            "Epoch: 347 Train loss: 0.034148 Validation loss: 0.0067293\n",
            "Epoch: 348 Train loss: 0.03402 Validation loss: 0.0066958\n",
            "Epoch: 349 Train loss: 0.033895 Validation loss: 0.006664\n",
            "Epoch: 350 Train loss: 0.033774 Validation loss: 0.0066341\n",
            "Epoch: 351 Train loss: 0.033656 Validation loss: 0.0066062\n",
            "Epoch: 352 Train loss: 0.033542 Validation loss: 0.0065802\n",
            "Epoch: 353 Train loss: 0.033431 Validation loss: 0.0065563\n",
            "Epoch: 354 Train loss: 0.033324 Validation loss: 0.0065345\n",
            "Epoch: 355 Train loss: 0.03322 Validation loss: 0.0065145\n",
            "Epoch: 356 Train loss: 0.03312 Validation loss: 0.0064966\n",
            "Epoch: 357 Train loss: 0.033024 Validation loss: 0.0064803\n",
            "Epoch: 358 Train loss: 0.03293 Validation loss: 0.006466\n",
            "Epoch: 359 Train loss: 0.03284 Validation loss: 0.0064532\n",
            "Epoch: 360 Train loss: 0.032753 Validation loss: 0.0064418\n",
            "Epoch: 361 Train loss: 0.032668 Validation loss: 0.006432\n",
            "Epoch: 362 Train loss: 0.032586 Validation loss: 0.0064234\n",
            "Epoch: 363 Train loss: 0.032506 Validation loss: 0.0064159\n",
            "Epoch: 364 Train loss: 0.032429 Validation loss: 0.0064093\n",
            "Epoch: 365 Train loss: 0.032354 Validation loss: 0.0064037\n",
            "Epoch: 366 Train loss: 0.032282 Validation loss: 0.0063987\n",
            "Epoch: 367 Train loss: 0.032211 Validation loss: 0.0063945\n",
            "Epoch: 368 Train loss: 0.032142 Validation loss: 0.0063907\n",
            "Epoch: 369 Train loss: 0.032074 Validation loss: 0.0063875\n",
            "Epoch: 370 Train loss: 0.032009 Validation loss: 0.0063844\n",
            "Epoch: 371 Train loss: 0.031945 Validation loss: 0.0063817\n",
            "Epoch: 372 Train loss: 0.031882 Validation loss: 0.0063792\n",
            "Epoch: 373 Train loss: 0.031821 Validation loss: 0.0063768\n",
            "Epoch: 374 Train loss: 0.031761 Validation loss: 0.0063744\n",
            "Epoch: 375 Train loss: 0.031702 Validation loss: 0.0063721\n",
            "Epoch: 376 Train loss: 0.031644 Validation loss: 0.0063696\n",
            "Epoch: 377 Train loss: 0.031588 Validation loss: 0.0063671\n",
            "Epoch: 378 Train loss: 0.031532 Validation loss: 0.0063645\n",
            "Epoch: 379 Train loss: 0.031478 Validation loss: 0.0063618\n",
            "Epoch: 380 Train loss: 0.031425 Validation loss: 0.0063587\n",
            "Epoch: 381 Train loss: 0.031372 Validation loss: 0.0063557\n",
            "Epoch: 382 Train loss: 0.03132 Validation loss: 0.0063522\n",
            "Epoch: 383 Train loss: 0.03127 Validation loss: 0.0063486\n",
            "Epoch: 384 Train loss: 0.03122 Validation loss: 0.0063446\n",
            "Epoch: 385 Train loss: 0.03117 Validation loss: 0.0063405\n",
            "Epoch: 386 Train loss: 0.031122 Validation loss: 0.0063361\n",
            "Epoch: 387 Train loss: 0.031074 Validation loss: 0.0063311\n",
            "Epoch: 388 Train loss: 0.031027 Validation loss: 0.006326\n",
            "Epoch: 389 Train loss: 0.030981 Validation loss: 0.0063206\n",
            "Epoch: 390 Train loss: 0.030935 Validation loss: 0.0063149\n",
            "Epoch: 391 Train loss: 0.03089 Validation loss: 0.006309\n",
            "Epoch: 392 Train loss: 0.030845 Validation loss: 0.0063026\n",
            "Epoch: 393 Train loss: 0.030801 Validation loss: 0.006296\n",
            "Epoch: 394 Train loss: 0.030757 Validation loss: 0.006289\n",
            "Epoch: 395 Train loss: 0.030715 Validation loss: 0.0062819\n",
            "Epoch: 396 Train loss: 0.030672 Validation loss: 0.0062743\n",
            "Epoch: 397 Train loss: 0.03063 Validation loss: 0.0062665\n",
            "Epoch: 398 Train loss: 0.030589 Validation loss: 0.0062585\n",
            "Epoch: 399 Train loss: 0.030548 Validation loss: 0.0062501\n",
            "Epoch: 400 Train loss: 0.030507 Validation loss: 0.0062415\n",
            "Epoch: 401 Train loss: 0.030467 Validation loss: 0.0062326\n",
            "Epoch: 402 Train loss: 0.030428 Validation loss: 0.0062233\n",
            "Epoch: 403 Train loss: 0.030389 Validation loss: 0.0062139\n",
            "Epoch: 404 Train loss: 0.03035 Validation loss: 0.0062041\n",
            "Epoch: 405 Train loss: 0.030311 Validation loss: 0.0061942\n",
            "Epoch: 406 Train loss: 0.030274 Validation loss: 0.006184\n",
            "Epoch: 407 Train loss: 0.030236 Validation loss: 0.0061735\n",
            "Epoch: 408 Train loss: 0.030199 Validation loss: 0.0061629\n",
            "Epoch: 409 Train loss: 0.030162 Validation loss: 0.0061521\n",
            "Epoch: 410 Train loss: 0.030125 Validation loss: 0.006141\n",
            "Epoch: 411 Train loss: 0.030089 Validation loss: 0.0061296\n",
            "Epoch: 412 Train loss: 0.030054 Validation loss: 0.0061181\n",
            "Epoch: 413 Train loss: 0.030018 Validation loss: 0.0061064\n",
            "Epoch: 414 Train loss: 0.029983 Validation loss: 0.0060945\n",
            "Epoch: 415 Train loss: 0.029948 Validation loss: 0.0060824\n",
            "Epoch: 416 Train loss: 0.029914 Validation loss: 0.0060701\n",
            "Epoch: 417 Train loss: 0.02988 Validation loss: 0.0060576\n",
            "Epoch: 418 Train loss: 0.029846 Validation loss: 0.006045\n",
            "Epoch: 419 Train loss: 0.029812 Validation loss: 0.0060322\n",
            "Epoch: 420 Train loss: 0.029779 Validation loss: 0.0060192\n",
            "Epoch: 421 Train loss: 0.029746 Validation loss: 0.006006\n",
            "Epoch: 422 Train loss: 0.029713 Validation loss: 0.0059928\n",
            "Epoch: 423 Train loss: 0.029681 Validation loss: 0.0059793\n",
            "Epoch: 424 Train loss: 0.029648 Validation loss: 0.0059658\n",
            "Epoch: 425 Train loss: 0.029617 Validation loss: 0.005952\n",
            "Epoch: 426 Train loss: 0.029585 Validation loss: 0.0059383\n",
            "Epoch: 427 Train loss: 0.029553 Validation loss: 0.0059243\n",
            "Epoch: 428 Train loss: 0.029522 Validation loss: 0.0059101\n",
            "Epoch: 429 Train loss: 0.029491 Validation loss: 0.0058959\n",
            "Epoch: 430 Train loss: 0.029461 Validation loss: 0.0058815\n",
            "Epoch: 431 Train loss: 0.02943 Validation loss: 0.0058671\n",
            "Epoch: 432 Train loss: 0.0294 Validation loss: 0.0058524\n",
            "Epoch: 433 Train loss: 0.02937 Validation loss: 0.0058378\n",
            "Epoch: 434 Train loss: 0.02934 Validation loss: 0.0058229\n",
            "Epoch: 435 Train loss: 0.02931 Validation loss: 0.005808\n",
            "Epoch: 436 Train loss: 0.029281 Validation loss: 0.005793\n",
            "Epoch: 437 Train loss: 0.029252 Validation loss: 0.0057778\n",
            "Epoch: 438 Train loss: 0.029223 Validation loss: 0.0057627\n",
            "Epoch: 439 Train loss: 0.029194 Validation loss: 0.0057473\n",
            "Epoch: 440 Train loss: 0.029165 Validation loss: 0.005732\n",
            "Epoch: 441 Train loss: 0.029137 Validation loss: 0.0057164\n",
            "Epoch: 442 Train loss: 0.029109 Validation loss: 0.0057009\n",
            "Epoch: 443 Train loss: 0.029081 Validation loss: 0.0056853\n",
            "Epoch: 444 Train loss: 0.029053 Validation loss: 0.0056696\n",
            "Epoch: 445 Train loss: 0.029025 Validation loss: 0.0056537\n",
            "Epoch: 446 Train loss: 0.028998 Validation loss: 0.0056378\n",
            "Epoch: 447 Train loss: 0.02897 Validation loss: 0.0056218\n",
            "Epoch: 448 Train loss: 0.028943 Validation loss: 0.0056057\n",
            "Epoch: 449 Train loss: 0.028916 Validation loss: 0.0055897\n",
            "Epoch: 450 Train loss: 0.028889 Validation loss: 0.0055734\n",
            "Epoch: 451 Train loss: 0.028863 Validation loss: 0.0055572\n",
            "Epoch: 452 Train loss: 0.028836 Validation loss: 0.0055408\n",
            "Epoch: 453 Train loss: 0.02881 Validation loss: 0.0055245\n",
            "Epoch: 454 Train loss: 0.028783 Validation loss: 0.005508\n",
            "Epoch: 455 Train loss: 0.028757 Validation loss: 0.0054914\n",
            "Epoch: 456 Train loss: 0.028731 Validation loss: 0.0054748\n",
            "Epoch: 457 Train loss: 0.028705 Validation loss: 0.0054583\n",
            "Epoch: 458 Train loss: 0.02868 Validation loss: 0.0054417\n",
            "Epoch: 459 Train loss: 0.028654 Validation loss: 0.0054249\n",
            "Epoch: 460 Train loss: 0.028629 Validation loss: 0.0054081\n",
            "Epoch: 461 Train loss: 0.028603 Validation loss: 0.0053912\n",
            "Epoch: 462 Train loss: 0.028578 Validation loss: 0.0053743\n",
            "Epoch: 463 Train loss: 0.028553 Validation loss: 0.0053575\n",
            "Epoch: 464 Train loss: 0.028528 Validation loss: 0.0053404\n",
            "Epoch: 465 Train loss: 0.028503 Validation loss: 0.0053235\n",
            "Epoch: 466 Train loss: 0.028478 Validation loss: 0.0053064\n",
            "Epoch: 467 Train loss: 0.028454 Validation loss: 0.0052893\n",
            "Epoch: 468 Train loss: 0.028429 Validation loss: 0.0052722\n",
            "Epoch: 469 Train loss: 0.028405 Validation loss: 0.005255\n",
            "Epoch: 470 Train loss: 0.02838 Validation loss: 0.0052378\n",
            "Epoch: 471 Train loss: 0.028356 Validation loss: 0.0052205\n",
            "Epoch: 472 Train loss: 0.028332 Validation loss: 0.0052033\n",
            "Epoch: 473 Train loss: 0.028308 Validation loss: 0.005186\n",
            "Epoch: 474 Train loss: 0.028284 Validation loss: 0.0051687\n",
            "Epoch: 475 Train loss: 0.02826 Validation loss: 0.0051513\n",
            "Epoch: 476 Train loss: 0.028236 Validation loss: 0.0051339\n",
            "Epoch: 477 Train loss: 0.028213 Validation loss: 0.0051165\n",
            "Epoch: 478 Train loss: 0.028189 Validation loss: 0.005099\n",
            "Epoch: 479 Train loss: 0.028166 Validation loss: 0.0050817\n",
            "Epoch: 480 Train loss: 0.028142 Validation loss: 0.0050641\n",
            "Epoch: 481 Train loss: 0.028119 Validation loss: 0.0050467\n",
            "Epoch: 482 Train loss: 0.028095 Validation loss: 0.0050292\n",
            "Epoch: 483 Train loss: 0.028072 Validation loss: 0.0050117\n",
            "Epoch: 484 Train loss: 0.028049 Validation loss: 0.0049942\n",
            "Epoch: 485 Train loss: 0.028026 Validation loss: 0.0049766\n",
            "Epoch: 486 Train loss: 0.028003 Validation loss: 0.0049591\n",
            "Epoch: 487 Train loss: 0.02798 Validation loss: 0.0049415\n",
            "Epoch: 488 Train loss: 0.027957 Validation loss: 0.0049239\n",
            "Epoch: 489 Train loss: 0.027934 Validation loss: 0.0049064\n",
            "Epoch: 490 Train loss: 0.027911 Validation loss: 0.0048889\n",
            "Epoch: 491 Train loss: 0.027888 Validation loss: 0.0048712\n",
            "Epoch: 492 Train loss: 0.027865 Validation loss: 0.0048536\n",
            "Epoch: 493 Train loss: 0.027842 Validation loss: 0.0048361\n",
            "Epoch: 494 Train loss: 0.02782 Validation loss: 0.0048185\n",
            "Epoch: 495 Train loss: 0.027797 Validation loss: 0.004801\n",
            "Epoch: 496 Train loss: 0.027774 Validation loss: 0.0047834\n",
            "Epoch: 497 Train loss: 0.027752 Validation loss: 0.0047659\n",
            "Epoch: 498 Train loss: 0.027729 Validation loss: 0.0047482\n",
            "Epoch: 499 Train loss: 0.027707 Validation loss: 0.0047307\n",
            "Epoch: 500 Train loss: 0.027684 Validation loss: 0.0047131\n",
            "Epoch: 501 Train loss: 0.027662 Validation loss: 0.0046957\n",
            "Epoch: 502 Train loss: 0.027639 Validation loss: 0.0046781\n",
            "Epoch: 503 Train loss: 0.027617 Validation loss: 0.0046606\n",
            "Epoch: 504 Train loss: 0.027594 Validation loss: 0.0046431\n",
            "Epoch: 505 Train loss: 0.027572 Validation loss: 0.0046256\n",
            "Epoch: 506 Train loss: 0.027549 Validation loss: 0.0046081\n",
            "Epoch: 507 Train loss: 0.027527 Validation loss: 0.0045907\n",
            "Epoch: 508 Train loss: 0.027504 Validation loss: 0.0045733\n",
            "Epoch: 509 Train loss: 0.027482 Validation loss: 0.0045559\n",
            "Epoch: 510 Train loss: 0.027459 Validation loss: 0.0045384\n",
            "Epoch: 511 Train loss: 0.027437 Validation loss: 0.0045211\n",
            "Epoch: 512 Train loss: 0.027414 Validation loss: 0.0045037\n",
            "Epoch: 513 Train loss: 0.027391 Validation loss: 0.0044864\n",
            "Epoch: 514 Train loss: 0.027369 Validation loss: 0.0044691\n",
            "Epoch: 515 Train loss: 0.027346 Validation loss: 0.0044518\n",
            "Epoch: 516 Train loss: 0.027324 Validation loss: 0.0044344\n",
            "Epoch: 517 Train loss: 0.027301 Validation loss: 0.0044171\n",
            "Epoch: 518 Train loss: 0.027278 Validation loss: 0.0043999\n",
            "Epoch: 519 Train loss: 0.027255 Validation loss: 0.0043827\n",
            "Epoch: 520 Train loss: 0.027232 Validation loss: 0.0043656\n",
            "Epoch: 521 Train loss: 0.027209 Validation loss: 0.0043484\n",
            "Epoch: 522 Train loss: 0.027187 Validation loss: 0.0043312\n",
            "Epoch: 523 Train loss: 0.027163 Validation loss: 0.0043141\n",
            "Epoch: 524 Train loss: 0.02714 Validation loss: 0.004297\n",
            "Epoch: 525 Train loss: 0.027117 Validation loss: 0.0042798\n",
            "Epoch: 526 Train loss: 0.027094 Validation loss: 0.0042628\n",
            "Epoch: 527 Train loss: 0.02707 Validation loss: 0.0042457\n",
            "Epoch: 528 Train loss: 0.027047 Validation loss: 0.0042287\n",
            "Epoch: 529 Train loss: 0.027023 Validation loss: 0.0042117\n",
            "Epoch: 530 Train loss: 0.027 Validation loss: 0.0041946\n",
            "Epoch: 531 Train loss: 0.026976 Validation loss: 0.0041777\n",
            "Epoch: 532 Train loss: 0.026952 Validation loss: 0.0041607\n",
            "Epoch: 533 Train loss: 0.026928 Validation loss: 0.0041438\n",
            "Epoch: 534 Train loss: 0.026904 Validation loss: 0.0041269\n",
            "Epoch: 535 Train loss: 0.026879 Validation loss: 0.0041099\n",
            "Epoch: 536 Train loss: 0.026855 Validation loss: 0.004093\n",
            "Epoch: 537 Train loss: 0.02683 Validation loss: 0.0040761\n",
            "Epoch: 538 Train loss: 0.026805 Validation loss: 0.0040592\n",
            "Epoch: 539 Train loss: 0.02678 Validation loss: 0.0040422\n",
            "Epoch: 540 Train loss: 0.026755 Validation loss: 0.0040253\n",
            "Epoch: 541 Train loss: 0.02673 Validation loss: 0.0040084\n",
            "Epoch: 542 Train loss: 0.026704 Validation loss: 0.0039915\n",
            "Epoch: 543 Train loss: 0.026678 Validation loss: 0.0039746\n",
            "Epoch: 544 Train loss: 0.026652 Validation loss: 0.0039577\n",
            "Epoch: 545 Train loss: 0.026626 Validation loss: 0.0039408\n",
            "Epoch: 546 Train loss: 0.026599 Validation loss: 0.003924\n",
            "Epoch: 547 Train loss: 0.026573 Validation loss: 0.003907\n",
            "Epoch: 548 Train loss: 0.026546 Validation loss: 0.00389\n",
            "Epoch: 549 Train loss: 0.026518 Validation loss: 0.0038731\n",
            "Epoch: 550 Train loss: 0.026491 Validation loss: 0.0038562\n",
            "Epoch: 551 Train loss: 0.026463 Validation loss: 0.0038391\n",
            "Epoch: 552 Train loss: 0.026435 Validation loss: 0.0038222\n",
            "Epoch: 553 Train loss: 0.026406 Validation loss: 0.0038052\n",
            "Epoch: 554 Train loss: 0.026378 Validation loss: 0.0037882\n",
            "Epoch: 555 Train loss: 0.026349 Validation loss: 0.0037712\n",
            "Epoch: 556 Train loss: 0.026319 Validation loss: 0.0037541\n",
            "Epoch: 557 Train loss: 0.026289 Validation loss: 0.0037371\n",
            "Epoch: 558 Train loss: 0.026259 Validation loss: 0.00372\n",
            "Epoch: 559 Train loss: 0.026229 Validation loss: 0.003703\n",
            "Epoch: 560 Train loss: 0.026198 Validation loss: 0.0036859\n",
            "Epoch: 561 Train loss: 0.026167 Validation loss: 0.0036688\n",
            "Epoch: 562 Train loss: 0.026136 Validation loss: 0.0036518\n",
            "Epoch: 563 Train loss: 0.026105 Validation loss: 0.0036347\n",
            "Epoch: 564 Train loss: 0.026073 Validation loss: 0.0036177\n",
            "Epoch: 565 Train loss: 0.02604 Validation loss: 0.0036007\n",
            "Epoch: 566 Train loss: 0.026008 Validation loss: 0.0035838\n",
            "Epoch: 567 Train loss: 0.025975 Validation loss: 0.0035667\n",
            "Epoch: 568 Train loss: 0.025942 Validation loss: 0.0035498\n",
            "Epoch: 569 Train loss: 0.025909 Validation loss: 0.0035329\n",
            "Epoch: 570 Train loss: 0.025875 Validation loss: 0.0035161\n",
            "Epoch: 571 Train loss: 0.025841 Validation loss: 0.0034994\n",
            "Epoch: 572 Train loss: 0.025807 Validation loss: 0.0034827\n",
            "Epoch: 573 Train loss: 0.025773 Validation loss: 0.0034662\n",
            "Epoch: 574 Train loss: 0.025739 Validation loss: 0.0034498\n",
            "Epoch: 575 Train loss: 0.025704 Validation loss: 0.0034334\n",
            "Epoch: 576 Train loss: 0.02567 Validation loss: 0.0034172\n",
            "Epoch: 577 Train loss: 0.025635 Validation loss: 0.0034011\n",
            "Epoch: 578 Train loss: 0.0256 Validation loss: 0.0033852\n",
            "Epoch: 579 Train loss: 0.025566 Validation loss: 0.0033694\n",
            "Epoch: 580 Train loss: 0.025531 Validation loss: 0.0033538\n",
            "Epoch: 581 Train loss: 0.025496 Validation loss: 0.0033384\n",
            "Epoch: 582 Train loss: 0.025461 Validation loss: 0.0033232\n",
            "Epoch: 583 Train loss: 0.025427 Validation loss: 0.0033082\n",
            "Epoch: 584 Train loss: 0.025392 Validation loss: 0.0032935\n",
            "Epoch: 585 Train loss: 0.025358 Validation loss: 0.0032789\n",
            "Epoch: 586 Train loss: 0.025324 Validation loss: 0.0032646\n",
            "Epoch: 587 Train loss: 0.02529 Validation loss: 0.0032505\n",
            "Epoch: 588 Train loss: 0.025256 Validation loss: 0.0032367\n",
            "Epoch: 589 Train loss: 0.025223 Validation loss: 0.0032231\n",
            "Epoch: 590 Train loss: 0.02519 Validation loss: 0.0032097\n",
            "Epoch: 591 Train loss: 0.025157 Validation loss: 0.0031966\n",
            "Epoch: 592 Train loss: 0.025124 Validation loss: 0.0031838\n",
            "Epoch: 593 Train loss: 0.025092 Validation loss: 0.0031713\n",
            "Epoch: 594 Train loss: 0.02506 Validation loss: 0.0031589\n",
            "Epoch: 595 Train loss: 0.025029 Validation loss: 0.0031469\n",
            "Epoch: 596 Train loss: 0.024998 Validation loss: 0.0031352\n",
            "Epoch: 597 Train loss: 0.024967 Validation loss: 0.0031235\n",
            "Epoch: 598 Train loss: 0.024936 Validation loss: 0.0031123\n",
            "Epoch: 599 Train loss: 0.024907 Validation loss: 0.0031013\n",
            "Epoch: 600 Train loss: 0.024877 Validation loss: 0.0030905\n",
            "Epoch: 601 Train loss: 0.024848 Validation loss: 0.00308\n",
            "Epoch: 602 Train loss: 0.024819 Validation loss: 0.0030697\n",
            "Epoch: 603 Train loss: 0.024791 Validation loss: 0.0030597\n",
            "Epoch: 604 Train loss: 0.024764 Validation loss: 0.0030499\n",
            "Epoch: 605 Train loss: 0.024736 Validation loss: 0.0030403\n",
            "Epoch: 606 Train loss: 0.024709 Validation loss: 0.003031\n",
            "Epoch: 607 Train loss: 0.024683 Validation loss: 0.0030218\n",
            "Epoch: 608 Train loss: 0.024657 Validation loss: 0.0030129\n",
            "Epoch: 609 Train loss: 0.024632 Validation loss: 0.0030042\n",
            "Epoch: 610 Train loss: 0.024607 Validation loss: 0.0029956\n",
            "Epoch: 611 Train loss: 0.024582 Validation loss: 0.0029873\n",
            "Epoch: 612 Train loss: 0.024558 Validation loss: 0.0029792\n",
            "Epoch: 613 Train loss: 0.024534 Validation loss: 0.0029711\n",
            "Epoch: 614 Train loss: 0.024511 Validation loss: 0.0029634\n",
            "Epoch: 615 Train loss: 0.024488 Validation loss: 0.0029558\n",
            "Epoch: 616 Train loss: 0.024465 Validation loss: 0.0029483\n",
            "Epoch: 617 Train loss: 0.024443 Validation loss: 0.002941\n",
            "Epoch: 618 Train loss: 0.024422 Validation loss: 0.0029339\n",
            "Epoch: 619 Train loss: 0.0244 Validation loss: 0.0029269\n",
            "Epoch: 620 Train loss: 0.02438 Validation loss: 0.0029201\n",
            "Epoch: 621 Train loss: 0.024359 Validation loss: 0.0029134\n",
            "Epoch: 622 Train loss: 0.024339 Validation loss: 0.0029069\n",
            "Epoch: 623 Train loss: 0.024319 Validation loss: 0.0029005\n",
            "Epoch: 624 Train loss: 0.0243 Validation loss: 0.0028943\n",
            "Epoch: 625 Train loss: 0.024281 Validation loss: 0.0028882\n",
            "Epoch: 626 Train loss: 0.024262 Validation loss: 0.0028822\n",
            "Epoch: 627 Train loss: 0.024244 Validation loss: 0.0028764\n",
            "Epoch: 628 Train loss: 0.024225 Validation loss: 0.0028706\n",
            "Epoch: 629 Train loss: 0.024208 Validation loss: 0.002865\n",
            "Epoch: 630 Train loss: 0.02419 Validation loss: 0.0028595\n",
            "Epoch: 631 Train loss: 0.024173 Validation loss: 0.0028541\n",
            "Epoch: 632 Train loss: 0.024156 Validation loss: 0.0028489\n",
            "Epoch: 633 Train loss: 0.024139 Validation loss: 0.0028438\n",
            "Epoch: 634 Train loss: 0.024123 Validation loss: 0.0028388\n",
            "Epoch: 635 Train loss: 0.024107 Validation loss: 0.0028339\n",
            "Epoch: 636 Train loss: 0.024091 Validation loss: 0.0028291\n",
            "Epoch: 637 Train loss: 0.024076 Validation loss: 0.0028244\n",
            "Epoch: 638 Train loss: 0.02406 Validation loss: 0.0028199\n",
            "Epoch: 639 Train loss: 0.024045 Validation loss: 0.0028155\n",
            "Epoch: 640 Train loss: 0.02403 Validation loss: 0.0028112\n",
            "Epoch: 641 Train loss: 0.024015 Validation loss: 0.0028069\n",
            "Epoch: 642 Train loss: 0.024001 Validation loss: 0.0028028\n",
            "Epoch: 643 Train loss: 0.023987 Validation loss: 0.0027988\n",
            "Epoch: 644 Train loss: 0.023972 Validation loss: 0.002795\n",
            "Epoch: 645 Train loss: 0.023958 Validation loss: 0.0027911\n",
            "Epoch: 646 Train loss: 0.023945 Validation loss: 0.0027875\n",
            "Epoch: 647 Train loss: 0.023931 Validation loss: 0.0027839\n",
            "Epoch: 648 Train loss: 0.023917 Validation loss: 0.0027804\n",
            "Epoch: 649 Train loss: 0.023904 Validation loss: 0.0027771\n",
            "Epoch: 650 Train loss: 0.023891 Validation loss: 0.0027739\n",
            "Epoch: 651 Train loss: 0.023878 Validation loss: 0.0027708\n",
            "Epoch: 652 Train loss: 0.023865 Validation loss: 0.0027678\n",
            "Epoch: 653 Train loss: 0.023852 Validation loss: 0.0027649\n",
            "Epoch: 654 Train loss: 0.023839 Validation loss: 0.0027621\n",
            "Epoch: 655 Train loss: 0.023826 Validation loss: 0.0027595\n",
            "Epoch: 656 Train loss: 0.023814 Validation loss: 0.0027569\n",
            "Epoch: 657 Train loss: 0.023801 Validation loss: 0.0027545\n",
            "Epoch: 658 Train loss: 0.023789 Validation loss: 0.0027522\n",
            "Epoch: 659 Train loss: 0.023776 Validation loss: 0.00275\n",
            "Epoch: 660 Train loss: 0.023764 Validation loss: 0.0027479\n",
            "Epoch: 661 Train loss: 0.023752 Validation loss: 0.002746\n",
            "Epoch: 662 Train loss: 0.023739 Validation loss: 0.0027442\n",
            "Epoch: 663 Train loss: 0.023727 Validation loss: 0.0027424\n",
            "Epoch: 664 Train loss: 0.023715 Validation loss: 0.0027408\n",
            "Epoch: 665 Train loss: 0.023703 Validation loss: 0.0027394\n",
            "Epoch: 666 Train loss: 0.023691 Validation loss: 0.002738\n",
            "Epoch: 667 Train loss: 0.023679 Validation loss: 0.0027368\n",
            "Epoch: 668 Train loss: 0.023667 Validation loss: 0.0027357\n",
            "Epoch: 669 Train loss: 0.023655 Validation loss: 0.0027348\n",
            "Epoch: 670 Train loss: 0.023643 Validation loss: 0.0027339\n",
            "Epoch: 671 Train loss: 0.02363 Validation loss: 0.0027332\n",
            "Epoch: 672 Train loss: 0.023618 Validation loss: 0.0027327\n",
            "Epoch: 673 Train loss: 0.023606 Validation loss: 0.0027322\n",
            "Epoch: 674 Train loss: 0.023594 Validation loss: 0.002732\n",
            "Epoch: 675 Train loss: 0.023582 Validation loss: 0.0027318\n",
            "Epoch: 676 Train loss: 0.02357 Validation loss: 0.0027317\n",
            "Epoch: 677 Train loss: 0.023557 Validation loss: 0.0027319\n",
            "Epoch: 678 Train loss: 0.023545 Validation loss: 0.0027321\n",
            "Epoch: 679 Train loss: 0.023533 Validation loss: 0.0027324\n",
            "Epoch: 680 Train loss: 0.02352 Validation loss: 0.002733\n",
            "Epoch: 681 Train loss: 0.023508 Validation loss: 0.0027336\n",
            "Epoch: 682 Train loss: 0.023495 Validation loss: 0.0027344\n",
            "Epoch: 683 Train loss: 0.023483 Validation loss: 0.0027353\n",
            "Epoch: 684 Train loss: 0.02347 Validation loss: 0.0027365\n",
            "Epoch: 685 Train loss: 0.023458 Validation loss: 0.0027377\n",
            "Epoch: 686 Train loss: 0.023445 Validation loss: 0.0027391\n",
            "Epoch: 687 Train loss: 0.023432 Validation loss: 0.0027406\n",
            "Epoch: 688 Train loss: 0.023419 Validation loss: 0.0027422\n",
            "Epoch: 689 Train loss: 0.023406 Validation loss: 0.002744\n",
            "Epoch: 690 Train loss: 0.023393 Validation loss: 0.002746\n",
            "Epoch: 691 Train loss: 0.02338 Validation loss: 0.0027481\n",
            "Epoch: 692 Train loss: 0.023367 Validation loss: 0.0027504\n",
            "Epoch: 693 Train loss: 0.023354 Validation loss: 0.0027528\n",
            "Epoch: 694 Train loss: 0.023341 Validation loss: 0.0027554\n",
            "Epoch: 695 Train loss: 0.023327 Validation loss: 0.0027581\n",
            "Epoch: 696 Train loss: 0.023314 Validation loss: 0.0027609\n",
            "Epoch: 697 Train loss: 0.023301 Validation loss: 0.0027639\n",
            "Epoch: 698 Train loss: 0.023287 Validation loss: 0.0027671\n",
            "Epoch: 699 Train loss: 0.023274 Validation loss: 0.0027703\n",
            "Epoch: 700 Train loss: 0.02326 Validation loss: 0.0027737\n",
            "Epoch: 701 Train loss: 0.023247 Validation loss: 0.0027774\n",
            "Epoch: 702 Train loss: 0.023233 Validation loss: 0.002781\n",
            "Epoch: 703 Train loss: 0.02322 Validation loss: 0.0027849\n",
            "Epoch: 704 Train loss: 0.023207 Validation loss: 0.0027889\n",
            "Epoch: 705 Train loss: 0.023193 Validation loss: 0.002793\n",
            "Epoch: 706 Train loss: 0.02318 Validation loss: 0.0027974\n",
            "Epoch: 707 Train loss: 0.023166 Validation loss: 0.0028017\n",
            "Epoch: 708 Train loss: 0.023153 Validation loss: 0.0028062\n",
            "Epoch: 709 Train loss: 0.02314 Validation loss: 0.0028109\n",
            "Epoch: 710 Train loss: 0.023126 Validation loss: 0.0028157\n",
            "Epoch: 711 Train loss: 0.023113 Validation loss: 0.0028206\n",
            "Epoch: 712 Train loss: 0.0231 Validation loss: 0.0028256\n",
            "Epoch: 713 Train loss: 0.023087 Validation loss: 0.0028308\n",
            "Epoch: 714 Train loss: 0.023075 Validation loss: 0.002836\n",
            "Epoch: 715 Train loss: 0.023062 Validation loss: 0.0028414\n",
            "Epoch: 716 Train loss: 0.023049 Validation loss: 0.0028468\n",
            "Epoch: 717 Train loss: 0.023037 Validation loss: 0.0028525\n",
            "Epoch: 718 Train loss: 0.023025 Validation loss: 0.0028581\n",
            "Epoch: 719 Train loss: 0.023013 Validation loss: 0.0028639\n",
            "Epoch: 720 Train loss: 0.023001 Validation loss: 0.0028698\n",
            "Epoch: 721 Train loss: 0.022989 Validation loss: 0.0028758\n",
            "Epoch: 722 Train loss: 0.022978 Validation loss: 0.0028819\n",
            "Epoch: 723 Train loss: 0.022967 Validation loss: 0.0028881\n",
            "Epoch: 724 Train loss: 0.022956 Validation loss: 0.0028944\n",
            "Epoch: 725 Train loss: 0.022945 Validation loss: 0.0029008\n",
            "Epoch: 726 Train loss: 0.022934 Validation loss: 0.0029072\n",
            "Epoch: 727 Train loss: 0.022924 Validation loss: 0.0029138\n",
            "Epoch: 728 Train loss: 0.022914 Validation loss: 0.0029204\n",
            "Epoch: 729 Train loss: 0.022904 Validation loss: 0.0029272\n",
            "Epoch: 730 Train loss: 0.022895 Validation loss: 0.002934\n",
            "Epoch: 731 Train loss: 0.022886 Validation loss: 0.0029409\n",
            "Epoch: 732 Train loss: 0.022877 Validation loss: 0.0029479\n",
            "Epoch: 733 Train loss: 0.022868 Validation loss: 0.0029551\n",
            "Epoch: 734 Train loss: 0.022859 Validation loss: 0.0029622\n",
            "Epoch: 735 Train loss: 0.022851 Validation loss: 0.0029695\n",
            "Epoch: 736 Train loss: 0.022843 Validation loss: 0.0029769\n",
            "Epoch: 737 Train loss: 0.022835 Validation loss: 0.0029844\n",
            "Epoch: 738 Train loss: 0.022828 Validation loss: 0.002992\n",
            "Epoch: 739 Train loss: 0.022821 Validation loss: 0.0029997\n",
            "Epoch: 740 Train loss: 0.022814 Validation loss: 0.0030075\n",
            "Epoch: 741 Train loss: 0.022807 Validation loss: 0.0030154\n",
            "Epoch: 742 Train loss: 0.0228 Validation loss: 0.0030234\n",
            "Epoch: 743 Train loss: 0.022794 Validation loss: 0.0030315\n",
            "Epoch: 744 Train loss: 0.022788 Validation loss: 0.0030397\n",
            "Epoch: 745 Train loss: 0.022782 Validation loss: 0.003048\n",
            "Epoch: 746 Train loss: 0.022776 Validation loss: 0.0030565\n",
            "Epoch: 747 Train loss: 0.022771 Validation loss: 0.003065\n",
            "Epoch: 748 Train loss: 0.022765 Validation loss: 0.0030737\n",
            "Epoch: 749 Train loss: 0.02276 Validation loss: 0.0030825\n",
            "Epoch: 750 Train loss: 0.022755 Validation loss: 0.0030914\n",
            "Epoch: 751 Train loss: 0.02275 Validation loss: 0.0031005\n",
            "Epoch: 752 Train loss: 0.022746 Validation loss: 0.0031096\n",
            "Epoch: 753 Train loss: 0.022741 Validation loss: 0.003119\n",
            "Epoch: 754 Train loss: 0.022737 Validation loss: 0.0031284\n",
            "Epoch: 755 Train loss: 0.022732 Validation loss: 0.003138\n",
            "Epoch: 756 Train loss: 0.022728 Validation loss: 0.0031477\n",
            "Epoch: 757 Train loss: 0.022724 Validation loss: 0.0031575\n",
            "Epoch: 758 Train loss: 0.02272 Validation loss: 0.0031675\n",
            "Epoch: 759 Train loss: 0.022717 Validation loss: 0.0031776\n",
            "Epoch: 760 Train loss: 0.022713 Validation loss: 0.0031878\n",
            "Epoch: 761 Train loss: 0.022709 Validation loss: 0.0031982\n",
            "Epoch: 762 Train loss: 0.022706 Validation loss: 0.0032086\n",
            "Epoch: 763 Train loss: 0.022702 Validation loss: 0.0032193\n",
            "Epoch: 764 Train loss: 0.022699 Validation loss: 0.0032301\n",
            "Epoch: 765 Train loss: 0.022696 Validation loss: 0.003241\n",
            "Epoch: 766 Train loss: 0.022692 Validation loss: 0.0032521\n",
            "Epoch: 767 Train loss: 0.022689 Validation loss: 0.0032632\n",
            "Epoch: 768 Train loss: 0.022686 Validation loss: 0.0032746\n",
            "Epoch: 769 Train loss: 0.022683 Validation loss: 0.0032861\n",
            "Epoch: 770 Train loss: 0.02268 Validation loss: 0.0032977\n",
            "Epoch: 771 Train loss: 0.022677 Validation loss: 0.0033095\n",
            "Epoch: 772 Train loss: 0.022674 Validation loss: 0.0033214\n",
            "Epoch: 773 Train loss: 0.022671 Validation loss: 0.0033335\n",
            "Epoch: 774 Train loss: 0.022668 Validation loss: 0.0033456\n",
            "Epoch: 775 Train loss: 0.022666 Validation loss: 0.003358\n",
            "Epoch: 776 Train loss: 0.022663 Validation loss: 0.0033704\n",
            "Epoch: 777 Train loss: 0.02266 Validation loss: 0.003383\n",
            "Epoch: 778 Train loss: 0.022657 Validation loss: 0.0033958\n",
            "Epoch: 779 Train loss: 0.022654 Validation loss: 0.0034087\n",
            "Epoch: 780 Train loss: 0.022652 Validation loss: 0.0034217\n",
            "Epoch: 781 Train loss: 0.022649 Validation loss: 0.0034349\n",
            "Epoch: 782 Train loss: 0.022646 Validation loss: 0.0034482\n",
            "Epoch: 783 Train loss: 0.022643 Validation loss: 0.0034616\n",
            "Epoch: 784 Train loss: 0.022641 Validation loss: 0.0034752\n",
            "Epoch: 785 Train loss: 0.022638 Validation loss: 0.0034889\n",
            "Epoch: 786 Train loss: 0.022635 Validation loss: 0.0035028\n",
            "Epoch: 787 Train loss: 0.022632 Validation loss: 0.0035168\n",
            "Epoch: 788 Train loss: 0.022629 Validation loss: 0.0035309\n",
            "Epoch: 789 Train loss: 0.022627 Validation loss: 0.0035452\n",
            "Epoch: 790 Train loss: 0.022624 Validation loss: 0.0035596\n",
            "Epoch: 791 Train loss: 0.022621 Validation loss: 0.0035741\n",
            "Epoch: 792 Train loss: 0.022618 Validation loss: 0.0035888\n",
            "Epoch: 793 Train loss: 0.022615 Validation loss: 0.0036036\n",
            "Epoch: 794 Train loss: 0.022613 Validation loss: 0.0036186\n",
            "Epoch: 795 Train loss: 0.02261 Validation loss: 0.0036337\n",
            "Epoch: 796 Train loss: 0.022607 Validation loss: 0.0036489\n",
            "Epoch: 797 Train loss: 0.022604 Validation loss: 0.0036642\n",
            "Epoch: 798 Train loss: 0.022601 Validation loss: 0.0036797\n",
            "Epoch: 799 Train loss: 0.022598 Validation loss: 0.0036953\n",
            "Epoch: 800 Train loss: 0.022595 Validation loss: 0.003711\n",
            "Epoch: 801 Train loss: 0.022592 Validation loss: 0.0037269\n",
            "Epoch: 802 Train loss: 0.022589 Validation loss: 0.0037429\n",
            "Epoch: 803 Train loss: 0.022586 Validation loss: 0.003759\n",
            "Epoch: 804 Train loss: 0.022583 Validation loss: 0.0037752\n",
            "Epoch: 805 Train loss: 0.02258 Validation loss: 0.0037916\n",
            "Epoch: 806 Train loss: 0.022576 Validation loss: 0.0038081\n",
            "Epoch: 807 Train loss: 0.022573 Validation loss: 0.0038247\n",
            "Epoch: 808 Train loss: 0.02257 Validation loss: 0.0038414\n",
            "Epoch: 809 Train loss: 0.022567 Validation loss: 0.0038583\n",
            "Epoch: 810 Train loss: 0.022564 Validation loss: 0.0038753\n",
            "Epoch: 811 Train loss: 0.02256 Validation loss: 0.0038923\n",
            "Epoch: 812 Train loss: 0.022557 Validation loss: 0.0039096\n",
            "Epoch: 813 Train loss: 0.022554 Validation loss: 0.0039269\n",
            "Epoch: 814 Train loss: 0.02255 Validation loss: 0.0039444\n",
            "Epoch: 815 Train loss: 0.022547 Validation loss: 0.0039619\n",
            "Epoch: 816 Train loss: 0.022544 Validation loss: 0.0039797\n",
            "Epoch: 817 Train loss: 0.02254 Validation loss: 0.0039975\n",
            "Epoch: 818 Train loss: 0.022537 Validation loss: 0.0040154\n",
            "Epoch: 819 Train loss: 0.022533 Validation loss: 0.0040334\n",
            "Epoch: 820 Train loss: 0.02253 Validation loss: 0.0040516\n",
            "Epoch: 821 Train loss: 0.022527 Validation loss: 0.0040699\n",
            "Epoch: 822 Train loss: 0.022523 Validation loss: 0.0040883\n",
            "Epoch: 823 Train loss: 0.02252 Validation loss: 0.0041067\n",
            "Epoch: 824 Train loss: 0.022516 Validation loss: 0.0041253\n",
            "Epoch: 825 Train loss: 0.022512 Validation loss: 0.004144\n",
            "Epoch: 826 Train loss: 0.022509 Validation loss: 0.0041628\n",
            "Epoch: 827 Train loss: 0.022505 Validation loss: 0.0041817\n",
            "Epoch: 828 Train loss: 0.022502 Validation loss: 0.0042007\n",
            "Epoch: 829 Train loss: 0.022498 Validation loss: 0.0042199\n",
            "Epoch: 830 Train loss: 0.022494 Validation loss: 0.0042391\n",
            "Epoch: 831 Train loss: 0.022491 Validation loss: 0.0042584\n",
            "Epoch: 832 Train loss: 0.022487 Validation loss: 0.0042779\n",
            "Epoch: 833 Train loss: 0.022483 Validation loss: 0.0042974\n",
            "Epoch: 834 Train loss: 0.02248 Validation loss: 0.004317\n",
            "Epoch: 835 Train loss: 0.022476 Validation loss: 0.0043368\n",
            "Epoch: 836 Train loss: 0.022472 Validation loss: 0.0043565\n",
            "Epoch: 837 Train loss: 0.022469 Validation loss: 0.0043765\n",
            "Epoch: 838 Train loss: 0.022465 Validation loss: 0.0043966\n",
            "Epoch: 839 Train loss: 0.022461 Validation loss: 0.0044167\n",
            "Epoch: 840 Train loss: 0.022458 Validation loss: 0.0044369\n",
            "Epoch: 841 Train loss: 0.022454 Validation loss: 0.0044572\n",
            "Epoch: 842 Train loss: 0.02245 Validation loss: 0.0044775\n",
            "Epoch: 843 Train loss: 0.022446 Validation loss: 0.0044981\n",
            "Epoch: 844 Train loss: 0.022443 Validation loss: 0.0045186\n",
            "Epoch: 845 Train loss: 0.022439 Validation loss: 0.0045393\n",
            "Epoch: 846 Train loss: 0.022435 Validation loss: 0.0045601\n",
            "Epoch: 847 Train loss: 0.022432 Validation loss: 0.0045809\n",
            "Epoch: 848 Train loss: 0.022428 Validation loss: 0.0046018\n",
            "Epoch: 849 Train loss: 0.022424 Validation loss: 0.0046228\n",
            "Epoch: 850 Train loss: 0.022421 Validation loss: 0.0046439\n",
            "Epoch: 851 Train loss: 0.022417 Validation loss: 0.004665\n",
            "Epoch: 852 Train loss: 0.022413 Validation loss: 0.0046862\n",
            "Epoch: 853 Train loss: 0.02241 Validation loss: 0.0047075\n",
            "Epoch: 854 Train loss: 0.022406 Validation loss: 0.0047289\n",
            "Epoch: 855 Train loss: 0.022403 Validation loss: 0.0047504\n",
            "Epoch: 856 Train loss: 0.022399 Validation loss: 0.004772\n",
            "Epoch: 857 Train loss: 0.022395 Validation loss: 0.0047935\n",
            "Epoch: 858 Train loss: 0.022392 Validation loss: 0.0048153\n",
            "Epoch: 859 Train loss: 0.022388 Validation loss: 0.004837\n",
            "Epoch: 860 Train loss: 0.022385 Validation loss: 0.0048588\n",
            "Epoch: 861 Train loss: 0.022381 Validation loss: 0.0048808\n",
            "Epoch: 862 Train loss: 0.022378 Validation loss: 0.0049027\n",
            "Epoch: 863 Train loss: 0.022375 Validation loss: 0.0049248\n",
            "Epoch: 864 Train loss: 0.022371 Validation loss: 0.0049468\n",
            "Epoch: 865 Train loss: 0.022368 Validation loss: 0.0049689\n",
            "Epoch: 866 Train loss: 0.022364 Validation loss: 0.0049911\n",
            "Epoch: 867 Train loss: 0.022361 Validation loss: 0.0050134\n",
            "Epoch: 868 Train loss: 0.022358 Validation loss: 0.0050358\n",
            "Epoch: 869 Train loss: 0.022355 Validation loss: 0.0050582\n",
            "Epoch: 870 Train loss: 0.022351 Validation loss: 0.0050807\n",
            "Epoch: 871 Train loss: 0.022348 Validation loss: 0.0051032\n",
            "Epoch: 872 Train loss: 0.022345 Validation loss: 0.0051258\n",
            "Epoch: 873 Train loss: 0.022342 Validation loss: 0.0051484\n",
            "Epoch: 874 Train loss: 0.022339 Validation loss: 0.0051711\n",
            "Epoch: 875 Train loss: 0.022336 Validation loss: 0.0051938\n",
            "Epoch: 876 Train loss: 0.022333 Validation loss: 0.0052166\n",
            "Epoch: 877 Train loss: 0.022329 Validation loss: 0.0052394\n",
            "Epoch: 878 Train loss: 0.022327 Validation loss: 0.0052622\n",
            "Epoch: 879 Train loss: 0.022324 Validation loss: 0.0052852\n",
            "Epoch: 880 Train loss: 0.022321 Validation loss: 0.0053082\n",
            "Epoch: 881 Train loss: 0.022318 Validation loss: 0.0053313\n",
            "Epoch: 882 Train loss: 0.022315 Validation loss: 0.0053544\n",
            "Epoch: 883 Train loss: 0.022312 Validation loss: 0.0053775\n",
            "Epoch: 884 Train loss: 0.022309 Validation loss: 0.0054006\n",
            "Epoch: 885 Train loss: 0.022307 Validation loss: 0.0054239\n",
            "Epoch: 886 Train loss: 0.022304 Validation loss: 0.0054472\n",
            "Epoch: 887 Train loss: 0.022301 Validation loss: 0.0054704\n",
            "Epoch: 888 Train loss: 0.022299 Validation loss: 0.0054937\n",
            "Epoch: 889 Train loss: 0.022296 Validation loss: 0.0055171\n",
            "Epoch: 890 Train loss: 0.022294 Validation loss: 0.0055406\n",
            "Epoch: 891 Train loss: 0.022291 Validation loss: 0.0055641\n",
            "Epoch: 892 Train loss: 0.022289 Validation loss: 0.0055876\n",
            "Epoch: 893 Train loss: 0.022286 Validation loss: 0.0056111\n",
            "Epoch: 894 Train loss: 0.022284 Validation loss: 0.0056347\n",
            "Epoch: 895 Train loss: 0.022281 Validation loss: 0.0056583\n",
            "Epoch: 896 Train loss: 0.022279 Validation loss: 0.0056819\n",
            "Epoch: 897 Train loss: 0.022277 Validation loss: 0.0057056\n",
            "Epoch: 898 Train loss: 0.022275 Validation loss: 0.0057292\n",
            "Epoch: 899 Train loss: 0.022272 Validation loss: 0.005753\n",
            "Epoch: 900 Train loss: 0.02227 Validation loss: 0.0057767\n",
            "Epoch: 901 Train loss: 0.022268 Validation loss: 0.0058005\n",
            "Epoch: 902 Train loss: 0.022266 Validation loss: 0.0058244\n",
            "Epoch: 903 Train loss: 0.022264 Validation loss: 0.0058483\n",
            "Epoch: 904 Train loss: 0.022262 Validation loss: 0.0058722\n",
            "Epoch: 905 Train loss: 0.02226 Validation loss: 0.0058961\n",
            "Epoch: 906 Train loss: 0.022258 Validation loss: 0.00592\n",
            "Epoch: 907 Train loss: 0.022256 Validation loss: 0.0059441\n",
            "Epoch: 908 Train loss: 0.022254 Validation loss: 0.005968\n",
            "Epoch: 909 Train loss: 0.022252 Validation loss: 0.005992\n",
            "Epoch: 910 Train loss: 0.02225 Validation loss: 0.0060161\n",
            "Epoch: 911 Train loss: 0.022249 Validation loss: 0.0060402\n",
            "Epoch: 912 Train loss: 0.022247 Validation loss: 0.0060643\n",
            "Epoch: 913 Train loss: 0.022245 Validation loss: 0.0060883\n",
            "Epoch: 914 Train loss: 0.022244 Validation loss: 0.0061125\n",
            "Epoch: 915 Train loss: 0.022242 Validation loss: 0.0061367\n",
            "Epoch: 916 Train loss: 0.02224 Validation loss: 0.0061609\n",
            "Epoch: 917 Train loss: 0.022239 Validation loss: 0.0061851\n",
            "Epoch: 918 Train loss: 0.022237 Validation loss: 0.0062093\n",
            "Epoch: 919 Train loss: 0.022236 Validation loss: 0.0062336\n",
            "Epoch: 920 Train loss: 0.022234 Validation loss: 0.0062578\n",
            "Epoch: 921 Train loss: 0.022233 Validation loss: 0.0062821\n",
            "Epoch: 922 Train loss: 0.022231 Validation loss: 0.0063064\n",
            "Epoch: 923 Train loss: 0.02223 Validation loss: 0.0063307\n",
            "Epoch: 924 Train loss: 0.022228 Validation loss: 0.0063551\n",
            "Epoch: 925 Train loss: 0.022227 Validation loss: 0.0063794\n",
            "Epoch: 926 Train loss: 0.022226 Validation loss: 0.0064038\n",
            "Epoch: 927 Train loss: 0.022225 Validation loss: 0.0064281\n",
            "Epoch: 928 Train loss: 0.022223 Validation loss: 0.0064525\n",
            "Epoch: 929 Train loss: 0.022222 Validation loss: 0.006477\n",
            "Epoch: 930 Train loss: 0.022221 Validation loss: 0.0065014\n",
            "Epoch: 931 Train loss: 0.02222 Validation loss: 0.0065258\n",
            "Epoch: 932 Train loss: 0.022219 Validation loss: 0.0065502\n",
            "Epoch: 933 Train loss: 0.022218 Validation loss: 0.0065747\n",
            "Epoch: 934 Train loss: 0.022217 Validation loss: 0.0065992\n",
            "Epoch: 935 Train loss: 0.022215 Validation loss: 0.0066237\n",
            "Epoch: 936 Train loss: 0.022214 Validation loss: 0.0066482\n",
            "Epoch: 937 Train loss: 0.022213 Validation loss: 0.0066727\n",
            "Epoch: 938 Train loss: 0.022213 Validation loss: 0.0066972\n",
            "Epoch: 939 Train loss: 0.022212 Validation loss: 0.0067217\n",
            "Epoch: 940 Train loss: 0.022211 Validation loss: 0.0067463\n",
            "Epoch: 941 Train loss: 0.02221 Validation loss: 0.0067707\n",
            "Epoch: 942 Train loss: 0.022209 Validation loss: 0.0067953\n",
            "Epoch: 943 Train loss: 0.022208 Validation loss: 0.0068199\n",
            "Epoch: 944 Train loss: 0.022207 Validation loss: 0.0068444\n",
            "Epoch: 945 Train loss: 0.022206 Validation loss: 0.006869\n",
            "Epoch: 946 Train loss: 0.022206 Validation loss: 0.0068935\n",
            "Epoch: 947 Train loss: 0.022205 Validation loss: 0.0069181\n",
            "Epoch: 948 Train loss: 0.022204 Validation loss: 0.0069427\n",
            "Epoch: 949 Train loss: 0.022204 Validation loss: 0.0069673\n",
            "Epoch: 950 Train loss: 0.022203 Validation loss: 0.0069919\n",
            "Epoch: 951 Train loss: 0.022202 Validation loss: 0.0070165\n",
            "Epoch: 952 Train loss: 0.022202 Validation loss: 0.007041\n",
            "Epoch: 953 Train loss: 0.022201 Validation loss: 0.0070657\n",
            "Epoch: 954 Train loss: 0.0222 Validation loss: 0.0070903\n",
            "Epoch: 955 Train loss: 0.0222 Validation loss: 0.0071148\n",
            "Epoch: 956 Train loss: 0.022199 Validation loss: 0.0071394\n",
            "Epoch: 957 Train loss: 0.022199 Validation loss: 0.007164\n",
            "Epoch: 958 Train loss: 0.022198 Validation loss: 0.0071886\n",
            "Epoch: 959 Train loss: 0.022198 Validation loss: 0.0072132\n",
            "Epoch: 960 Train loss: 0.022198 Validation loss: 0.0072378\n",
            "Epoch: 961 Train loss: 0.022197 Validation loss: 0.0072624\n",
            "Epoch: 962 Train loss: 0.022197 Validation loss: 0.007287\n",
            "Epoch: 963 Train loss: 0.022196 Validation loss: 0.0073115\n",
            "Epoch: 964 Train loss: 0.022196 Validation loss: 0.0073361\n",
            "Epoch: 965 Train loss: 0.022196 Validation loss: 0.0073607\n",
            "Epoch: 966 Train loss: 0.022195 Validation loss: 0.0073853\n",
            "Epoch: 967 Train loss: 0.022195 Validation loss: 0.0074099\n",
            "Epoch: 968 Train loss: 0.022195 Validation loss: 0.0074343\n",
            "Epoch: 969 Train loss: 0.022194 Validation loss: 0.007459\n",
            "Epoch: 970 Train loss: 0.022194 Validation loss: 0.0074835\n",
            "Epoch: 971 Train loss: 0.022194 Validation loss: 0.007508\n",
            "Epoch: 972 Train loss: 0.022194 Validation loss: 0.0075326\n",
            "Epoch: 973 Train loss: 0.022194 Validation loss: 0.0075571\n",
            "Epoch: 974 Train loss: 0.022193 Validation loss: 0.0075817\n",
            "Epoch: 975 Train loss: 0.022193 Validation loss: 0.0076061\n",
            "Epoch: 976 Train loss: 0.022193 Validation loss: 0.0076307\n",
            "Epoch: 977 Train loss: 0.022193 Validation loss: 0.0076552\n",
            "Epoch: 978 Train loss: 0.022193 Validation loss: 0.0076795\n",
            "Epoch: 979 Train loss: 0.022193 Validation loss: 0.007704\n",
            "Epoch: 980 Train loss: 0.022193 Validation loss: 0.0077286\n",
            "Epoch: 981 Train loss: 0.022193 Validation loss: 0.0077529\n",
            "Epoch: 982 Train loss: 0.022193 Validation loss: 0.0077774\n",
            "Epoch: 983 Train loss: 0.022193 Validation loss: 0.0078019\n",
            "Epoch: 984 Train loss: 0.022193 Validation loss: 0.0078262\n",
            "Epoch: 985 Train loss: 0.022193 Validation loss: 0.0078507\n",
            "Epoch: 986 Train loss: 0.022193 Validation loss: 0.007875\n",
            "Epoch: 987 Train loss: 0.022193 Validation loss: 0.0078994\n",
            "Epoch: 988 Train loss: 0.022193 Validation loss: 0.0079239\n",
            "Epoch: 989 Train loss: 0.022193 Validation loss: 0.0079481\n",
            "Epoch: 990 Train loss: 0.022193 Validation loss: 0.0079725\n",
            "Epoch: 991 Train loss: 0.022193 Validation loss: 0.0079968\n",
            "Epoch: 992 Train loss: 0.022193 Validation loss: 0.0080211\n",
            "Epoch: 993 Train loss: 0.022193 Validation loss: 0.0080453\n",
            "Epoch: 994 Train loss: 0.022194 Validation loss: 0.0080696\n",
            "Epoch: 995 Train loss: 0.022194 Validation loss: 0.0080938\n",
            "Epoch: 996 Train loss: 0.022194 Validation loss: 0.0081181\n",
            "Epoch: 997 Train loss: 0.022194 Validation loss: 0.0081423\n",
            "Epoch: 998 Train loss: 0.022194 Validation loss: 0.0081665\n",
            "Epoch: 999 Train loss: 0.022195 Validation loss: 0.0081907\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylr1C2ovWXbO",
        "colab_type": "text"
      },
      "source": [
        "During the training of the regularized model we can already notice, that, although there is still a difference between training and validation loss, the validation loss decreases as the training loss dreases. The effect of the regularization becomes even more evident if we plot the predictions of the regularized model and the overfitting model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdMRV0vAWLmm",
        "colab_type": "code",
        "outputId": "bc77f82c-d508-4049-b2d4-75a473bf5cfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "y_pred = big_reg_mdl(x)\n",
        "y_pred_overfit = big_mdl(x)\n",
        "plt.scatter(x_train_overfit, y_train_overfit)\n",
        "plt.plot(x, y_true)\n",
        "plt.plot(x, y_pred.numpy())\n",
        "plt.plot(x, y_pred_overfit.numpy())\n",
        "plt.legend([\"Target\", \"Regularization\", \"No regularization\", \"Training samples\"])\n",
        "plt.show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sePQBaGqZzT6",
        "colab_type": "text"
      },
      "source": [
        "The model with regularization seems to follow the overall trend of the data, while the model without any regularization very precisely fits the training samples. This is espacilly evident in the interval $\\left[0,0.5\\right]$, where the prediction of the unregularized model shows an oscillating behavior. Such oscillations are however not present in the ground truth and therefore undesirable. The regularized model on the other hand is not as flexible as the unregularized model and therefore does not fit the target function well in the interval $\\left[2.25, 3.0\\right]$.\n",
        "\n",
        "## Conclusion\n",
        "In this exercise we revisited the mathematical background for a simple regression task and covered it's practical implementation in Tensorflow 2. We also explored the phenomenon of overfitting and derived different regularizations from a probabilistic perspective. This exercise covers a very simple task with a very basic neural architecture and is intended as a primer for the second part of the regression exercise, which is dealing with a bigger and more realistic problem. In this second part we will consider the problem of regressing the age of a person from a potrait picture."
      ]
    }
  ]
}